{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series forecasting\n",
    "\n",
    "The dataset we are using for this is [this one](https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption).\n",
    "\n",
    "Get the dataset using the command as follows:\n",
    "\n",
    "`wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from dateutil.parser import parse\n",
    "from sagemaker import get_execution_role,Session,estimator,predictor ## install this with 'sudo pip install sagemaker'\n",
    "\n",
    "roleARN = get_execution_role()\n",
    "bucket = 'rnszsdemo' ## Replace with your bucket name\n",
    "prefix = 'sagemaker/data/Household_Electricity_Consumption' ## Replace with the folder structure inside your bucket or simply ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import json\n",
    "import bisect\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sagemaker.amazon.common as smac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-07 10:00:59--  https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20640916 (20M) [application/zip]\n",
      "Saving to: ‘household_power_consumption.zip.1’\n",
      "\n",
      "household_power_con 100%[===================>]  19.68M  10.4MB/s    in 1.9s    \n",
      "\n",
      "2018-10-07 10:01:01 (10.4 MB/s) - ‘household_power_consumption.zip.1’ saved [20640916/20640916]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following files have been extracted: \n",
      "['household_power_consumption.txt']\n"
     ]
    }
   ],
   "source": [
    "with ZipFile('household_power_consumption.zip') as zfl:\n",
    "    zfl.extractall()\n",
    "    print(\"Following files have been extracted: \\n{}\".format(zfl.namelist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date;Time;Global_active_power;Global_reactive_power;Voltage;Global_intensity;Sub_metering_1;Sub_metering_2;Sub_metering_3\r",
      "\r\n",
      "16/12/2006;17:24:00;4.216;0.418;234.840;18.400;0.000;1.000;17.000\r",
      "\r\n",
      "16/12/2006;17:25:00;5.360;0.436;233.630;23.000;0.000;1.000;16.000\r",
      "\r\n",
      "16/12/2006;17:26:00;5.374;0.498;233.290;23.000;0.000;2.000;17.000\r",
      "\r\n",
      "16/12/2006;17:27:00;5.388;0.502;233.740;23.000;0.000;1.000;17.000\r",
      "\r\n",
      "16/12/2006;17:28:00;3.666;0.528;235.680;15.800;0.000;1.000;17.000\r",
      "\r\n",
      "16/12/2006;17:29:00;3.520;0.522;235.020;15.000;0.000;2.000;17.000\r",
      "\r\n",
      "16/12/2006;17:30:00;3.702;0.520;235.090;15.800;0.000;1.000;17.000\r",
      "\r\n",
      "16/12/2006;17:31:00;3.700;0.520;235.220;15.800;0.000;1.000;17.000\r",
      "\r\n",
      "16/12/2006;17:32:00;3.668;0.510;233.990;15.800;0.000;1.000;17.000\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head household_power_consumption.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2075260 household_power_consumption.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l household_power_consumption.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "inputdata = pd.read_csv('./household_power_consumption.txt',sep=';',header=0,index_col=[0],parse_dates=[0,1],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-16</th>\n",
       "      <td>2018-10-01 17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16</th>\n",
       "      <td>2018-10-01 17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16</th>\n",
       "      <td>2018-10-01 17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16</th>\n",
       "      <td>2018-10-01 17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16</th>\n",
       "      <td>2018-10-01 17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Time Global_active_power Global_reactive_power  \\\n",
       "Date                                                                       \n",
       "2006-12-16 2018-10-01 17:24:00               4.216                 0.418   \n",
       "2006-12-16 2018-10-01 17:25:00               5.360                 0.436   \n",
       "2006-12-16 2018-10-01 17:26:00               5.374                 0.498   \n",
       "2006-12-16 2018-10-01 17:27:00               5.388                 0.502   \n",
       "2006-12-16 2018-10-01 17:28:00               3.666                 0.528   \n",
       "\n",
       "            Voltage Global_intensity Sub_metering_1 Sub_metering_2  \\\n",
       "Date                                                                 \n",
       "2006-12-16  234.840           18.400          0.000          1.000   \n",
       "2006-12-16  233.630           23.000          0.000          1.000   \n",
       "2006-12-16  233.290           23.000          0.000          2.000   \n",
       "2006-12-16  233.740           23.000          0.000          1.000   \n",
       "2006-12-16  235.680           15.800          0.000          1.000   \n",
       "\n",
       "            Sub_metering_3  \n",
       "Date                        \n",
       "2006-12-16            17.0  \n",
       "2006-12-16            16.0  \n",
       "2006-12-16            17.0  \n",
       "2006-12-16            17.0  \n",
       "2006-12-16            17.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdata.replace('?',np.nan,inplace=True)\n",
    "inputdata[['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']] = inputdata[['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3']].apply(pd.to_numeric)\n",
    "inputdata['Date2'] = inputdata.index.values\n",
    "inputdata['TimeHrs'] = inputdata['Time'].dt.hour\n",
    "inp = inputdata\n",
    "grp = inp.groupby(['Date2','TimeHrs'],as_index=False)\n",
    "inp1 = grp['Global_active_power','Global_reactive_power','Voltage','Global_intensity','Sub_metering_1','Sub_metering_2','Sub_metering_3'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date2</th>\n",
       "      <th>TimeHrs</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>17</td>\n",
       "      <td>4.222889</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>234.643889</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>16.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>18</td>\n",
       "      <td>3.632200</td>\n",
       "      <td>0.080033</td>\n",
       "      <td>234.580167</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.716667</td>\n",
       "      <td>16.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>19</td>\n",
       "      <td>3.400233</td>\n",
       "      <td>0.085233</td>\n",
       "      <td>233.232500</td>\n",
       "      <td>14.503333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>16.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>20</td>\n",
       "      <td>3.268567</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>234.071500</td>\n",
       "      <td>13.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>21</td>\n",
       "      <td>3.056467</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>237.158667</td>\n",
       "      <td>13.046667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>17.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>22</td>\n",
       "      <td>2.200133</td>\n",
       "      <td>0.056167</td>\n",
       "      <td>238.760000</td>\n",
       "      <td>9.523333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>4.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>23</td>\n",
       "      <td>2.061600</td>\n",
       "      <td>0.071433</td>\n",
       "      <td>240.619667</td>\n",
       "      <td>8.896667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.882467</td>\n",
       "      <td>0.102433</td>\n",
       "      <td>240.961833</td>\n",
       "      <td>8.126667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.349400</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>240.448333</td>\n",
       "      <td>14.246667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.233333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>2</td>\n",
       "      <td>1.587267</td>\n",
       "      <td>0.078233</td>\n",
       "      <td>245.818667</td>\n",
       "      <td>6.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>3</td>\n",
       "      <td>1.662200</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>244.513500</td>\n",
       "      <td>7.206667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>4</td>\n",
       "      <td>2.215767</td>\n",
       "      <td>0.093467</td>\n",
       "      <td>243.855500</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>8.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>1.996733</td>\n",
       "      <td>0.060233</td>\n",
       "      <td>243.710167</td>\n",
       "      <td>8.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>6</td>\n",
       "      <td>1.303300</td>\n",
       "      <td>0.094833</td>\n",
       "      <td>244.141500</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>7</td>\n",
       "      <td>1.620033</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>241.845667</td>\n",
       "      <td>6.803333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>8</td>\n",
       "      <td>1.890567</td>\n",
       "      <td>0.118567</td>\n",
       "      <td>241.311500</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>9</td>\n",
       "      <td>2.549067</td>\n",
       "      <td>0.079233</td>\n",
       "      <td>238.847833</td>\n",
       "      <td>10.973333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.316667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>10</td>\n",
       "      <td>3.628900</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>235.441000</td>\n",
       "      <td>15.533333</td>\n",
       "      <td>8.883333</td>\n",
       "      <td>22.350000</td>\n",
       "      <td>16.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>11</td>\n",
       "      <td>2.471000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>236.924667</td>\n",
       "      <td>10.493333</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>5.016667</td>\n",
       "      <td>17.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>12</td>\n",
       "      <td>1.915867</td>\n",
       "      <td>0.294033</td>\n",
       "      <td>237.784667</td>\n",
       "      <td>8.136667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>17.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>13</td>\n",
       "      <td>1.660767</td>\n",
       "      <td>0.171533</td>\n",
       "      <td>239.920667</td>\n",
       "      <td>6.923333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>17.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>14</td>\n",
       "      <td>2.092633</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>244.045667</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>15</td>\n",
       "      <td>2.985400</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>240.480833</td>\n",
       "      <td>12.426667</td>\n",
       "      <td>6.966667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>17.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>16</td>\n",
       "      <td>3.326033</td>\n",
       "      <td>0.093133</td>\n",
       "      <td>236.781833</td>\n",
       "      <td>14.006667</td>\n",
       "      <td>8.583333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>17.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>17</td>\n",
       "      <td>3.406767</td>\n",
       "      <td>0.166633</td>\n",
       "      <td>234.229833</td>\n",
       "      <td>14.510000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>16.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>18</td>\n",
       "      <td>3.697100</td>\n",
       "      <td>0.135067</td>\n",
       "      <td>234.372333</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>19</td>\n",
       "      <td>2.908400</td>\n",
       "      <td>0.265167</td>\n",
       "      <td>233.195667</td>\n",
       "      <td>12.516667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>16.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>20</td>\n",
       "      <td>3.361500</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>236.426500</td>\n",
       "      <td>14.276667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>17.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>21</td>\n",
       "      <td>3.040767</td>\n",
       "      <td>0.267967</td>\n",
       "      <td>239.104167</td>\n",
       "      <td>12.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>22</td>\n",
       "      <td>1.518000</td>\n",
       "      <td>0.235367</td>\n",
       "      <td>242.192333</td>\n",
       "      <td>6.383333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>23</td>\n",
       "      <td>0.437733</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>245.734000</td>\n",
       "      <td>1.996667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.099067</td>\n",
       "      <td>244.607500</td>\n",
       "      <td>1.223333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>243.519333</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.284467</td>\n",
       "      <td>0.109067</td>\n",
       "      <td>246.891000</td>\n",
       "      <td>1.256667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.309933</td>\n",
       "      <td>0.147767</td>\n",
       "      <td>245.731833</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>4</td>\n",
       "      <td>1.026333</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>245.507000</td>\n",
       "      <td>4.176667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>12.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.121933</td>\n",
       "      <td>245.253167</td>\n",
       "      <td>1.293333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>245.704333</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>7</td>\n",
       "      <td>2.450433</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>240.962333</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>11.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>8</td>\n",
       "      <td>2.082133</td>\n",
       "      <td>0.073733</td>\n",
       "      <td>237.880333</td>\n",
       "      <td>8.726667</td>\n",
       "      <td>12.466667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>17.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>9</td>\n",
       "      <td>1.629333</td>\n",
       "      <td>0.085167</td>\n",
       "      <td>237.404667</td>\n",
       "      <td>6.816667</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>17.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>10</td>\n",
       "      <td>1.309633</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>239.080500</td>\n",
       "      <td>5.426667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>11</td>\n",
       "      <td>1.561933</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>239.771000</td>\n",
       "      <td>6.470000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>17.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>12</td>\n",
       "      <td>1.756067</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>242.580000</td>\n",
       "      <td>7.246667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>13</td>\n",
       "      <td>1.682067</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>241.954167</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>17.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>14</td>\n",
       "      <td>1.733033</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>242.196500</td>\n",
       "      <td>7.096667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.383333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>15</td>\n",
       "      <td>1.784300</td>\n",
       "      <td>0.104067</td>\n",
       "      <td>242.423500</td>\n",
       "      <td>7.310000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>17.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>16</td>\n",
       "      <td>1.949300</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>242.380667</td>\n",
       "      <td>8.096667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.983333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date2  TimeHrs  Global_active_power  Global_reactive_power  \\\n",
       "0  2006-12-16       17             4.222889               0.229000   \n",
       "1  2006-12-16       18             3.632200               0.080033   \n",
       "2  2006-12-16       19             3.400233               0.085233   \n",
       "3  2006-12-16       20             3.268567               0.075100   \n",
       "4  2006-12-16       21             3.056467               0.076667   \n",
       "5  2006-12-16       22             2.200133               0.056167   \n",
       "6  2006-12-16       23             2.061600               0.071433   \n",
       "7  2006-12-17        0             1.882467               0.102433   \n",
       "8  2006-12-17        1             3.349400               0.136933   \n",
       "9  2006-12-17        2             1.587267               0.078233   \n",
       "10 2006-12-17        3             1.662200               0.079533   \n",
       "11 2006-12-17        4             2.215767               0.093467   \n",
       "12 2006-12-17        5             1.996733               0.060233   \n",
       "13 2006-12-17        6             1.303300               0.094833   \n",
       "14 2006-12-17        7             1.620033               0.059800   \n",
       "15 2006-12-17        8             1.890567               0.118567   \n",
       "16 2006-12-17        9             2.549067               0.079233   \n",
       "17 2006-12-17       10             3.628900               0.194600   \n",
       "18 2006-12-17       11             2.471000               0.200700   \n",
       "19 2006-12-17       12             1.915867               0.294033   \n",
       "20 2006-12-17       13             1.660767               0.171533   \n",
       "21 2006-12-17       14             2.092633               0.257400   \n",
       "22 2006-12-17       15             2.985400               0.088600   \n",
       "23 2006-12-17       16             3.326033               0.093133   \n",
       "24 2006-12-17       17             3.406767               0.166633   \n",
       "25 2006-12-17       18             3.697100               0.135067   \n",
       "26 2006-12-17       19             2.908400               0.265167   \n",
       "27 2006-12-17       20             3.361500               0.271500   \n",
       "28 2006-12-17       21             3.040767               0.267967   \n",
       "29 2006-12-17       22             1.518000               0.235367   \n",
       "30 2006-12-17       23             0.437733               0.221800   \n",
       "31 2006-12-18        0             0.276367               0.099067   \n",
       "32 2006-12-18        1             0.313300               0.151900   \n",
       "33 2006-12-18        2             0.284467               0.109067   \n",
       "34 2006-12-18        3             0.309933               0.147767   \n",
       "35 2006-12-18        4             1.026333               0.080000   \n",
       "36 2006-12-18        5             0.293500               0.121933   \n",
       "37 2006-12-18        6             0.610000               0.112900   \n",
       "38 2006-12-18        7             2.450433               0.153000   \n",
       "39 2006-12-18        8             2.082133               0.073733   \n",
       "40 2006-12-18        9             1.629333               0.085167   \n",
       "41 2006-12-18       10             1.309633               0.052833   \n",
       "42 2006-12-18       11             1.561933               0.127333   \n",
       "43 2006-12-18       12             1.756067               0.087300   \n",
       "44 2006-12-18       13             1.682067               0.126500   \n",
       "45 2006-12-18       14             1.733033               0.049300   \n",
       "46 2006-12-18       15             1.784300               0.104067   \n",
       "47 2006-12-18       16             1.949300               0.204433   \n",
       "\n",
       "       Voltage  Global_intensity  Sub_metering_1  Sub_metering_2  \\\n",
       "0   234.643889         18.100000        0.000000        0.527778   \n",
       "1   234.580167         15.600000        0.000000        6.716667   \n",
       "2   233.232500         14.503333        0.000000        1.433333   \n",
       "3   234.071500         13.916667        0.000000        0.000000   \n",
       "4   237.158667         13.046667        0.000000        0.416667   \n",
       "5   238.760000          9.523333        0.000000        0.133333   \n",
       "6   240.619667          8.896667        0.000000        0.083333   \n",
       "7   240.961833          8.126667        0.000000        0.466667   \n",
       "8   240.448333         14.246667        0.000000       25.233333   \n",
       "9   245.818667          6.870000        0.000000        0.566667   \n",
       "10  244.513500          7.206667        0.000000        0.766667   \n",
       "11  243.855500          9.333333        0.000000        0.566667   \n",
       "12  243.710167          8.566667        0.000000        0.166667   \n",
       "13  244.141500          5.530000        0.000000        0.716667   \n",
       "14  241.845667          6.803333        0.000000        0.166667   \n",
       "15  241.311500          8.050000        0.000000        0.750000   \n",
       "16  238.847833         10.973333        0.000000        7.316667   \n",
       "17  235.441000         15.533333        8.883333       22.350000   \n",
       "18  236.924667         10.493333        9.450000        5.016667   \n",
       "19  237.784667          8.136667        0.000000        0.533333   \n",
       "20  239.920667          6.923333        0.000000        0.050000   \n",
       "21  244.045667          8.600000        0.000000        0.333333   \n",
       "22  240.480833         12.426667        6.966667        0.200000   \n",
       "23  236.781833         14.006667        8.583333        0.066667   \n",
       "24  234.229833         14.510000        0.000000        0.466667   \n",
       "25  234.372333         15.750000        0.000000        0.000000   \n",
       "26  233.195667         12.516667        0.000000        0.516667   \n",
       "27  236.426500         14.276667        0.000000        1.116667   \n",
       "28  239.104167         12.716667        0.000000        1.200000   \n",
       "29  242.192333          6.383333        0.000000        0.416667   \n",
       "30  245.734000          1.996667        0.000000        0.800000   \n",
       "31  244.607500          1.223333        0.000000        0.033333   \n",
       "32  243.519333          1.416667        0.000000        0.583333   \n",
       "33  246.891000          1.256667        0.000000        0.000000   \n",
       "34  245.731833          1.400000        0.000000        0.566667   \n",
       "35  245.507000          4.176667        0.000000        0.066667   \n",
       "36  245.253167          1.293333        0.000000        0.466667   \n",
       "37  245.704333          2.600000        0.000000        0.083333   \n",
       "38  240.962333         10.220000        0.083333        0.450000   \n",
       "39  237.880333          8.726667       12.466667        0.033333   \n",
       "40  237.404667          6.816667        5.166667        0.483333   \n",
       "41  239.080500          5.426667        0.000000        0.000000   \n",
       "42  239.771000          6.470000        0.000000        0.516667   \n",
       "43  242.580000          7.246667        0.000000        0.000000   \n",
       "44  241.954167          6.940000        0.000000        0.533333   \n",
       "45  242.196500          7.096667        0.000000        0.000000   \n",
       "46  242.423500          7.310000        0.000000        0.516667   \n",
       "47  242.380667          8.096667        0.000000        0.000000   \n",
       "\n",
       "    Sub_metering_3  \n",
       "0        16.861111  \n",
       "1        16.866667  \n",
       "2        16.683333  \n",
       "3        16.783333  \n",
       "4        17.216667  \n",
       "5         4.433333  \n",
       "6         0.000000  \n",
       "7         0.000000  \n",
       "8         0.000000  \n",
       "9         0.000000  \n",
       "10        0.000000  \n",
       "11        8.883333  \n",
       "12        4.650000  \n",
       "13        0.000000  \n",
       "14        0.000000  \n",
       "15        0.000000  \n",
       "16        0.000000  \n",
       "17       16.150000  \n",
       "18       17.183333  \n",
       "19       17.316667  \n",
       "20       17.616667  \n",
       "21       18.233333  \n",
       "22       17.700000  \n",
       "23       17.166667  \n",
       "24       16.816667  \n",
       "25       16.833333  \n",
       "26       16.683333  \n",
       "27       17.116667  \n",
       "28       17.500000  \n",
       "29        2.500000  \n",
       "30        0.000000  \n",
       "31        0.000000  \n",
       "32        0.000000  \n",
       "33        0.000000  \n",
       "34        0.000000  \n",
       "35       12.550000  \n",
       "36        0.000000  \n",
       "37        0.000000  \n",
       "38       11.300000  \n",
       "39       17.350000  \n",
       "40       17.266667  \n",
       "41       17.500000  \n",
       "42       17.600000  \n",
       "43       18.016667  \n",
       "44       17.366667  \n",
       "45       17.383333  \n",
       "46       17.983333  \n",
       "47       17.983333  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1.head(48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6f711fa20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp1['Global_active_power'].plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sollistv2 = [datetime.date(2000, 6, 21), datetime.date(2000, 12, 21), datetime.date(2001, 6, 21), datetime.date(2001, 12, 21), datetime.date(2002, 6, 21), datetime.date(2002, 12, 22), datetime.date(2003, 6, 21), datetime.date(2003, 12, 22), datetime.date(2004, 6, 21), datetime.date(2004, 12, 21), datetime.date(2005, 6, 21), datetime.date(2005, 12, 21), datetime.date(2006, 6, 21), datetime.date(2006, 12, 22), datetime.date(2007, 6, 21), datetime.date(2007, 12, 22), datetime.date(2008, 6, 20), datetime.date(2008, 12, 21), datetime.date(2009, 6, 21), datetime.date(2009, 12, 21), datetime.date(2010, 6, 21), datetime.date(2010, 12, 21), datetime.date(2011, 6, 21), datetime.date(2011, 12, 22), datetime.date(2012, 6, 20), datetime.date(2012, 12, 21), datetime.date(2013, 6, 21), datetime.date(2013, 12, 21), datetime.date(2014, 6, 21), datetime.date(2014, 12, 21), datetime.date(2015, 6, 21), datetime.date(2015, 12, 22), datetime.date(2016, 6, 20), datetime.date(2016, 12, 21), datetime.date(2017, 6, 21), datetime.date(2017, 12, 21), datetime.date(2018, 6, 21), datetime.date(2018, 12, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build the _sollistv2_ array like its done here below, you could use this code in a file in the terminal on this Jupyter Notebook instance. You can then use the array for the code in the next cell. You would need to install pyephem using pip prior to running this program. Although for the purposes for this notebook, you don't have to run this again as the data output by this code will remain the same and has been assigned to the _sollistv2_ above.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import ephem\n",
    "import datetime\n",
    "\n",
    "def main():\n",
    "        sollist,sollistv2 = [],[]\n",
    "        for yr in range(2000,2019):\n",
    "                nsol1 = ephem.next_solstice(str(yr))\n",
    "                nsol2 = ephem.next_solstice(nsol1)\n",
    "                strdt = nsol1.datetime().strftime(\"%Y-%m-%d\")\n",
    "                sollist.append(strdt)\n",
    "                sollistv2.append(datetime.datetime.strptime(sollist[-1],\"%Y-%m-%d\").date())\n",
    "                strdt = nsol2.datetime().strftime(\"%Y-%m-%d\")\n",
    "                sollist.append(strdt)\n",
    "                sollistv2.append(datetime.datetime.strptime(sollist[-1],\"%Y-%m-%d\").date())\n",
    "        print(sollistv2)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We calculate the distances from solstices and return the values, the input is observation date \n",
    "## i.e. same name as the column in our dataframes(inputs). We then populate the other columns.\n",
    "def getDistFromSolstices(obdt):\n",
    "    indx = bisect.bisect(sollistv2,obdt.date())\n",
    "    return (obdt.date() - sollistv2[indx-1]).days,(sollistv2[indx] - obdt.date()).days\n",
    "\n",
    "def populateDistance(data):\n",
    "    dsfromlast,dsfromnext = [],[]\n",
    "    for row in data.itertuples():\n",
    "        dfromlast,dfromnext = getDistFromSolstices(row[1]) #Get 'Date2' column\n",
    "        dsfromlast.append(dfromlast)\n",
    "        dsfromnext.append(dfromnext)\n",
    "    data = data.assign(distanceFromLastSolstice=pd.Series(dsfromlast,index=data.index))\n",
    "    data = data.assign(distanceFromNextSolstice=pd.Series(dsfromnext,index=data.index))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDF = populateDistance(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date2</th>\n",
       "      <th>TimeHrs</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "      <th>distanceFromLastSolstice</th>\n",
       "      <th>distanceFromNextSolstice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>17</td>\n",
       "      <td>4.222889</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>234.643889</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>16.861111</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>18</td>\n",
       "      <td>3.632200</td>\n",
       "      <td>0.080033</td>\n",
       "      <td>234.580167</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.716667</td>\n",
       "      <td>16.866667</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>19</td>\n",
       "      <td>3.400233</td>\n",
       "      <td>0.085233</td>\n",
       "      <td>233.232500</td>\n",
       "      <td>14.503333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>16.683333</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>20</td>\n",
       "      <td>3.268567</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>234.071500</td>\n",
       "      <td>13.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.783333</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>21</td>\n",
       "      <td>3.056467</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>237.158667</td>\n",
       "      <td>13.046667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>17.216667</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>22</td>\n",
       "      <td>2.200133</td>\n",
       "      <td>0.056167</td>\n",
       "      <td>238.760000</td>\n",
       "      <td>9.523333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>4.433333</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-12-16</td>\n",
       "      <td>23</td>\n",
       "      <td>2.061600</td>\n",
       "      <td>0.071433</td>\n",
       "      <td>240.619667</td>\n",
       "      <td>8.896667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.882467</td>\n",
       "      <td>0.102433</td>\n",
       "      <td>240.961833</td>\n",
       "      <td>8.126667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>1</td>\n",
       "      <td>3.349400</td>\n",
       "      <td>0.136933</td>\n",
       "      <td>240.448333</td>\n",
       "      <td>14.246667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.233333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>2</td>\n",
       "      <td>1.587267</td>\n",
       "      <td>0.078233</td>\n",
       "      <td>245.818667</td>\n",
       "      <td>6.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>3</td>\n",
       "      <td>1.662200</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>244.513500</td>\n",
       "      <td>7.206667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>4</td>\n",
       "      <td>2.215767</td>\n",
       "      <td>0.093467</td>\n",
       "      <td>243.855500</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>8.883333</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>5</td>\n",
       "      <td>1.996733</td>\n",
       "      <td>0.060233</td>\n",
       "      <td>243.710167</td>\n",
       "      <td>8.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>6</td>\n",
       "      <td>1.303300</td>\n",
       "      <td>0.094833</td>\n",
       "      <td>244.141500</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>7</td>\n",
       "      <td>1.620033</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>241.845667</td>\n",
       "      <td>6.803333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>8</td>\n",
       "      <td>1.890567</td>\n",
       "      <td>0.118567</td>\n",
       "      <td>241.311500</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>9</td>\n",
       "      <td>2.549067</td>\n",
       "      <td>0.079233</td>\n",
       "      <td>238.847833</td>\n",
       "      <td>10.973333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.316667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>10</td>\n",
       "      <td>3.628900</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>235.441000</td>\n",
       "      <td>15.533333</td>\n",
       "      <td>8.883333</td>\n",
       "      <td>22.350000</td>\n",
       "      <td>16.150000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>11</td>\n",
       "      <td>2.471000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>236.924667</td>\n",
       "      <td>10.493333</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>5.016667</td>\n",
       "      <td>17.183333</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>12</td>\n",
       "      <td>1.915867</td>\n",
       "      <td>0.294033</td>\n",
       "      <td>237.784667</td>\n",
       "      <td>8.136667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>17.316667</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>13</td>\n",
       "      <td>1.660767</td>\n",
       "      <td>0.171533</td>\n",
       "      <td>239.920667</td>\n",
       "      <td>6.923333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>17.616667</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>14</td>\n",
       "      <td>2.092633</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>244.045667</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>18.233333</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>15</td>\n",
       "      <td>2.985400</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>240.480833</td>\n",
       "      <td>12.426667</td>\n",
       "      <td>6.966667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>16</td>\n",
       "      <td>3.326033</td>\n",
       "      <td>0.093133</td>\n",
       "      <td>236.781833</td>\n",
       "      <td>14.006667</td>\n",
       "      <td>8.583333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>17</td>\n",
       "      <td>3.406767</td>\n",
       "      <td>0.166633</td>\n",
       "      <td>234.229833</td>\n",
       "      <td>14.510000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>16.816667</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>18</td>\n",
       "      <td>3.697100</td>\n",
       "      <td>0.135067</td>\n",
       "      <td>234.372333</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.833333</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>19</td>\n",
       "      <td>2.908400</td>\n",
       "      <td>0.265167</td>\n",
       "      <td>233.195667</td>\n",
       "      <td>12.516667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>16.683333</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>20</td>\n",
       "      <td>3.361500</td>\n",
       "      <td>0.271500</td>\n",
       "      <td>236.426500</td>\n",
       "      <td>14.276667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.116667</td>\n",
       "      <td>17.116667</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>21</td>\n",
       "      <td>3.040767</td>\n",
       "      <td>0.267967</td>\n",
       "      <td>239.104167</td>\n",
       "      <td>12.716667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>22</td>\n",
       "      <td>1.518000</td>\n",
       "      <td>0.235367</td>\n",
       "      <td>242.192333</td>\n",
       "      <td>6.383333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2006-12-17</td>\n",
       "      <td>23</td>\n",
       "      <td>0.437733</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>245.734000</td>\n",
       "      <td>1.996667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>179</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.099067</td>\n",
       "      <td>244.607500</td>\n",
       "      <td>1.223333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.313300</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>243.519333</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.284467</td>\n",
       "      <td>0.109067</td>\n",
       "      <td>246.891000</td>\n",
       "      <td>1.256667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.309933</td>\n",
       "      <td>0.147767</td>\n",
       "      <td>245.731833</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>4</td>\n",
       "      <td>1.026333</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>245.507000</td>\n",
       "      <td>4.176667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>12.550000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>5</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.121933</td>\n",
       "      <td>245.253167</td>\n",
       "      <td>1.293333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>245.704333</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>7</td>\n",
       "      <td>2.450433</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>240.962333</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>8</td>\n",
       "      <td>2.082133</td>\n",
       "      <td>0.073733</td>\n",
       "      <td>237.880333</td>\n",
       "      <td>8.726667</td>\n",
       "      <td>12.466667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>17.350000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>9</td>\n",
       "      <td>1.629333</td>\n",
       "      <td>0.085167</td>\n",
       "      <td>237.404667</td>\n",
       "      <td>6.816667</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>17.266667</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>10</td>\n",
       "      <td>1.309633</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>239.080500</td>\n",
       "      <td>5.426667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>11</td>\n",
       "      <td>1.561933</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>239.771000</td>\n",
       "      <td>6.470000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>12</td>\n",
       "      <td>1.756067</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>242.580000</td>\n",
       "      <td>7.246667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.016667</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>13</td>\n",
       "      <td>1.682067</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>241.954167</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>17.366667</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>14</td>\n",
       "      <td>1.733033</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>242.196500</td>\n",
       "      <td>7.096667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.383333</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>15</td>\n",
       "      <td>1.784300</td>\n",
       "      <td>0.104067</td>\n",
       "      <td>242.423500</td>\n",
       "      <td>7.310000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>17.983333</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2006-12-18</td>\n",
       "      <td>16</td>\n",
       "      <td>1.949300</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>242.380667</td>\n",
       "      <td>8.096667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.983333</td>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date2  TimeHrs  Global_active_power  Global_reactive_power  \\\n",
       "0  2006-12-16       17             4.222889               0.229000   \n",
       "1  2006-12-16       18             3.632200               0.080033   \n",
       "2  2006-12-16       19             3.400233               0.085233   \n",
       "3  2006-12-16       20             3.268567               0.075100   \n",
       "4  2006-12-16       21             3.056467               0.076667   \n",
       "5  2006-12-16       22             2.200133               0.056167   \n",
       "6  2006-12-16       23             2.061600               0.071433   \n",
       "7  2006-12-17        0             1.882467               0.102433   \n",
       "8  2006-12-17        1             3.349400               0.136933   \n",
       "9  2006-12-17        2             1.587267               0.078233   \n",
       "10 2006-12-17        3             1.662200               0.079533   \n",
       "11 2006-12-17        4             2.215767               0.093467   \n",
       "12 2006-12-17        5             1.996733               0.060233   \n",
       "13 2006-12-17        6             1.303300               0.094833   \n",
       "14 2006-12-17        7             1.620033               0.059800   \n",
       "15 2006-12-17        8             1.890567               0.118567   \n",
       "16 2006-12-17        9             2.549067               0.079233   \n",
       "17 2006-12-17       10             3.628900               0.194600   \n",
       "18 2006-12-17       11             2.471000               0.200700   \n",
       "19 2006-12-17       12             1.915867               0.294033   \n",
       "20 2006-12-17       13             1.660767               0.171533   \n",
       "21 2006-12-17       14             2.092633               0.257400   \n",
       "22 2006-12-17       15             2.985400               0.088600   \n",
       "23 2006-12-17       16             3.326033               0.093133   \n",
       "24 2006-12-17       17             3.406767               0.166633   \n",
       "25 2006-12-17       18             3.697100               0.135067   \n",
       "26 2006-12-17       19             2.908400               0.265167   \n",
       "27 2006-12-17       20             3.361500               0.271500   \n",
       "28 2006-12-17       21             3.040767               0.267967   \n",
       "29 2006-12-17       22             1.518000               0.235367   \n",
       "30 2006-12-17       23             0.437733               0.221800   \n",
       "31 2006-12-18        0             0.276367               0.099067   \n",
       "32 2006-12-18        1             0.313300               0.151900   \n",
       "33 2006-12-18        2             0.284467               0.109067   \n",
       "34 2006-12-18        3             0.309933               0.147767   \n",
       "35 2006-12-18        4             1.026333               0.080000   \n",
       "36 2006-12-18        5             0.293500               0.121933   \n",
       "37 2006-12-18        6             0.610000               0.112900   \n",
       "38 2006-12-18        7             2.450433               0.153000   \n",
       "39 2006-12-18        8             2.082133               0.073733   \n",
       "40 2006-12-18        9             1.629333               0.085167   \n",
       "41 2006-12-18       10             1.309633               0.052833   \n",
       "42 2006-12-18       11             1.561933               0.127333   \n",
       "43 2006-12-18       12             1.756067               0.087300   \n",
       "44 2006-12-18       13             1.682067               0.126500   \n",
       "45 2006-12-18       14             1.733033               0.049300   \n",
       "46 2006-12-18       15             1.784300               0.104067   \n",
       "47 2006-12-18       16             1.949300               0.204433   \n",
       "\n",
       "       Voltage  Global_intensity  Sub_metering_1  Sub_metering_2  \\\n",
       "0   234.643889         18.100000        0.000000        0.527778   \n",
       "1   234.580167         15.600000        0.000000        6.716667   \n",
       "2   233.232500         14.503333        0.000000        1.433333   \n",
       "3   234.071500         13.916667        0.000000        0.000000   \n",
       "4   237.158667         13.046667        0.000000        0.416667   \n",
       "5   238.760000          9.523333        0.000000        0.133333   \n",
       "6   240.619667          8.896667        0.000000        0.083333   \n",
       "7   240.961833          8.126667        0.000000        0.466667   \n",
       "8   240.448333         14.246667        0.000000       25.233333   \n",
       "9   245.818667          6.870000        0.000000        0.566667   \n",
       "10  244.513500          7.206667        0.000000        0.766667   \n",
       "11  243.855500          9.333333        0.000000        0.566667   \n",
       "12  243.710167          8.566667        0.000000        0.166667   \n",
       "13  244.141500          5.530000        0.000000        0.716667   \n",
       "14  241.845667          6.803333        0.000000        0.166667   \n",
       "15  241.311500          8.050000        0.000000        0.750000   \n",
       "16  238.847833         10.973333        0.000000        7.316667   \n",
       "17  235.441000         15.533333        8.883333       22.350000   \n",
       "18  236.924667         10.493333        9.450000        5.016667   \n",
       "19  237.784667          8.136667        0.000000        0.533333   \n",
       "20  239.920667          6.923333        0.000000        0.050000   \n",
       "21  244.045667          8.600000        0.000000        0.333333   \n",
       "22  240.480833         12.426667        6.966667        0.200000   \n",
       "23  236.781833         14.006667        8.583333        0.066667   \n",
       "24  234.229833         14.510000        0.000000        0.466667   \n",
       "25  234.372333         15.750000        0.000000        0.000000   \n",
       "26  233.195667         12.516667        0.000000        0.516667   \n",
       "27  236.426500         14.276667        0.000000        1.116667   \n",
       "28  239.104167         12.716667        0.000000        1.200000   \n",
       "29  242.192333          6.383333        0.000000        0.416667   \n",
       "30  245.734000          1.996667        0.000000        0.800000   \n",
       "31  244.607500          1.223333        0.000000        0.033333   \n",
       "32  243.519333          1.416667        0.000000        0.583333   \n",
       "33  246.891000          1.256667        0.000000        0.000000   \n",
       "34  245.731833          1.400000        0.000000        0.566667   \n",
       "35  245.507000          4.176667        0.000000        0.066667   \n",
       "36  245.253167          1.293333        0.000000        0.466667   \n",
       "37  245.704333          2.600000        0.000000        0.083333   \n",
       "38  240.962333         10.220000        0.083333        0.450000   \n",
       "39  237.880333          8.726667       12.466667        0.033333   \n",
       "40  237.404667          6.816667        5.166667        0.483333   \n",
       "41  239.080500          5.426667        0.000000        0.000000   \n",
       "42  239.771000          6.470000        0.000000        0.516667   \n",
       "43  242.580000          7.246667        0.000000        0.000000   \n",
       "44  241.954167          6.940000        0.000000        0.533333   \n",
       "45  242.196500          7.096667        0.000000        0.000000   \n",
       "46  242.423500          7.310000        0.000000        0.516667   \n",
       "47  242.380667          8.096667        0.000000        0.000000   \n",
       "\n",
       "    Sub_metering_3  distanceFromLastSolstice  distanceFromNextSolstice  \n",
       "0        16.861111                       178                         6  \n",
       "1        16.866667                       178                         6  \n",
       "2        16.683333                       178                         6  \n",
       "3        16.783333                       178                         6  \n",
       "4        17.216667                       178                         6  \n",
       "5         4.433333                       178                         6  \n",
       "6         0.000000                       178                         6  \n",
       "7         0.000000                       179                         5  \n",
       "8         0.000000                       179                         5  \n",
       "9         0.000000                       179                         5  \n",
       "10        0.000000                       179                         5  \n",
       "11        8.883333                       179                         5  \n",
       "12        4.650000                       179                         5  \n",
       "13        0.000000                       179                         5  \n",
       "14        0.000000                       179                         5  \n",
       "15        0.000000                       179                         5  \n",
       "16        0.000000                       179                         5  \n",
       "17       16.150000                       179                         5  \n",
       "18       17.183333                       179                         5  \n",
       "19       17.316667                       179                         5  \n",
       "20       17.616667                       179                         5  \n",
       "21       18.233333                       179                         5  \n",
       "22       17.700000                       179                         5  \n",
       "23       17.166667                       179                         5  \n",
       "24       16.816667                       179                         5  \n",
       "25       16.833333                       179                         5  \n",
       "26       16.683333                       179                         5  \n",
       "27       17.116667                       179                         5  \n",
       "28       17.500000                       179                         5  \n",
       "29        2.500000                       179                         5  \n",
       "30        0.000000                       179                         5  \n",
       "31        0.000000                       180                         4  \n",
       "32        0.000000                       180                         4  \n",
       "33        0.000000                       180                         4  \n",
       "34        0.000000                       180                         4  \n",
       "35       12.550000                       180                         4  \n",
       "36        0.000000                       180                         4  \n",
       "37        0.000000                       180                         4  \n",
       "38       11.300000                       180                         4  \n",
       "39       17.350000                       180                         4  \n",
       "40       17.266667                       180                         4  \n",
       "41       17.500000                       180                         4  \n",
       "42       17.600000                       180                         4  \n",
       "43       18.016667                       180                         4  \n",
       "44       17.366667                       180                         4  \n",
       "45       17.383333                       180                         4  \n",
       "46       17.983333                       180                         4  \n",
       "47       17.983333                       180                         4  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedDF.head(48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix all NaN values before we start writing this to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDF.fillna(value=\"NaN\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the training data \n",
    "\n",
    "trdata = []\n",
    "\n",
    "## Category for Global Active Power, Sub-Meter-1,2,3 -> 0\n",
    "starttime = str(processedDF.iloc[0]['Date2'])[:10] +' '+str(processedDF.iloc[0]['TimeHrs'])+':00:00'\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Global_active_power'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['distanceFromLastSolstice'].tolist(),processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['distanceFromNextSolstice'].tolist()]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-1\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_1'].tolist()\n",
    "cat = [0,1]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_2'].tolist()\n",
    "cat = [0,2]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Sub_metering_3'].tolist()\n",
    "cat = [0,3]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Global_reactive_power'].tolist()\n",
    "cat = [1,4]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "target = processedDF[processedDF['Date2'] < pd.to_datetime('2008-01').date()]['Voltage'].tolist()\n",
    "cat = [2,5]\n",
    "trdata.append({\"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat})\n",
    "\n",
    "random.shuffle(trdata)\n",
    "\n",
    "f = open(\"trainingdata.json\",'wb')\n",
    "for datapoint in trdata:\n",
    "    f.write(json.dumps(datapoint).encode(\"utf-8\"))\n",
    "    f.write(\"\\n\".encode('utf-8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the test data\n",
    "\n",
    "tedata = []\n",
    "\n",
    "## Category for Global Active Power, Sub-Meter-1,2,3 -> 0\n",
    "\n",
    "starttime = str(processedDF[(processedDF['Date2'] == '2008-01')].iloc[0]['Date2'])[:10]+' '+ '0' + str(processedDF[processedDF['Date2'] == '2008-01'].iloc[0]['TimeHrs'])+':00:00'\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Global_active_power'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['distanceFromLastSolstice'].tolist(),processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['distanceFromNextSolstice'].tolist()]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-1\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_1'].tolist()\n",
    "cat = [0,1]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_2'].tolist()\n",
    "cat = [0,2]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "## llly, for sub-meter-2\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Sub_metering_3'].tolist()\n",
    "cat = [0,3]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Global_reactive_power'].tolist()\n",
    "cat = [1,4]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-01') & (processedDF['Date2'] < '2008-07')]['Voltage'].tolist()\n",
    "cat = [2,5]\n",
    "tedata.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "f = open(\"testdata.json\",'wb')\n",
    "for datapoint in tedata:\n",
    "    f.write(json.dumps(datapoint).encode(\"utf-8\"))\n",
    "    f.write(\"\\n\".encode('utf-8'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_key = 'trainingdata.json'\n",
    "testing_key = 'testdata.json'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix = '{}/{}'.format(prefix,'test')\n",
    "\n",
    "## Lets create the sagemaker session and upload the data from where Amazon SageMaker will pick it \n",
    "## up to put in the container\n",
    "sg_sess = Session()\n",
    "training_path  = sg_sess.upload_data(training_key, bucket=bucket, key_prefix=train_prefix)\n",
    "testing_path = sg_sess.upload_data(testing_key, bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "img = get_image_uri(boto3.Session().region_name,'forecasting-deepar')\n",
    "prediction_length = 24 # hours ahead\n",
    "context_length = 216 # hours prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DevDay-DeepARTraining-2018-10-01-15-25-01\n",
      "training artifacts will be uploaded to: s3://rnszsdemo/sagemaker/data/Household_Electricity_Consumption/output\n",
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = 'DevDay-DeepARTraining-' + strftime(\"%Y-%m-%d-%H-%M-%S\",gmtime())\n",
    "print(job_name)\n",
    "                                \n",
    "## We have already setup the containers above\n",
    "                                \n",
    "## We choose a different output location to keep these separate from our old artifacts\n",
    "output_location = 's3://{}/{}/output'.format(bucket,prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "create_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": img,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": roleARN,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 4,\n",
    "        \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"time_freq\": 'H', # hourly series\n",
    "        \"context_length\": str(context_length),\n",
    "        \"prediction_length\": str(prediction_length), # number of data points to predict\n",
    "        \"num_dynamic_feat\": \"auto\",\n",
    "        \"num_cells\": \"60\", \n",
    "        \"num_layers\": \"3\",\n",
    "        \"likelihood\": \"gaussian\", #real world data\n",
    "        \"epochs\": \"15\", ## keep small, to save on time!!! 250 seems to give good results though\n",
    "        \"mini_batch_size\": \"128\", ## keep large, to save on time!!! 128 seems to give good results though\n",
    "        \"learning_rate\": \"0.01\",\n",
    "        \"dropout_rate\": \"0.1\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 240 # Give it four hours at best, could increase this for production scale\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": training_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"test\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": testing_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sgmaker = boto3.client('sagemaker')\n",
    "sgmaker.create_training_job(**create_training_params)\n",
    "\n",
    "## Lets check the status of the job to see if its complete, and lets wait until its done\n",
    "status = sgmaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "\n",
    "## You can either wait for the job, like below, or do what we do in the next cell\n",
    "#try:\n",
    "#    sgmaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "#finally:\n",
    "#    status = sgmaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "#    print(\"Training job ended with status: \" + status)\n",
    "#    if status == 'Failed':\n",
    "#        message = sgmaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "#        print('Training failed with the following error: {}'.format(message))\n",
    "#        raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 15:25:01 Starting - Launching requested ML instances............\n",
      "Preparing the instances for training......\n",
      "2018-10-01 15:28:00 Downloading - Downloading input data\n",
      "2018-10-01 15:28:05 Training - Downloading the training image.\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.1', u'learning_rate': u'0.01', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'250', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'3', u'mini_batch_size': u'128', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.01', u'num_layers': u'3', u'epochs': u'250', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d097495a-072b-4742-831a-59092d50d37e', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/9ae0313c-ce58-4df4-bafa-deba29c2cdf0', 'PWD': '/'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d097495a-072b-4742-831a-59092d50d37e', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/9ae0313c-ce58-4df4-bafa-deba29c2cdf0', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d097495a-072b-4742-831a-59092d50d37e', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/9ae0313c-ce58-4df4-bafa-deba29c2cdf0', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Training set statistics:\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Real time series\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] number of time series: 6\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] number of observations: 54762\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] mean target length: 9127\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Test set statistics:\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] Real time series\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] number of time series: 6\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] number of observations: 26208\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] mean target length: 4368\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:11 INFO 140196492334912] contains missing values: no\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:12 INFO 140196492334912] nvidia-smi took: 0.0252020359039 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:12 INFO 140196492334912] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:12 INFO 140196492334912] Create Store: dist_async\u001b[0m\n",
      "\u001b[33mArguments: train\u001b[0m\n",
      "\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.1', u'learning_rate': u'0.01', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'250', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'3', u'mini_batch_size': u'128', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.01', u'num_layers': u'3', u'epochs': u'250', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d1579a69-aa34-47f9-82ce-438c9746cbfc', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1ddf76b-3172-4182-ae20-adf39d7508ef', 'PWD': '/'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d1579a69-aa34-47f9-82ce-438c9746cbfc', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1ddf76b-3172-4182-ae20-adf39d7508ef', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/d1579a69-aa34-47f9-82ce-438c9746cbfc', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/c1ddf76b-3172-4182-ae20-adf39d7508ef', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Training set statistics:\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Real time series\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] number of time series: 6\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] number of observations: 54762\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] mean target length: 9127\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Test set statistics:\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Real time series\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] number of time series: 6\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] number of observations: 26208\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] mean target length: 4368\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] contains missing values: no\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] nvidia-smi took: 0.0252449512482 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:11 INFO 139663579178816] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.1', u'learning_rate': u'0.01', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'250', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'3', u'mini_batch_size': u'128', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.01', u'num_layers': u'3', u'epochs': u'250', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e37ddccc-80ed-4e61-8a3b-1aca2ef5a131', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/a279cd04-07eb-4e77-bbb6-6fad6e98bbe3', 'PWD': '/'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e37ddccc-80ed-4e61-8a3b-1aca2ef5a131', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/a279cd04-07eb-4e77-bbb6-6fad6e98bbe3', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e37ddccc-80ed-4e61-8a3b-1aca2ef5a131', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/a279cd04-07eb-4e77-bbb6-6fad6e98bbe3', 'PWD': '/'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e37ddccc-80ed-4e61-8a3b-1aca2ef5a131', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/a279cd04-07eb-4e77-bbb6-6fad6e98bbe3', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/e37ddccc-80ed-4e61-8a3b-1aca2ef5a131', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/a279cd04-07eb-4e77-bbb6-6fad6e98bbe3', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:12 INFO 140252856112960] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Training set statistics:\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Real time series\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] number of time series: 6\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] number of observations: 54762\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] mean target length: 9127\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.1', u'learning_rate': u'0.01', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'250', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'3', u'mini_batch_size': u'128', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Final configuration: {u'dropout_rate': u'0.1', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.01', u'num_layers': u'3', u'epochs': u'250', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Launching parameter server for role server\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8dc60d92-3136-4ba4-a08a-58766e90edc9', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/292b92a0-e64b-46a6-b688-5a56993d71b3', 'PWD': '/'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8dc60d92-3136-4ba4-a08a-58766e90edc9', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/292b92a0-e64b-46a6-b688-5a56993d71b3', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8dc60d92-3136-4ba4-a08a-58766e90edc9', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.40.0.3', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'DevDay-DeepARTraining-2018-10-01-15-25-01', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/292b92a0-e64b-46a6-b688-5a56993d71b3', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Training set statistics:\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Real time series\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] number of time series: 6\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] number of observations: 54762\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] mean target length: 9127\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Test set statistics:\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Real time series\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] number of time series: 6\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] number of observations: 26208\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] mean target length: 4368\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] contains missing values: no\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] nvidia-smi took: 0.0251998901367 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:13 INFO 140624803325760] Create Store: dist_async\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Test set statistics:\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Real time series\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] number of time series: 6\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] number of observations: 26208\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] mean target length: 4368\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] contains missing values: no\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] nvidia-smi took: 0.0251860618591 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:13 INFO 140252856112960] Create Store: dist_async\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training image download completed. Training in progress.\u001b[32m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 14911.645889282227, \"sum\": 14911.645889282227, \"min\": 14911.645889282227}}, \"EndTime\": 1538407708.934768, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407691.416248}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:28 INFO 139663579178816] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 14934.770822525024, \"sum\": 14934.770822525024, \"min\": 14934.770822525024}}, \"EndTime\": 1538407708.955876, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407692.004713}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:28 INFO 140196492334912] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 15029.124021530151, \"sum\": 15029.124021530151, \"min\": 15029.124021530151}}, \"EndTime\": 1538407709.052334, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407693.475587}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:29 INFO 140624803325760] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 14986.824989318848, \"sum\": 14986.824989318848, \"min\": 14986.824989318848}}, \"EndTime\": 1538407709.010054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407693.157796}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:29 INFO 140252856112960] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.001953125 vs. 0.0078125). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.001953125 vs. 0.0078125). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 18173.090934753418, \"sum\": 18173.090934753418, \"min\": 18173.090934753418}}, \"EndTime\": 1538407710.177893, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407708.956021}\n",
      "\u001b[0m\n",
      "\u001b[33m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.001953125 vs. 0.0078125). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 16704.538106918335, \"sum\": 16704.538106918335, \"min\": 16704.538106918335}}, \"EndTime\": 1538407710.180214, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407709.05248}\n",
      "\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.001953125 vs. 0.0078125). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 17022.85599708557, \"sum\": 17022.85599708557, \"min\": 17022.85599708557}}, \"EndTime\": 1538407710.180748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407709.010196}\n",
      "\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 18764.30106163025, \"sum\": 18764.30106163025, \"min\": 18764.30106163025}}, \"EndTime\": 1538407710.180653, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407708.934912}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:32 INFO 139663579178816] Epoch[0] Batch[0] avg_epoch_loss=6.829250\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:32 INFO 140196492334912] Epoch[0] Batch[0] avg_epoch_loss=5.534469\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:32 INFO 140624803325760] Epoch[0] Batch[0] avg_epoch_loss=7.007420\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:32 INFO 140252856112960] Epoch[0] Batch[0] avg_epoch_loss=6.028242\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:36 INFO 140624803325760] Epoch[0] Batch[5] avg_epoch_loss=3.910003\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:36 INFO 140624803325760] Epoch[0] Batch [5]#011Speed: 170.76 samples/sec#011loss=3.910003\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:36 INFO 139663579178816] Epoch[0] Batch[5] avg_epoch_loss=4.810498\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:36 INFO 139663579178816] Epoch[0] Batch [5]#011Speed: 166.11 samples/sec#011loss=4.810498\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:36 INFO 140196492334912] Epoch[0] Batch[5] avg_epoch_loss=3.681534\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:36 INFO 140196492334912] Epoch[0] Batch [5]#011Speed: 162.05 samples/sec#011loss=3.681534\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:36 INFO 140252856112960] Epoch[0] Batch[5] avg_epoch_loss=3.799681\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:36 INFO 140252856112960] Epoch[0] Batch [5]#011Speed: 162.90 samples/sec#011loss=3.799681\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:40 INFO 140624803325760] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 250, \"sum\": 250.0, \"min\": 250}, \"update.time\": {\"count\": 1, \"max\": 9884.392976760864, \"sum\": 9884.392976760864, \"min\": 9884.392976760864}}, \"EndTime\": 1538407720.064793, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407710.180269}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.359234929 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:40 INFO 140252856112960] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 250, \"sum\": 250.0, \"min\": 250}, \"update.time\": {\"count\": 1, \"max\": 10218.38092803955, \"sum\": 10218.38092803955, \"min\": 10218.38092803955}}, \"EndTime\": 1538407720.399269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407710.180801}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.348464938 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:40 INFO 139663579178816] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 250, \"sum\": 250.0, \"min\": 250}, \"update.time\": {\"count\": 1, \"max\": 10079.632043838501, \"sum\": 10079.632043838501, \"min\": 10079.632043838501}}, \"EndTime\": 1538407720.260434, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407710.180708}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:40 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.828334059 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:40 INFO 139663579178816] #progress_metric: host=algo-2, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:42 INFO 140196492334912] Epoch[0] Batch[10] avg_epoch_loss=3.413324\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:42 INFO 140196492334912] Epoch[0] Batch [10]#011Speed: 121.75 samples/sec#011loss=3.091473\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:42 INFO 140196492334912] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 250, \"sum\": 250.0, \"min\": 250}, \"update.time\": {\"count\": 1, \"max\": 11842.275857925415, \"sum\": 11842.275857925415, \"min\": 11842.275857925415}}, \"EndTime\": 1538407722.020327, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407710.17795}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:42 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.606060929 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:42 INFO 140196492334912] #progress_metric: host=algo-4, completed 0 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:42 INFO 140624803325760] Epoch[1] Batch[0] avg_epoch_loss=3.295697\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:42 INFO 139663579178816] Epoch[1] Batch[0] avg_epoch_loss=3.153143\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:42 INFO 140252856112960] Epoch[1] Batch[0] avg_epoch_loss=3.578915\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:43 INFO 140196492334912] Epoch[1] Batch[0] avg_epoch_loss=3.123432\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:46 INFO 140624803325760] Epoch[1] Batch[5] avg_epoch_loss=3.344803\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:46 INFO 140624803325760] Epoch[1] Batch [5]#011Speed: 155.49 samples/sec#011loss=3.344803\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:28:46 INFO 139663579178816] Epoch[1] Batch[5] avg_epoch_loss=3.158150\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:46 INFO 139663579178816] Epoch[1] Batch [5]#011Speed: 163.66 samples/sec#011loss=3.158150\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:47 INFO 140252856112960] Epoch[1] Batch[5] avg_epoch_loss=3.296534\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:47 INFO 140252856112960] Epoch[1] Batch [5]#011Speed: 146.98 samples/sec#011loss=3.296534\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:48 INFO 140196492334912] Epoch[1] Batch[5] avg_epoch_loss=3.222671\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:48 INFO 140196492334912] Epoch[1] Batch [5]#011Speed: 120.44 samples/sec#011loss=3.222671\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:50 INFO 139663579178816] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9810.744047164917, \"sum\": 9810.744047164917, \"min\": 9810.744047164917}}, \"EndTime\": 1538407730.071515, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407720.260523}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:50 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=128.327259316 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:50 INFO 139663579178816] #progress_metric: host=algo-2, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:51 INFO 140252856112960] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10703.449964523315, \"sum\": 10703.449964523315, \"min\": 10703.449964523315}}, \"EndTime\": 1538407731.103064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407720.399359}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.409793529 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:51 INFO 140624803325760] Epoch[1] Batch[10] avg_epoch_loss=3.299101\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:51 INFO 140624803325760] Epoch[1] Batch [10]#011Speed: 122.99 samples/sec#011loss=3.244258\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:51 INFO 140624803325760] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11609.336853027344, \"sum\": 11609.336853027344, \"min\": 11609.336853027344}}, \"EndTime\": 1538407731.674467, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407720.064883}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.116403713 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 0 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:52 INFO 139663579178816] Epoch[2] Batch[0] avg_epoch_loss=3.187853\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:52 INFO 140196492334912] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10716.23706817627, \"sum\": 10716.23706817627, \"min\": 10716.23706817627}}, \"EndTime\": 1538407732.73691, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407722.020413}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.270628292 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 0 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:53 INFO 140624803325760] Epoch[2] Batch[0] avg_epoch_loss=2.952461\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:53 INFO 140252856112960] Epoch[2] Batch[0] avg_epoch_loss=2.960129\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:54 INFO 140196492334912] Epoch[2] Batch[0] avg_epoch_loss=2.964614\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:56 INFO 139663579178816] Epoch[2] Batch[5] avg_epoch_loss=2.970472\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:28:56 INFO 139663579178816] Epoch[2] Batch [5]#011Speed: 162.19 samples/sec#011loss=2.970472\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:57 INFO 140252856112960] Epoch[2] Batch[5] avg_epoch_loss=3.070736\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:28:57 INFO 140252856112960] Epoch[2] Batch [5]#011Speed: 154.56 samples/sec#011loss=3.070736\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:58 INFO 140624803325760] Epoch[2] Batch[5] avg_epoch_loss=3.024577\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:28:58 INFO 140624803325760] Epoch[2] Batch [5]#011Speed: 128.43 samples/sec#011loss=3.024577\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:59 INFO 140196492334912] Epoch[2] Batch[5] avg_epoch_loss=3.019064\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:28:59 INFO 140196492334912] Epoch[2] Batch [5]#011Speed: 122.40 samples/sec#011loss=3.019064\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:01 INFO 139663579178816] Epoch[2] Batch[10] avg_epoch_loss=2.667571\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:01 INFO 139663579178816] Epoch[2] Batch [10]#011Speed: 122.44 samples/sec#011loss=2.304090\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:01 INFO 139663579178816] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11454.938888549805, \"sum\": 11454.938888549805, \"min\": 11454.938888549805}}, \"EndTime\": 1538407741.526764, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407730.07159}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.582220173 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:02 INFO 140624803325760] Epoch[2] Batch[10] avg_epoch_loss=3.020763\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:02 INFO 140624803325760] Epoch[2] Batch [10]#011Speed: 155.25 samples/sec#011loss=3.016186\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:02 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10765.19799232483, \"sum\": 10765.19799232483, \"min\": 10765.19799232483}}, \"EndTime\": 1538407742.439985, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407731.674543}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:02 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.03702454 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:02 INFO 140624803325760] #progress_metric: host=algo-3, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:03 INFO 140196492334912] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10662.291049957275, \"sum\": 10662.291049957275, \"min\": 10662.291049957275}}, \"EndTime\": 1538407743.39955, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407732.737009}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:03 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.297452249 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:03 INFO 140196492334912] #progress_metric: host=algo-4, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:02 INFO 140252856112960] Epoch[2] Batch[10] avg_epoch_loss=2.917869\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:02 INFO 140252856112960] Epoch[2] Batch [10]#011Speed: 122.07 samples/sec#011loss=2.734429\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:02 INFO 140252856112960] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11777.529001235962, \"sum\": 11777.529001235962, \"min\": 11777.529001235962}}, \"EndTime\": 1538407742.880904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407731.103144}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.020080495 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:03 INFO 139663579178816] Epoch[3] Batch[0] avg_epoch_loss=3.182794\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:04 INFO 140252856112960] Epoch[3] Batch[0] avg_epoch_loss=3.097596\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:05 INFO 140196492334912] Epoch[3] Batch[0] avg_epoch_loss=2.787977\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:04 INFO 140624803325760] Epoch[3] Batch[0] avg_epoch_loss=2.421753\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:08 INFO 140624803325760] Epoch[3] Batch[5] avg_epoch_loss=3.155520\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:08 INFO 140624803325760] Epoch[3] Batch [5]#011Speed: 155.54 samples/sec#011loss=3.155520\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:29:08 INFO 139663579178816] Epoch[3] Batch[5] avg_epoch_loss=3.086981\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:08 INFO 139663579178816] Epoch[3] Batch [5]#011Speed: 125.61 samples/sec#011loss=3.086981\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:10 INFO 140196492334912] Epoch[3] Batch[5] avg_epoch_loss=3.175690\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:10 INFO 140196492334912] Epoch[3] Batch [5]#011Speed: 124.43 samples/sec#011loss=3.175690\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:09 INFO 140252856112960] Epoch[3] Batch[5] avg_epoch_loss=3.092533\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:09 INFO 140252856112960] Epoch[3] Batch [5]#011Speed: 120.78 samples/sec#011loss=3.092533\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:11 INFO 139663579178816] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10414.896965026855, \"sum\": 10414.896965026855, \"min\": 10414.896965026855}}, \"EndTime\": 1538407751.941965, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407741.526841}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:11 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.899523727 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:11 INFO 139663579178816] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:13 INFO 139663579178816] Epoch[4] Batch[0] avg_epoch_loss=2.626457\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:13 INFO 140196492334912] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10489.800930023193, \"sum\": 10489.800930023193, \"min\": 10489.800930023193}}, \"EndTime\": 1538407753.88971, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407743.399641}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:13 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.494560294 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:13 INFO 140196492334912] #progress_metric: host=algo-4, completed 1 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:14 INFO 140624803325760] Epoch[3] Batch[10] avg_epoch_loss=3.177861\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:14 INFO 140624803325760] Epoch[3] Batch [10]#011Speed: 123.04 samples/sec#011loss=3.204671\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:14 INFO 140624803325760] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11755.137920379639, \"sum\": 11755.137920379639, \"min\": 11755.137920379639}}, \"EndTime\": 1538407754.195419, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407742.440053}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:14 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.800672904 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:14 INFO 140624803325760] #progress_metric: host=algo-3, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:14 INFO 140252856112960] Epoch[3] Batch[10] avg_epoch_loss=2.943614\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:14 INFO 140252856112960] Epoch[3] Batch [10]#011Speed: 148.74 samples/sec#011loss=2.764910\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:14 INFO 140252856112960] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11268.929958343506, \"sum\": 11268.929958343506, \"min\": 11268.929958343506}}, \"EndTime\": 1538407754.150148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407742.880988}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.425261711 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:15 INFO 140196492334912] Epoch[4] Batch[0] avg_epoch_loss=2.964207\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:15 INFO 140624803325760] Epoch[4] Batch[0] avg_epoch_loss=2.659874\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:16 INFO 140252856112960] Epoch[4] Batch[0] avg_epoch_loss=2.980943\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:18 INFO 139663579178816] Epoch[4] Batch[5] avg_epoch_loss=3.155940\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:18 INFO 139663579178816] Epoch[4] Batch [5]#011Speed: 124.00 samples/sec#011loss=3.155940\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:20 INFO 140196492334912] Epoch[4] Batch[5] avg_epoch_loss=3.175791\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:20 INFO 140196492334912] Epoch[4] Batch [5]#011Speed: 120.13 samples/sec#011loss=3.175791\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:20 INFO 140624803325760] Epoch[4] Batch[5] avg_epoch_loss=2.989408\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:20 INFO 140624803325760] Epoch[4] Batch [5]#011Speed: 125.57 samples/sec#011loss=2.989408\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:20 INFO 140252856112960] Epoch[4] Batch[5] avg_epoch_loss=3.029200\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:20 INFO 140252856112960] Epoch[4] Batch [5]#011Speed: 149.58 samples/sec#011loss=3.029200\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:23 INFO 139663579178816] Epoch[4] Batch[10] avg_epoch_loss=3.078403\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:23 INFO 139663579178816] Epoch[4] Batch [10]#011Speed: 148.61 samples/sec#011loss=2.985359\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:23 INFO 139663579178816] processed a total of 1344 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11124.54605102539, \"sum\": 11124.54605102539, \"min\": 11124.54605102539}}, \"EndTime\": 1538407763.066849, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407751.942045}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:23 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.812812902 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:23 INFO 139663579178816] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:24 INFO 140196492334912] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10890.632152557373, \"sum\": 10890.632152557373, \"min\": 10890.632152557373}}, \"EndTime\": 1538407764.780701, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407753.889799}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:24 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.674302393 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:24 INFO 140196492334912] #progress_metric: host=algo-4, completed 2 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:25 INFO 140624803325760] Epoch[4] Batch[10] avg_epoch_loss=3.058003\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:25 INFO 140624803325760] Epoch[4] Batch [10]#011Speed: 149.99 samples/sec#011loss=3.140317\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:25 INFO 140624803325760] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11038.568019866943, \"sum\": 11038.568019866943, \"min\": 11038.568019866943}}, \"EndTime\": 1538407765.234298, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407754.195496}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:25 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.670202872 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:25 INFO 140624803325760] #progress_metric: host=algo-3, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:24 INFO 140252856112960] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10633.22401046753, \"sum\": 10633.22401046753, \"min\": 10633.22401046753}}, \"EndTime\": 1538407764.783665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407754.150216}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:24 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.050106087 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:24 INFO 140252856112960] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:25 INFO 139663579178816] Epoch[5] Batch[0] avg_epoch_loss=3.086942\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:29 INFO 139663579178816] Epoch[5] Batch[5] avg_epoch_loss=3.098584\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:29 INFO 139663579178816] Epoch[5] Batch [5]#011Speed: 160.59 samples/sec#011loss=3.098584\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:26 INFO 140196492334912] Epoch[5] Batch[0] avg_epoch_loss=3.342417\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:27 INFO 140624803325760] Epoch[5] Batch[0] avg_epoch_loss=3.324065\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:27 INFO 140252856112960] Epoch[5] Batch[0] avg_epoch_loss=3.335038\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:31 INFO 140196492334912] Epoch[5] Batch[5] avg_epoch_loss=2.978735\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:31 INFO 140196492334912] Epoch[5] Batch [5]#011Speed: 120.30 samples/sec#011loss=2.978735\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:31 INFO 140624803325760] Epoch[5] Batch[5] avg_epoch_loss=2.942554\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:31 INFO 140624803325760] Epoch[5] Batch [5]#011Speed: 153.19 samples/sec#011loss=2.942554\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:31 INFO 140252856112960] Epoch[5] Batch[5] avg_epoch_loss=3.078138\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:31 INFO 140252856112960] Epoch[5] Batch [5]#011Speed: 146.31 samples/sec#011loss=3.078138\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:33 INFO 139663579178816] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9971.220970153809, \"sum\": 9971.220970153809, \"min\": 9971.220970153809}}, \"EndTime\": 1538407773.038374, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407763.066917}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.560056432 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:35 INFO 139663579178816] Epoch[6] Batch[0] avg_epoch_loss=2.647138\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:36 INFO 140196492334912] Epoch[5] Batch[10] avg_epoch_loss=2.568669\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:36 INFO 140196492334912] Epoch[5] Batch [10]#011Speed: 146.95 samples/sec#011loss=2.076589\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:36 INFO 140196492334912] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11362.407922744751, \"sum\": 11362.407922744751, \"min\": 11362.407922744751}}, \"EndTime\": 1538407776.143442, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407764.78079}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:36 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.55544208 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:36 INFO 140196492334912] #progress_metric: host=algo-4, completed 2 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:29:37 INFO 140624803325760] Epoch[5] Batch[10] avg_epoch_loss=2.749562\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:37 INFO 140624803325760] Epoch[5] Batch [10]#011Speed: 120.62 samples/sec#011loss=2.517971\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:37 INFO 140624803325760] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11842.89002418518, \"sum\": 11842.89002418518, \"min\": 11842.89002418518}}, \"EndTime\": 1538407777.077486, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407765.234368}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.756293586 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:37 INFO 140252856112960] Epoch[5] Batch[10] avg_epoch_loss=3.225192\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:37 INFO 140252856112960] Epoch[5] Batch [10]#011Speed: 116.68 samples/sec#011loss=3.401658\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:37 INFO 140252856112960] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12389.489889144897, \"sum\": 12389.489889144897, \"min\": 12389.489889144897}}, \"EndTime\": 1538407777.173472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407764.783745}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:37 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=103.877503246 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:37 INFO 140252856112960] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:38 INFO 140196492334912] Epoch[6] Batch[0] avg_epoch_loss=2.782091\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:38 INFO 140624803325760] Epoch[6] Batch[0] avg_epoch_loss=2.567755\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:38 INFO 140252856112960] Epoch[6] Batch[0] avg_epoch_loss=3.106349\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:39 INFO 139663579178816] Epoch[6] Batch[5] avg_epoch_loss=2.692309\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:39 INFO 139663579178816] Epoch[6] Batch [5]#011Speed: 155.17 samples/sec#011loss=2.692309\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:42 INFO 140196492334912] Epoch[6] Batch[5] avg_epoch_loss=2.890279\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:42 INFO 140196492334912] Epoch[6] Batch [5]#011Speed: 147.87 samples/sec#011loss=2.890279\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:43 INFO 140624803325760] Epoch[6] Batch[5] avg_epoch_loss=3.034683\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:43 INFO 140624803325760] Epoch[6] Batch [5]#011Speed: 122.38 samples/sec#011loss=3.034683\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:44 INFO 140252856112960] Epoch[6] Batch[5] avg_epoch_loss=3.070902\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:44 INFO 140252856112960] Epoch[6] Batch [5]#011Speed: 117.24 samples/sec#011loss=3.070902\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:44 INFO 139663579178816] Epoch[6] Batch[10] avg_epoch_loss=3.115274\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:44 INFO 139663579178816] Epoch[6] Batch [10]#011Speed: 120.50 samples/sec#011loss=3.622831\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:44 INFO 139663579178816] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11727.051973342896, \"sum\": 11727.051973342896, \"min\": 11727.051973342896}}, \"EndTime\": 1538407784.76572, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407773.038444}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.194811002 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:46 INFO 139663579178816] Epoch[7] Batch[0] avg_epoch_loss=2.719737\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:46 INFO 140196492334912] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10538.742065429688, \"sum\": 10538.742065429688, \"min\": 10538.742065429688}}, \"EndTime\": 1538407786.682474, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407776.14351}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:46 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.74739915 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:46 INFO 140196492334912] #progress_metric: host=algo-4, completed 2 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:48 INFO 140624803325760] Epoch[6] Batch[10] avg_epoch_loss=2.989535\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:48 INFO 140624803325760] Epoch[6] Batch [10]#011Speed: 147.54 samples/sec#011loss=2.935357\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:48 INFO 140624803325760] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11212.822914123535, \"sum\": 11212.822914123535, \"min\": 11212.822914123535}}, \"EndTime\": 1538407788.290611, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407777.077556}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.86742182 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:48 INFO 140252856112960] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11153.30696105957, \"sum\": 11153.30696105957, \"min\": 11153.30696105957}}, \"EndTime\": 1538407788.327082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407777.173542}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:48 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.404444695 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:48 INFO 140252856112960] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:49 INFO 140196492334912] Epoch[7] Batch[0] avg_epoch_loss=2.742874\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:49 INFO 140252856112960] Epoch[7] Batch[0] avg_epoch_loss=2.665170\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:50 INFO 140624803325760] Epoch[7] Batch[0] avg_epoch_loss=2.570078\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:51 INFO 139663579178816] Epoch[7] Batch[5] avg_epoch_loss=2.756462\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:51 INFO 139663579178816] Epoch[7] Batch [5]#011Speed: 119.67 samples/sec#011loss=2.756462\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:53 INFO 140196492334912] Epoch[7] Batch[5] avg_epoch_loss=2.878760\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:53 INFO 140196492334912] Epoch[7] Batch [5]#011Speed: 148.63 samples/sec#011loss=2.878760\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:54 INFO 140624803325760] Epoch[7] Batch[5] avg_epoch_loss=2.662422\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:54 INFO 140624803325760] Epoch[7] Batch [5]#011Speed: 150.25 samples/sec#011loss=2.662422\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:55 INFO 140252856112960] Epoch[7] Batch[5] avg_epoch_loss=2.768549\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:55 INFO 140252856112960] Epoch[7] Batch [5]#011Speed: 115.48 samples/sec#011loss=2.768549\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:56 INFO 139663579178816] Epoch[7] Batch[10] avg_epoch_loss=2.779165\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:56 INFO 139663579178816] Epoch[7] Batch [10]#011Speed: 140.94 samples/sec#011loss=2.806410\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:56 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11622.123003005981, \"sum\": 11622.123003005981, \"min\": 11622.123003005981}}, \"EndTime\": 1538407796.38815, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407784.765791}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:56 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.305692362 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:56 INFO 139663579178816] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:29:58 INFO 139663579178816] Epoch[8] Batch[0] avg_epoch_loss=3.013676\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:59 INFO 140252856112960] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11148.778915405273, \"sum\": 11148.778915405273, \"min\": 11148.778915405273}}, \"EndTime\": 1538407799.476176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407788.327155}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:59 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.118796722 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:29:59 INFO 140252856112960] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:29:58 INFO 140196492334912] Epoch[7] Batch[10] avg_epoch_loss=2.797138\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:58 INFO 140196492334912] Epoch[7] Batch [10]#011Speed: 116.57 samples/sec#011loss=2.699192\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:58 INFO 140196492334912] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12191.917181015015, \"sum\": 12191.917181015015, \"min\": 12191.917181015015}}, \"EndTime\": 1538407798.874703, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407786.682547}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:58 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.923812877 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:29:58 INFO 140196492334912] #progress_metric: host=algo-4, completed 3 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:58 INFO 140624803325760] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10475.82197189331, \"sum\": 10475.82197189331, \"min\": 10475.82197189331}}, \"EndTime\": 1538407798.766729, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407788.290681}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.084839736 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:29:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:00 INFO 140196492334912] Epoch[8] Batch[0] avg_epoch_loss=2.616735\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:01 INFO 140624803325760] Epoch[8] Batch[0] avg_epoch_loss=2.743118\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:01 INFO 140252856112960] Epoch[8] Batch[0] avg_epoch_loss=2.836623\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:03 INFO 139663579178816] Epoch[8] Batch[5] avg_epoch_loss=2.684718\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:03 INFO 139663579178816] Epoch[8] Batch [5]#011Speed: 148.93 samples/sec#011loss=2.684718\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:05 INFO 140624803325760] Epoch[8] Batch[5] avg_epoch_loss=2.498006\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:05 INFO 140624803325760] Epoch[8] Batch [5]#011Speed: 151.93 samples/sec#011loss=2.498006\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:05 INFO 140196492334912] Epoch[8] Batch[5] avg_epoch_loss=2.672584\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:05 INFO 140196492334912] Epoch[8] Batch [5]#011Speed: 117.34 samples/sec#011loss=2.672584\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:06 INFO 140252856112960] Epoch[8] Batch[5] avg_epoch_loss=2.663908\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:06 INFO 140252856112960] Epoch[8] Batch [5]#011Speed: 115.75 samples/sec#011loss=2.663908\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:06 INFO 139663579178816] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10579.034090042114, \"sum\": 10579.034090042114, \"min\": 10579.034090042114}}, \"EndTime\": 1538407806.967517, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407796.388234}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.629376478 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:10 INFO 140196492334912] Epoch[8] Batch[10] avg_epoch_loss=2.605053\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:10 INFO 140196492334912] Epoch[8] Batch [10]#011Speed: 142.79 samples/sec#011loss=2.524016\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:10 INFO 140196492334912] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11530.16996383667, \"sum\": 11530.16996383667, \"min\": 11530.16996383667}}, \"EndTime\": 1538407810.40513, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407798.874759}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.700681332 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:10 INFO 140252856112960] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11175.2028465271, \"sum\": 11175.2028465271, \"min\": 11175.2028465271}}, \"EndTime\": 1538407810.651727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407799.47625}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.495439849 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:09 INFO 139663579178816] Epoch[9] Batch[0] avg_epoch_loss=2.400013\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:10 INFO 140624803325760] Epoch[8] Batch[10] avg_epoch_loss=2.849176\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:10 INFO 140624803325760] Epoch[8] Batch [10]#011Speed: 117.09 samples/sec#011loss=3.270579\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:10 INFO 140624803325760] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12049.049854278564, \"sum\": 12049.049854278564, \"min\": 12049.049854278564}}, \"EndTime\": 1538407810.81608, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407798.766799}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=107.725217487 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 3 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:12 INFO 140624803325760] Epoch[9] Batch[0] avg_epoch_loss=2.641566\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:12 INFO 140252856112960] Epoch[9] Batch[0] avg_epoch_loss=2.527692\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:12 INFO 140196492334912] Epoch[9] Batch[0] avg_epoch_loss=2.480082\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:13 INFO 139663579178816] Epoch[9] Batch[5] avg_epoch_loss=2.532413\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:13 INFO 139663579178816] Epoch[9] Batch [5]#011Speed: 153.17 samples/sec#011loss=2.532413\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:17 INFO 140196492334912] Epoch[9] Batch[5] avg_epoch_loss=2.757890\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:17 INFO 140196492334912] Epoch[9] Batch [5]#011Speed: 149.15 samples/sec#011loss=2.757890\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:17 INFO 139663579178816] processed a total of 1205 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10382.08293914795, \"sum\": 10382.08293914795, \"min\": 10382.08293914795}}, \"EndTime\": 1538407817.349938, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407806.967605}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.063899376 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:19 INFO 139663579178816] Epoch[10] Batch[0] avg_epoch_loss=2.610474\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:17 INFO 140624803325760] Epoch[9] Batch[5] avg_epoch_loss=2.793180\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:17 INFO 140624803325760] Epoch[9] Batch [5]#011Speed: 120.87 samples/sec#011loss=2.793180\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:17 INFO 140252856112960] Epoch[9] Batch[5] avg_epoch_loss=2.752350\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:17 INFO 140252856112960] Epoch[9] Batch [5]#011Speed: 116.19 samples/sec#011loss=2.752350\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:20 INFO 140196492334912] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10582.921981811523, \"sum\": 10582.921981811523, \"min\": 10582.921981811523}}, \"EndTime\": 1538407820.988343, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407810.4052}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:20 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.278840228 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:20 INFO 140196492334912] #progress_metric: host=algo-4, completed 4 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:21 INFO 140624803325760] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10815.083980560303, \"sum\": 10815.083980560303, \"min\": 10815.083980560303}}, \"EndTime\": 1538407821.631549, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407810.81617}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:21 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.670582693 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:21 INFO 140624803325760] #progress_metric: host=algo-3, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:22 INFO 140252856112960] Epoch[9] Batch[10] avg_epoch_loss=2.390702\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:22 INFO 140252856112960] Epoch[9] Batch [10]#011Speed: 142.02 samples/sec#011loss=1.956724\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:22 INFO 140252856112960] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11693.28498840332, \"sum\": 11693.28498840332, \"min\": 11693.28498840332}}, \"EndTime\": 1538407822.345366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407810.651826}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:22 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.285446938 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:22 INFO 140252856112960] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:23 INFO 140196492334912] Epoch[10] Batch[0] avg_epoch_loss=2.722334\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:23 INFO 140624803325760] Epoch[10] Batch[0] avg_epoch_loss=2.729818\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:24 INFO 139663579178816] Epoch[10] Batch[5] avg_epoch_loss=2.734085\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:24 INFO 139663579178816] Epoch[10] Batch [5]#011Speed: 146.76 samples/sec#011loss=2.734085\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:24 INFO 140252856112960] Epoch[10] Batch[0] avg_epoch_loss=2.628071\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:27 INFO 140196492334912] Epoch[10] Batch[5] avg_epoch_loss=2.554888\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:27 INFO 140196492334912] Epoch[10] Batch [5]#011Speed: 149.23 samples/sec#011loss=2.554888\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:29 INFO 140252856112960] Epoch[10] Batch[5] avg_epoch_loss=2.504562\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:29 INFO 140252856112960] Epoch[10] Batch [5]#011Speed: 142.35 samples/sec#011loss=2.504562\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:31 INFO 140196492334912] processed a total of 1189 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10592.741012573242, \"sum\": 10592.741012573242, \"min\": 10592.741012573242}}, \"EndTime\": 1538407831.581416, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407820.988421}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:31 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.245099995 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:31 INFO 140196492334912] #progress_metric: host=algo-4, completed 4 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:30:28 INFO 140624803325760] Epoch[10] Batch[5] avg_epoch_loss=2.451314\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:28 INFO 140624803325760] Epoch[10] Batch [5]#011Speed: 119.00 samples/sec#011loss=2.451314\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:29 INFO 139663579178816] Epoch[10] Batch[10] avg_epoch_loss=2.741024\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:29 INFO 139663579178816] Epoch[10] Batch [10]#011Speed: 113.18 samples/sec#011loss=2.749351\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:29 INFO 139663579178816] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12501.466989517212, \"sum\": 12501.466989517212, \"min\": 12501.466989517212}}, \"EndTime\": 1538407829.85175, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407817.350025}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=103.186858775 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:31 INFO 139663579178816] Epoch[11] Batch[0] avg_epoch_loss=2.291152\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:33 INFO 140624803325760] Epoch[10] Batch[10] avg_epoch_loss=2.798096\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:33 INFO 140624803325760] Epoch[10] Batch [10]#011Speed: 140.14 samples/sec#011loss=3.214236\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:33 INFO 140624803325760] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11583.863019943237, \"sum\": 11583.863019943237, \"min\": 11583.863019943237}}, \"EndTime\": 1538407833.215732, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407821.631626}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:33 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.53347637 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:33 INFO 140624803325760] #progress_metric: host=algo-3, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:34 INFO 140196492334912] Epoch[11] Batch[0] avg_epoch_loss=2.320697\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:35 INFO 140624803325760] Epoch[11] Batch[0] avg_epoch_loss=2.885507\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:35 INFO 140252856112960] Epoch[10] Batch[10] avg_epoch_loss=2.553060\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:35 INFO 140252856112960] Epoch[10] Batch [10]#011Speed: 112.06 samples/sec#011loss=2.611256\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:35 INFO 140252856112960] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12665.755987167358, \"sum\": 12665.755987167358, \"min\": 12665.755987167358}}, \"EndTime\": 1538407835.011469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407822.34545}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:35 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=104.374860349 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:35 INFO 140252856112960] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:36 INFO 140252856112960] Epoch[11] Batch[0] avg_epoch_loss=2.918395\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:36 INFO 139663579178816] Epoch[11] Batch[5] avg_epoch_loss=2.598967\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:36 INFO 139663579178816] Epoch[11] Batch [5]#011Speed: 119.76 samples/sec#011loss=2.598967\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:38 INFO 140196492334912] Epoch[11] Batch[5] avg_epoch_loss=2.478647\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:38 INFO 140196492334912] Epoch[11] Batch [5]#011Speed: 151.74 samples/sec#011loss=2.478647\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:39 INFO 140624803325760] Epoch[11] Batch[5] avg_epoch_loss=2.599982\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:39 INFO 140624803325760] Epoch[11] Batch [5]#011Speed: 148.74 samples/sec#011loss=2.599982\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:41 INFO 139663579178816] Epoch[11] Batch[10] avg_epoch_loss=2.586445\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:41 INFO 139663579178816] Epoch[11] Batch [10]#011Speed: 147.62 samples/sec#011loss=2.571418\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:41 INFO 139663579178816] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11341.991901397705, \"sum\": 11341.991901397705, \"min\": 11341.991901397705}}, \"EndTime\": 1538407841.194064, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407829.851834}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.735600386 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:42 INFO 140252856112960] Epoch[11] Batch[5] avg_epoch_loss=2.384335\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:42 INFO 140252856112960] Epoch[11] Batch [5]#011Speed: 114.56 samples/sec#011loss=2.384335\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:43 INFO 139663579178816] Epoch[12] Batch[0] avg_epoch_loss=2.495101\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:43 INFO 140196492334912] Epoch[11] Batch[10] avg_epoch_loss=2.387891\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:43 INFO 140196492334912] Epoch[11] Batch [10]#011Speed: 120.02 samples/sec#011loss=2.278984\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:43 INFO 140196492334912] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11998.998880386353, \"sum\": 11998.998880386353, \"min\": 11998.998880386353}}, \"EndTime\": 1538407843.580779, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407831.58153}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:43 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.091480069 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:43 INFO 140196492334912] #progress_metric: host=algo-4, completed 4 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:43 INFO 140624803325760] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10591.194868087769, \"sum\": 10591.194868087769, \"min\": 10591.194868087769}}, \"EndTime\": 1538407843.807227, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407833.2158}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.454614485 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:45 INFO 140196492334912] Epoch[12] Batch[0] avg_epoch_loss=2.389793\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:46 INFO 140624803325760] Epoch[12] Batch[0] avg_epoch_loss=2.709844\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:46 INFO 140252856112960] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11274.224996566772, \"sum\": 11274.224996566772, \"min\": 11274.224996566772}}, \"EndTime\": 1538407846.286028, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407835.011555}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.161452634 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:47 INFO 139663579178816] Epoch[12] Batch[5] avg_epoch_loss=2.450180\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:47 INFO 139663579178816] Epoch[12] Batch [5]#011Speed: 148.84 samples/sec#011loss=2.450180\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:47 INFO 140252856112960] Epoch[12] Batch[0] avg_epoch_loss=2.438226\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:50 INFO 140624803325760] Epoch[12] Batch[5] avg_epoch_loss=2.469765\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:50 INFO 140624803325760] Epoch[12] Batch [5]#011Speed: 151.13 samples/sec#011loss=2.469765\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:51 INFO 139663579178816] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10615.791082382202, \"sum\": 10615.791082382202, \"min\": 10615.791082382202}}, \"EndTime\": 1538407851.810144, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407841.194132}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:51 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.653718247 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:51 INFO 139663579178816] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:30:50 INFO 140196492334912] Epoch[12] Batch[5] avg_epoch_loss=2.529488\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:50 INFO 140196492334912] Epoch[12] Batch [5]#011Speed: 113.88 samples/sec#011loss=2.529488\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:53 INFO 140252856112960] Epoch[12] Batch[5] avg_epoch_loss=2.479917\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:53 INFO 140252856112960] Epoch[12] Batch [5]#011Speed: 114.88 samples/sec#011loss=2.479917\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:54 INFO 139663579178816] Epoch[13] Batch[0] avg_epoch_loss=2.655368\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:55 INFO 140196492334912] Epoch[12] Batch[10] avg_epoch_loss=2.679373\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:55 INFO 140196492334912] Epoch[12] Batch [10]#011Speed: 139.94 samples/sec#011loss=2.859235\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:55 INFO 140196492334912] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11911.39006614685, \"sum\": 11911.39006614685, \"min\": 11911.39006614685}}, \"EndTime\": 1538407855.492513, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407843.58086}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:55 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.046945175 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:55 INFO 140196492334912] #progress_metric: host=algo-4, completed 5 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:55 INFO 140624803325760] Epoch[12] Batch[10] avg_epoch_loss=2.215324\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:55 INFO 140624803325760] Epoch[12] Batch [10]#011Speed: 118.34 samples/sec#011loss=1.909995\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:55 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12004.99701499939, \"sum\": 12004.99701499939, \"min\": 12004.99701499939}}, \"EndTime\": 1538407855.812564, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407843.807313}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:55 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.536989362 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:55 INFO 140624803325760] #progress_metric: host=algo-3, completed 5 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:30:57 INFO 140624803325760] Epoch[13] Batch[0] avg_epoch_loss=2.732700\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:57 INFO 140252856112960] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11141.82996749878, \"sum\": 11141.82996749878, \"min\": 11141.82996749878}}, \"EndTime\": 1538407857.428223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407846.28612}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.791107744 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:58 INFO 139663579178816] Epoch[13] Batch[5] avg_epoch_loss=2.756919\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:30:58 INFO 139663579178816] Epoch[13] Batch [5]#011Speed: 159.39 samples/sec#011loss=2.756919\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:30:58 INFO 140196492334912] Epoch[13] Batch[0] avg_epoch_loss=2.391984\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:30:59 INFO 140252856112960] Epoch[13] Batch[0] avg_epoch_loss=2.572988\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:01 INFO 139663579178816] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10052.363157272339, \"sum\": 10052.363157272339, \"min\": 10052.363157272339}}, \"EndTime\": 1538407861.86281, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407851.810217}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=126.933911583 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:02 INFO 140196492334912] Epoch[13] Batch[5] avg_epoch_loss=2.412922\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:02 INFO 140196492334912] Epoch[13] Batch [5]#011Speed: 144.30 samples/sec#011loss=2.412922\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:02 INFO 140624803325760] Epoch[13] Batch[5] avg_epoch_loss=2.672003\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:02 INFO 140624803325760] Epoch[13] Batch [5]#011Speed: 121.49 samples/sec#011loss=2.672003\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:04 INFO 139663579178816] Epoch[14] Batch[0] avg_epoch_loss=2.428536\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:04 INFO 140252856112960] Epoch[13] Batch[5] avg_epoch_loss=2.457534\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:04 INFO 140252856112960] Epoch[13] Batch [5]#011Speed: 113.72 samples/sec#011loss=2.457534\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:07 INFO 140624803325760] Epoch[13] Batch[10] avg_epoch_loss=2.477176\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:07 INFO 140624803325760] Epoch[13] Batch [10]#011Speed: 149.51 samples/sec#011loss=2.243384\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:07 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11204.504013061523, \"sum\": 11204.504013061523, \"min\": 11204.504013061523}}, \"EndTime\": 1538407867.017398, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407855.812648}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:07 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.291244985 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:07 INFO 140624803325760] #progress_metric: host=algo-3, completed 5 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:08 INFO 139663579178816] Epoch[14] Batch[5] avg_epoch_loss=2.344435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:08 INFO 139663579178816] Epoch[14] Batch [5]#011Speed: 161.43 samples/sec#011loss=2.344435\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:08 INFO 140196492334912] Epoch[13] Batch[10] avg_epoch_loss=2.494899\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:08 INFO 140196492334912] Epoch[13] Batch [10]#011Speed: 113.73 samples/sec#011loss=2.593272\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:08 INFO 140196492334912] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12677.95205116272, \"sum\": 12677.95205116272, \"min\": 12677.95205116272}}, \"EndTime\": 1538407868.170744, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407855.49258}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:08 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=101.987171837 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:08 INFO 140196492334912] #progress_metric: host=algo-4, completed 5 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:09 INFO 140624803325760] Epoch[14] Batch[0] avg_epoch_loss=2.425189\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:09 INFO 140252856112960] Epoch[13] Batch[10] avg_epoch_loss=2.387959\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:09 INFO 140252856112960] Epoch[13] Batch [10]#011Speed: 140.52 samples/sec#011loss=2.304469\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:09 INFO 140252856112960] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11847.686052322388, \"sum\": 11847.686052322388, \"min\": 11847.686052322388}}, \"EndTime\": 1538407869.27626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407857.428321}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.471672749 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:09 INFO 140196492334912] Epoch[14] Batch[0] avg_epoch_loss=2.546009\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:11 INFO 139663579178816] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9870.239973068237, \"sum\": 9870.239973068237, \"min\": 9870.239973068237}}, \"EndTime\": 1538407871.733354, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407861.862887}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:11 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.716917628 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:11 INFO 139663579178816] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:13 INFO 140624803325760] Epoch[14] Batch[5] avg_epoch_loss=2.429875\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:13 INFO 140624803325760] Epoch[14] Batch [5]#011Speed: 153.19 samples/sec#011loss=2.429875\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:31:11 INFO 140252856112960] Epoch[14] Batch[0] avg_epoch_loss=2.287405\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:14 INFO 139663579178816] Epoch[15] Batch[0] avg_epoch_loss=2.395024\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:15 INFO 140196492334912] Epoch[14] Batch[5] avg_epoch_loss=2.411259\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:15 INFO 140196492334912] Epoch[14] Batch [5]#011Speed: 110.10 samples/sec#011loss=2.411259\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:16 INFO 140252856112960] Epoch[14] Batch[5] avg_epoch_loss=2.336063\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:16 INFO 140252856112960] Epoch[14] Batch [5]#011Speed: 145.83 samples/sec#011loss=2.336063\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:18 INFO 139663579178816] Epoch[15] Batch[5] avg_epoch_loss=2.359406\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:18 INFO 139663579178816] Epoch[15] Batch [5]#011Speed: 150.03 samples/sec#011loss=2.359406\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:18 INFO 140624803325760] Epoch[14] Batch[10] avg_epoch_loss=2.124298\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:18 INFO 140624803325760] Epoch[14] Batch [10]#011Speed: 118.37 samples/sec#011loss=1.757606\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:18 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11981.575965881348, \"sum\": 11981.575965881348, \"min\": 11981.575965881348}}, \"EndTime\": 1538407878.999317, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407867.017478}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.665605451 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:20 INFO 140196492334912] Epoch[14] Batch[10] avg_epoch_loss=2.460945\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:20 INFO 140196492334912] Epoch[14] Batch [10]#011Speed: 137.03 samples/sec#011loss=2.520569\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:20 INFO 140196492334912] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12153.36298942566, \"sum\": 12153.36298942566, \"min\": 12153.36298942566}}, \"EndTime\": 1538407880.324423, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407868.170823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:20 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.224824988 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:20 INFO 140196492334912] #progress_metric: host=algo-4, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:20 INFO 140252856112960] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10804.203987121582, \"sum\": 10804.203987121582, \"min\": 10804.203987121582}}, \"EndTime\": 1538407880.080804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407869.276345}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.990025617 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:20 INFO 140624803325760] Epoch[15] Batch[0] avg_epoch_loss=2.217320\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:22 INFO 140252856112960] Epoch[15] Batch[0] avg_epoch_loss=2.392219\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:22 INFO 140196492334912] Epoch[15] Batch[0] avg_epoch_loss=2.385804\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:23 INFO 139663579178816] Epoch[15] Batch[10] avg_epoch_loss=2.370246\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:23 INFO 139663579178816] Epoch[15] Batch [10]#011Speed: 122.14 samples/sec#011loss=2.383254\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:23 INFO 139663579178816] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11790.0869846344, \"sum\": 11790.0869846344, \"min\": 11790.0869846344}}, \"EndTime\": 1538407883.523748, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407871.733431}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:23 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.243205817 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:23 INFO 139663579178816] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:25 INFO 139663579178816] Epoch[16] Batch[0] avg_epoch_loss=2.680184\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:25 INFO 140624803325760] Epoch[15] Batch[5] avg_epoch_loss=2.355173\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:25 INFO 140624803325760] Epoch[15] Batch [5]#011Speed: 121.13 samples/sec#011loss=2.355173\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:27 INFO 140196492334912] Epoch[15] Batch[5] avg_epoch_loss=2.551344\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:27 INFO 140196492334912] Epoch[15] Batch [5]#011Speed: 146.81 samples/sec#011loss=2.551344\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:27 INFO 140252856112960] Epoch[15] Batch[5] avg_epoch_loss=2.475492\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:27 INFO 140252856112960] Epoch[15] Batch [5]#011Speed: 144.84 samples/sec#011loss=2.475492\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:30 INFO 140624803325760] Epoch[15] Batch[10] avg_epoch_loss=2.416685\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:30 INFO 140624803325760] Epoch[15] Batch [10]#011Speed: 149.37 samples/sec#011loss=2.490499\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:30 INFO 140624803325760] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11227.86808013916, \"sum\": 11227.86808013916, \"min\": 11227.86808013916}}, \"EndTime\": 1538407890.227515, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407878.999401}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:30 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.850812972 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:30 INFO 140624803325760] #progress_metric: host=algo-3, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:30 INFO 139663579178816] Epoch[16] Batch[5] avg_epoch_loss=2.543704\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:30 INFO 139663579178816] Epoch[16] Batch [5]#011Speed: 124.67 samples/sec#011loss=2.543704\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:32 INFO 140624803325760] Epoch[16] Batch[0] avg_epoch_loss=2.391407\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:32 INFO 140252856112960] Epoch[15] Batch[10] avg_epoch_loss=2.540126\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:32 INFO 140252856112960] Epoch[15] Batch [10]#011Speed: 114.65 samples/sec#011loss=2.617686\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:32 INFO 140252856112960] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12537.585973739624, \"sum\": 12537.585973739624, \"min\": 12537.585973739624}}, \"EndTime\": 1538407892.618733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407880.080894}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:32 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=103.846658364 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:32 INFO 140252856112960] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:34 INFO 140252856112960] Epoch[16] Batch[0] avg_epoch_loss=2.189412\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:34 INFO 139663579178816] Epoch[16] Batch[10] avg_epoch_loss=2.431102\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:34 INFO 139663579178816] Epoch[16] Batch [10]#011Speed: 149.35 samples/sec#011loss=2.295980\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:34 INFO 139663579178816] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11230.92007637024, \"sum\": 11230.92007637024, \"min\": 11230.92007637024}}, \"EndTime\": 1538407894.754984, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407883.523834}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:34 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.353302647 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:34 INFO 139663579178816] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:32 INFO 140196492334912] Epoch[15] Batch[10] avg_epoch_loss=2.818477\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:32 INFO 140196492334912] Epoch[15] Batch [10]#011Speed: 112.36 samples/sec#011loss=3.139035\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:32 INFO 140196492334912] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12481.382131576538, \"sum\": 12481.382131576538, \"min\": 12481.382131576538}}, \"EndTime\": 1538407892.806098, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407880.324492}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:32 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=102.792239124 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:32 INFO 140196492334912] #progress_metric: host=algo-4, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:34 INFO 140196492334912] Epoch[16] Batch[0] avg_epoch_loss=2.309553\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:37 INFO 139663579178816] Epoch[17] Batch[0] avg_epoch_loss=2.618107\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:36 INFO 140624803325760] Epoch[16] Batch[5] avg_epoch_loss=2.420829\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:36 INFO 140624803325760] Epoch[16] Batch [5]#011Speed: 152.27 samples/sec#011loss=2.420829\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:40 INFO 140196492334912] Epoch[16] Batch[5] avg_epoch_loss=2.335153\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:40 INFO 140196492334912] Epoch[16] Batch [5]#011Speed: 114.99 samples/sec#011loss=2.335153\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:40 INFO 140624803325760] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10335.753917694092, \"sum\": 10335.753917694092, \"min\": 10335.753917694092}}, \"EndTime\": 1538407900.563598, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407890.227597}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.16380581 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:39 INFO 140252856112960] Epoch[16] Batch[5] avg_epoch_loss=2.303292\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:39 INFO 140252856112960] Epoch[16] Batch [5]#011Speed: 114.97 samples/sec#011loss=2.303292\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:41 INFO 139663579178816] Epoch[17] Batch[5] avg_epoch_loss=2.404179\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:41 INFO 139663579178816] Epoch[17] Batch [5]#011Speed: 150.61 samples/sec#011loss=2.404179\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:44 INFO 140252856112960] Epoch[16] Batch[10] avg_epoch_loss=2.268683\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:44 INFO 140252856112960] Epoch[16] Batch [10]#011Speed: 138.99 samples/sec#011loss=2.227153\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:44 INFO 140252856112960] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11813.938856124878, \"sum\": 11813.938856124878, \"min\": 11813.938856124878}}, \"EndTime\": 1538407904.433008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407892.618819}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.599368822 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:31:42 INFO 140624803325760] Epoch[17] Batch[0] avg_epoch_loss=2.222067\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:44 INFO 140196492334912] Epoch[16] Batch[10] avg_epoch_loss=2.615141\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:44 INFO 140196492334912] Epoch[16] Batch [10]#011Speed: 140.27 samples/sec#011loss=2.951127\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:44 INFO 140196492334912] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11789.685010910034, \"sum\": 11789.685010910034, \"min\": 11789.685010910034}}, \"EndTime\": 1538407904.596074, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407892.806166}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:44 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.840848148 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:44 INFO 140196492334912] #progress_metric: host=algo-4, completed 6 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:46 INFO 139663579178816] Epoch[17] Batch[10] avg_epoch_loss=2.276344\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:46 INFO 139663579178816] Epoch[17] Batch [10]#011Speed: 115.70 samples/sec#011loss=2.122942\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:46 INFO 139663579178816] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12160.858869552612, \"sum\": 12160.858869552612, \"min\": 12160.858869552612}}, \"EndTime\": 1538407906.916237, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407894.755066}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:46 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=106.899199981 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:46 INFO 139663579178816] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:46 INFO 140196492334912] Epoch[17] Batch[0] avg_epoch_loss=2.355482\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:47 INFO 140624803325760] Epoch[17] Batch[5] avg_epoch_loss=2.409695\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:47 INFO 140624803325760] Epoch[17] Batch [5]#011Speed: 151.56 samples/sec#011loss=2.409695\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:46 INFO 140252856112960] Epoch[17] Batch[0] avg_epoch_loss=2.468577\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:48 INFO 139663579178816] Epoch[18] Batch[0] avg_epoch_loss=3.084581\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:51 INFO 140196492334912] Epoch[17] Batch[5] avg_epoch_loss=2.610239\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:51 INFO 140196492334912] Epoch[17] Batch [5]#011Speed: 145.76 samples/sec#011loss=2.610239\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:51 INFO 140624803325760] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10499.189138412476, \"sum\": 10499.189138412476, \"min\": 10499.189138412476}}, \"EndTime\": 1538407911.063136, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407900.56369}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.246015839 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:51 INFO 140252856112960] Epoch[17] Batch[5] avg_epoch_loss=2.489316\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:51 INFO 140252856112960] Epoch[17] Batch [5]#011Speed: 143.76 samples/sec#011loss=2.489316\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:53 INFO 140624803325760] Epoch[18] Batch[0] avg_epoch_loss=2.127197\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:55 INFO 140196492334912] processed a total of 1198 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10761.719942092896, \"sum\": 10761.719942092896, \"min\": 10761.719942092896}}, \"EndTime\": 1538407915.358078, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407904.596141}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:55 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.319346585 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:55 INFO 140196492334912] #progress_metric: host=algo-4, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:53 INFO 139663579178816] Epoch[18] Batch[5] avg_epoch_loss=2.524351\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:53 INFO 139663579178816] Epoch[18] Batch [5]#011Speed: 117.10 samples/sec#011loss=2.524351\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:57 INFO 140624803325760] Epoch[18] Batch[5] avg_epoch_loss=2.397280\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:31:57 INFO 140624803325760] Epoch[18] Batch [5]#011Speed: 151.76 samples/sec#011loss=2.397280\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:57 INFO 140252856112960] Epoch[17] Batch[10] avg_epoch_loss=2.456844\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:57 INFO 140252856112960] Epoch[17] Batch [10]#011Speed: 112.79 samples/sec#011loss=2.417877\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:57 INFO 140252856112960] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12612.657070159912, \"sum\": 12612.657070159912, \"min\": 12612.657070159912}}, \"EndTime\": 1538407917.045998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407904.433094}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=103.54564653 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:57 INFO 139663579178816] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10982.846975326538, \"sum\": 10982.846975326538, \"min\": 10982.846975326538}}, \"EndTime\": 1538407917.899417, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407906.916324}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.904947321 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:31:57 INFO 140196492334912] Epoch[18] Batch[0] avg_epoch_loss=2.447947\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:31:58 INFO 140252856112960] Epoch[18] Batch[0] avg_epoch_loss=2.543076\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:31:59 INFO 139663579178816] Epoch[19] Batch[0] avg_epoch_loss=2.305575\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:02 INFO 140196492334912] Epoch[18] Batch[5] avg_epoch_loss=2.446046\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:02 INFO 140196492334912] Epoch[18] Batch [5]#011Speed: 146.11 samples/sec#011loss=2.446046\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:03 INFO 140624803325760] Epoch[18] Batch[10] avg_epoch_loss=2.618059\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:03 INFO 140624803325760] Epoch[18] Batch [10]#011Speed: 117.04 samples/sec#011loss=2.882993\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:03 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12039.583206176758, \"sum\": 12039.583206176758, \"min\": 12039.583206176758}}, \"EndTime\": 1538407923.10305, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407911.063218}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:03 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.142122101 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:03 INFO 140624803325760] #progress_metric: host=algo-3, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:04 INFO 140252856112960] Epoch[18] Batch[5] avg_epoch_loss=2.145936\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:04 INFO 140252856112960] Epoch[18] Batch [5]#011Speed: 113.90 samples/sec#011loss=2.145936\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:05 INFO 139663579178816] Epoch[19] Batch[5] avg_epoch_loss=2.228167\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:05 INFO 139663579178816] Epoch[19] Batch [5]#011Speed: 116.44 samples/sec#011loss=2.228167\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:04 INFO 140624803325760] Epoch[19] Batch[0] avg_epoch_loss=2.144258\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:07 INFO 140196492334912] Epoch[18] Batch[10] avg_epoch_loss=2.024427\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:07 INFO 140196492334912] Epoch[18] Batch [10]#011Speed: 113.68 samples/sec#011loss=1.518485\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:07 INFO 140196492334912] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12354.257106781006, \"sum\": 12354.257106781006, \"min\": 12354.257106781006}}, \"EndTime\": 1538407927.712644, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407915.358153}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:07 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.578286129 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:07 INFO 140196492334912] #progress_metric: host=algo-4, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:08 INFO 140252856112960] processed a total of 1199 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11268.32914352417, \"sum\": 11268.32914352417, \"min\": 11268.32914352417}}, \"EndTime\": 1538407928.314719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407917.046094}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:08 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.402634586 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:08 INFO 140252856112960] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:09 INFO 140196492334912] Epoch[19] Batch[0] avg_epoch_loss=2.060965\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:09 INFO 139663579178816] Epoch[19] Batch[10] avg_epoch_loss=2.290209\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:09 INFO 139663579178816] Epoch[19] Batch [10]#011Speed: 142.70 samples/sec#011loss=2.364659\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:09 INFO 139663579178816] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11622.992038726807, \"sum\": 11622.992038726807, \"min\": 11622.992038726807}}, \"EndTime\": 1538407929.522764, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407917.899518}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:09 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.362528592 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:09 INFO 139663579178816] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:09 INFO 140624803325760] Epoch[19] Batch[5] avg_epoch_loss=2.267397\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:09 INFO 140624803325760] Epoch[19] Batch [5]#011Speed: 124.25 samples/sec#011loss=2.267397\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:09 INFO 140252856112960] Epoch[19] Batch[0] avg_epoch_loss=2.231645\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:11 INFO 139663579178816] Epoch[20] Batch[0] avg_epoch_loss=2.382366\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:14 INFO 140624803325760] Epoch[19] Batch[10] avg_epoch_loss=2.211872\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:14 INFO 140624803325760] Epoch[19] Batch [10]#011Speed: 153.59 samples/sec#011loss=2.145242\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:14 INFO 140624803325760] processed a total of 1373 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10997.424125671387, \"sum\": 10997.424125671387, \"min\": 10997.424125671387}}, \"EndTime\": 1538407934.100802, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407923.103138}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:14 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.846226482 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:14 INFO 140624803325760] #progress_metric: host=algo-3, completed 8 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:32:14 INFO 140196492334912] Epoch[19] Batch[5] avg_epoch_loss=2.355883\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:14 INFO 140196492334912] Epoch[19] Batch [5]#011Speed: 113.39 samples/sec#011loss=2.355883\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:15 INFO 140252856112960] Epoch[19] Batch[5] avg_epoch_loss=2.402911\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:15 INFO 140252856112960] Epoch[19] Batch [5]#011Speed: 116.70 samples/sec#011loss=2.402911\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:16 INFO 140624803325760] Epoch[20] Batch[0] avg_epoch_loss=3.015562\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:16 INFO 139663579178816] Epoch[20] Batch[5] avg_epoch_loss=2.327552\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:16 INFO 139663579178816] Epoch[20] Batch [5]#011Speed: 147.47 samples/sec#011loss=2.327552\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:19 INFO 140196492334912] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11291.840076446533, \"sum\": 11291.840076446533, \"min\": 11291.840076446533}}, \"EndTime\": 1538407939.004812, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407927.712727}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.672316787 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:19 INFO 140252856112960] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11035.948038101196, \"sum\": 11035.948038101196, \"min\": 11035.948038101196}}, \"EndTime\": 1538407939.351029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407928.314824}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:19 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.727551098 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:19 INFO 140252856112960] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:20 INFO 140624803325760] Epoch[20] Batch[5] avg_epoch_loss=2.537998\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:20 INFO 140624803325760] Epoch[20] Batch [5]#011Speed: 155.90 samples/sec#011loss=2.537998\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:20 INFO 139663579178816] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10791.9340133667, \"sum\": 10791.9340133667, \"min\": 10791.9340133667}}, \"EndTime\": 1538407940.314996, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407929.52283}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:20 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.343237966 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:20 INFO 139663579178816] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:20 INFO 140196492334912] Epoch[20] Batch[0] avg_epoch_loss=2.545578\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:21 INFO 140252856112960] Epoch[20] Batch[0] avg_epoch_loss=2.251614\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:22 INFO 139663579178816] Epoch[21] Batch[0] avg_epoch_loss=2.701350\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:26 INFO 139663579178816] Epoch[21] Batch[5] avg_epoch_loss=2.329262\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:26 INFO 139663579178816] Epoch[21] Batch [5]#011Speed: 150.46 samples/sec#011loss=2.329262\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:26 INFO 140196492334912] Epoch[20] Batch[5] avg_epoch_loss=2.255367\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:26 INFO 140196492334912] Epoch[20] Batch [5]#011Speed: 117.30 samples/sec#011loss=2.255367\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:25 INFO 140624803325760] Epoch[20] Batch[10] avg_epoch_loss=2.717500\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:25 INFO 140624803325760] Epoch[20] Batch [10]#011Speed: 120.05 samples/sec#011loss=2.932902\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:25 INFO 140624803325760] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11723.665952682495, \"sum\": 11723.665952682495, \"min\": 11723.665952682495}}, \"EndTime\": 1538407945.824763, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407934.100871}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:25 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.435636862 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:25 INFO 140624803325760] #progress_metric: host=algo-3, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:26 INFO 140252856112960] Epoch[20] Batch[5] avg_epoch_loss=2.312671\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:26 INFO 140252856112960] Epoch[20] Batch [5]#011Speed: 119.21 samples/sec#011loss=2.312671\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:27 INFO 140624803325760] Epoch[21] Batch[0] avg_epoch_loss=2.029617\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:30 INFO 140196492334912] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11053.761959075928, \"sum\": 11053.761959075928, \"min\": 11053.761959075928}}, \"EndTime\": 1538407950.0589, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407939.004896}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.434476114 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:30 INFO 140252856112960] Epoch[20] Batch[10] avg_epoch_loss=2.414445\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:30 INFO 140252856112960] Epoch[20] Batch [10]#011Speed: 146.95 samples/sec#011loss=2.536574\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:30 INFO 140252856112960] processed a total of 1353 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11428.352117538452, \"sum\": 11428.352117538452, \"min\": 11428.352117538452}}, \"EndTime\": 1538407950.779748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407939.351122}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.388403585 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:31 INFO 140196492334912] Epoch[21] Batch[0] avg_epoch_loss=2.297482\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:32 INFO 140624803325760] Epoch[21] Batch[5] avg_epoch_loss=2.295188\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:32 INFO 140624803325760] Epoch[21] Batch [5]#011Speed: 122.26 samples/sec#011loss=2.295188\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:32 INFO 139663579178816] Epoch[21] Batch[10] avg_epoch_loss=2.191488\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:32 INFO 139663579178816] Epoch[21] Batch [10]#011Speed: 117.64 samples/sec#011loss=2.026160\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:32 INFO 139663579178816] processed a total of 1338 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12088.385105133057, \"sum\": 12088.385105133057, \"min\": 12088.385105133057}}, \"EndTime\": 1538407952.403789, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407940.315087}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:32 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.683744557 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:32 INFO 139663579178816] #progress_metric: host=algo-2, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:33 INFO 140252856112960] Epoch[21] Batch[0] avg_epoch_loss=2.240347\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:34 INFO 139663579178816] Epoch[22] Batch[0] avg_epoch_loss=2.402409\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:36 INFO 140624803325760] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10635.730981826782, \"sum\": 10635.730981826782, \"min\": 10635.730981826782}}, \"EndTime\": 1538407956.460808, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407945.824849}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:36 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.903251611 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:36 INFO 140624803325760] #progress_metric: host=algo-3, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:36 INFO 140196492334912] Epoch[21] Batch[5] avg_epoch_loss=2.350810\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:36 INFO 140196492334912] Epoch[21] Batch [5]#011Speed: 119.69 samples/sec#011loss=2.350810\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:37 INFO 140252856112960] Epoch[21] Batch[5] avg_epoch_loss=2.138544\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:37 INFO 140252856112960] Epoch[21] Batch [5]#011Speed: 151.72 samples/sec#011loss=2.138544\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:38 INFO 140624803325760] Epoch[22] Batch[0] avg_epoch_loss=2.337977\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:39 INFO 139663579178816] Epoch[22] Batch[5] avg_epoch_loss=2.396045\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:39 INFO 139663579178816] Epoch[22] Batch [5]#011Speed: 121.22 samples/sec#011loss=2.396045\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:41 INFO 140196492334912] Epoch[21] Batch[10] avg_epoch_loss=2.457099\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:41 INFO 140196492334912] Epoch[21] Batch [10]#011Speed: 147.30 samples/sec#011loss=2.584646\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:41 INFO 140196492334912] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11253.546953201294, \"sum\": 11253.546953201294, \"min\": 11253.546953201294}}, \"EndTime\": 1538407961.312775, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407950.058987}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.096132215 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 8 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:43 INFO 139663579178816] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10783.962965011597, \"sum\": 10783.962965011597, \"min\": 10783.962965011597}}, \"EndTime\": 1538407963.18803, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407952.403856}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:43 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.911290084 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:43 INFO 139663579178816] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:43 INFO 140624803325760] Epoch[22] Batch[5] avg_epoch_loss=2.249462\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:43 INFO 140624803325760] Epoch[22] Batch [5]#011Speed: 122.52 samples/sec#011loss=2.249462\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:42 INFO 140252856112960] Epoch[21] Batch[10] avg_epoch_loss=2.501979\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:42 INFO 140252856112960] Epoch[21] Batch [10]#011Speed: 117.75 samples/sec#011loss=2.938100\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:42 INFO 140252856112960] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12075.132131576538, \"sum\": 12075.132131576538, \"min\": 12075.132131576538}}, \"EndTime\": 1538407962.855217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407950.779836}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:42 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.409713169 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:42 INFO 140252856112960] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:43 INFO 140196492334912] Epoch[22] Batch[0] avg_epoch_loss=2.082785\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:44 INFO 140252856112960] Epoch[22] Batch[0] avg_epoch_loss=2.061168\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:47 INFO 140624803325760] Epoch[22] Batch[10] avg_epoch_loss=1.899782\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:47 INFO 140624803325760] Epoch[22] Batch [10]#011Speed: 146.81 samples/sec#011loss=1.480166\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:47 INFO 140624803325760] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11199.60880279541, \"sum\": 11199.60880279541, \"min\": 11199.60880279541}}, \"EndTime\": 1538407967.660737, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407956.460882}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:47 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.377950498 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:47 INFO 140624803325760] #progress_metric: host=algo-3, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:47 INFO 140196492334912] Epoch[22] Batch[5] avg_epoch_loss=2.223336\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:47 INFO 140196492334912] Epoch[22] Batch [5]#011Speed: 153.64 samples/sec#011loss=2.223336\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:32:44 INFO 139663579178816] Epoch[23] Batch[0] avg_epoch_loss=2.369803\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:49 INFO 140624803325760] Epoch[23] Batch[0] avg_epoch_loss=2.346704\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:49 INFO 140252856112960] Epoch[22] Batch[5] avg_epoch_loss=2.147310\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:49 INFO 140252856112960] Epoch[22] Batch [5]#011Speed: 119.67 samples/sec#011loss=2.147310\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:50 INFO 139663579178816] Epoch[23] Batch[5] avg_epoch_loss=2.230260\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:50 INFO 139663579178816] Epoch[23] Batch [5]#011Speed: 118.83 samples/sec#011loss=2.230260\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:51 INFO 140196492334912] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10313.658952713013, \"sum\": 10313.658952713013, \"min\": 10313.658952713013}}, \"EndTime\": 1538407971.626758, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407961.312855}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:51 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.391051521 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:51 INFO 140196492334912] #progress_metric: host=algo-4, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:53 INFO 140252856112960] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10863.928079605103, \"sum\": 10863.928079605103, \"min\": 10863.928079605103}}, \"EndTime\": 1538407973.719478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407962.855302}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:53 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.610491802 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:53 INFO 140252856112960] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:54 INFO 139663579178816] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10954.737901687622, \"sum\": 10954.737901687622, \"min\": 10954.737901687622}}, \"EndTime\": 1538407974.143125, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407963.188134}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:54 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.199465416 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:54 INFO 139663579178816] #progress_metric: host=algo-2, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:53 INFO 140196492334912] Epoch[23] Batch[0] avg_epoch_loss=2.533610\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:54 INFO 140624803325760] Epoch[23] Batch[5] avg_epoch_loss=2.378970\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:54 INFO 140624803325760] Epoch[23] Batch [5]#011Speed: 153.92 samples/sec#011loss=2.378970\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:32:55 INFO 140252856112960] Epoch[23] Batch[0] avg_epoch_loss=1.934381\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:32:55 INFO 139663579178816] Epoch[24] Batch[0] avg_epoch_loss=2.123391\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:58 INFO 140196492334912] Epoch[23] Batch[5] avg_epoch_loss=2.352930\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:32:58 INFO 140196492334912] Epoch[23] Batch [5]#011Speed: 154.26 samples/sec#011loss=2.352930\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:59 INFO 140624803325760] Epoch[23] Batch[10] avg_epoch_loss=2.383467\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:59 INFO 140624803325760] Epoch[23] Batch [10]#011Speed: 121.77 samples/sec#011loss=2.388863\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:59 INFO 140624803325760] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11751.428842544556, \"sum\": 11751.428842544556, \"min\": 11751.428842544556}}, \"EndTime\": 1538407979.412454, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407967.660805}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.347266881 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:32:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:01 INFO 139663579178816] Epoch[24] Batch[5] avg_epoch_loss=2.229206\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:01 INFO 139663579178816] Epoch[24] Batch [5]#011Speed: 118.68 samples/sec#011loss=2.229206\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:00 INFO 140624803325760] Epoch[24] Batch[0] avg_epoch_loss=2.365765\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:00 INFO 140252856112960] Epoch[23] Batch[5] avg_epoch_loss=2.228535\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:00 INFO 140252856112960] Epoch[23] Batch [5]#011Speed: 116.81 samples/sec#011loss=2.228535\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:03 INFO 140196492334912] Epoch[23] Batch[10] avg_epoch_loss=2.272670\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:03 INFO 140196492334912] Epoch[23] Batch [10]#011Speed: 119.46 samples/sec#011loss=2.176358\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:03 INFO 140196492334912] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11834.78593826294, \"sum\": 11834.78593826294, \"min\": 11834.78593826294}}, \"EndTime\": 1538407983.461855, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407971.626836}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:03 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.703551118 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:03 INFO 140196492334912] #progress_metric: host=algo-4, completed 9 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:05 INFO 139663579178816] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11013.63492012024, \"sum\": 11013.63492012024, \"min\": 11013.63492012024}}, \"EndTime\": 1538407985.157148, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407974.143254}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.769066295 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:05 INFO 140196492334912] Epoch[24] Batch[0] avg_epoch_loss=2.248141\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:04 INFO 140252856112960] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11018.777847290039, \"sum\": 11018.777847290039, \"min\": 11018.777847290039}}, \"EndTime\": 1538407984.738605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407973.719574}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:04 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.16324449 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:04 INFO 140252856112960] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:06 INFO 140624803325760] Epoch[24] Batch[5] avg_epoch_loss=2.213270\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:06 INFO 140624803325760] Epoch[24] Batch [5]#011Speed: 124.06 samples/sec#011loss=2.213270\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:06 INFO 140252856112960] Epoch[24] Batch[0] avg_epoch_loss=2.258725\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:06 INFO 139663579178816] Epoch[25] Batch[0] avg_epoch_loss=2.240564\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:10 INFO 140196492334912] Epoch[24] Batch[5] avg_epoch_loss=2.209074\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:10 INFO 140196492334912] Epoch[24] Batch [5]#011Speed: 121.20 samples/sec#011loss=2.209074\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:10 INFO 140624803325760] Epoch[24] Batch[10] avg_epoch_loss=1.904314\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:10 INFO 140624803325760] Epoch[24] Batch [10]#011Speed: 151.68 samples/sec#011loss=1.533565\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:10 INFO 140624803325760] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10937.175035476685, \"sum\": 10937.175035476685, \"min\": 10937.175035476685}}, \"EndTime\": 1538407990.34993, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407979.412543}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.671003072 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:12 INFO 139663579178816] Epoch[25] Batch[5] avg_epoch_loss=2.132179\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:12 INFO 139663579178816] Epoch[25] Batch [5]#011Speed: 119.36 samples/sec#011loss=2.132179\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:12 INFO 140624803325760] Epoch[25] Batch[0] avg_epoch_loss=1.964092\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:33:11 INFO 140252856112960] Epoch[24] Batch[5] avg_epoch_loss=2.267479\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:11 INFO 140252856112960] Epoch[24] Batch [5]#011Speed: 119.69 samples/sec#011loss=2.267479\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:14 INFO 140196492334912] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10667.255878448486, \"sum\": 10667.255878448486, \"min\": 10667.255878448486}}, \"EndTime\": 1538407994.12941, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407983.461927}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.335688187 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:15 INFO 140196492334912] Epoch[25] Batch[0] avg_epoch_loss=2.224138\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:16 INFO 140252856112960] Epoch[24] Batch[10] avg_epoch_loss=2.279306\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:16 INFO 140252856112960] Epoch[24] Batch [10]#011Speed: 146.47 samples/sec#011loss=2.293499\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:16 INFO 140252856112960] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11416.759967803955, \"sum\": 11416.759967803955, \"min\": 11416.759967803955}}, \"EndTime\": 1538407996.155768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407984.738759}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:16 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.041803827 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:16 INFO 140252856112960] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:16 INFO 139663579178816] Epoch[25] Batch[10] avg_epoch_loss=2.161529\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:16 INFO 139663579178816] Epoch[25] Batch [10]#011Speed: 142.74 samples/sec#011loss=2.196749\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:16 INFO 139663579178816] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11538.391828536987, \"sum\": 11538.391828536987, \"min\": 11538.391828536987}}, \"EndTime\": 1538407996.695902, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407985.157248}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.22634422 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:16 INFO 140624803325760] Epoch[25] Batch[5] avg_epoch_loss=2.077122\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:16 INFO 140624803325760] Epoch[25] Batch [5]#011Speed: 156.69 samples/sec#011loss=2.077122\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:18 INFO 140252856112960] Epoch[25] Batch[0] avg_epoch_loss=2.533103\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:19 INFO 139663579178816] Epoch[26] Batch[0] avg_epoch_loss=1.949018\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:20 INFO 140624803325760] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10240.87405204773, \"sum\": 10240.87405204773, \"min\": 10240.87405204773}}, \"EndTime\": 1538408000.591087, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407990.349996}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.839781689 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:21 INFO 140196492334912] Epoch[25] Batch[5] avg_epoch_loss=2.164848\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:21 INFO 140196492334912] Epoch[25] Batch [5]#011Speed: 121.43 samples/sec#011loss=2.164848\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:22 INFO 140252856112960] Epoch[25] Batch[5] avg_epoch_loss=2.389156\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:23 INFO 139663579178816] Epoch[26] Batch[5] avg_epoch_loss=2.354877\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:23 INFO 139663579178816] Epoch[26] Batch [5]#011Speed: 155.13 samples/sec#011loss=2.354877\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:22 INFO 140624803325760] Epoch[26] Batch[0] avg_epoch_loss=2.481232\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:22 INFO 140252856112960] Epoch[25] Batch [5]#011Speed: 151.99 samples/sec#011loss=2.389156\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:25 INFO 140196492334912] Epoch[25] Batch[10] avg_epoch_loss=2.277263\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:25 INFO 140196492334912] Epoch[25] Batch [10]#011Speed: 148.90 samples/sec#011loss=2.412162\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:25 INFO 140196492334912] processed a total of 1342 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11246.156930923462, \"sum\": 11246.156930923462, \"min\": 11246.156930923462}}, \"EndTime\": 1538408005.37593, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407994.129524}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:25 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.328559772 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:25 INFO 140196492334912] #progress_metric: host=algo-4, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:26 INFO 140252856112960] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10399.79600906372, \"sum\": 10399.79600906372, \"min\": 10399.79600906372}}, \"EndTime\": 1538408006.555854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407996.155837}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:26 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.058787424 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:26 INFO 140252856112960] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:26 INFO 139663579178816] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10267.729997634888, \"sum\": 10267.729997634888, \"min\": 10267.729997634888}}, \"EndTime\": 1538408006.96395, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538407996.69597}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:26 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.200192484 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:26 INFO 139663579178816] #progress_metric: host=algo-2, completed 10 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:27 INFO 140624803325760] Epoch[26] Batch[5] avg_epoch_loss=2.558895\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:27 INFO 140624803325760] Epoch[26] Batch [5]#011Speed: 150.78 samples/sec#011loss=2.558895\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:27 INFO 140196492334912] Epoch[26] Batch[0] avg_epoch_loss=2.813611\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:28 INFO 140252856112960] Epoch[26] Batch[0] avg_epoch_loss=2.345332\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:29 INFO 139663579178816] Epoch[27] Batch[0] avg_epoch_loss=2.356673\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:31 INFO 140196492334912] Epoch[26] Batch[5] avg_epoch_loss=2.478908\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:31 INFO 140196492334912] Epoch[26] Batch [5]#011Speed: 155.23 samples/sec#011loss=2.478908\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:32 INFO 140624803325760] Epoch[26] Batch[10] avg_epoch_loss=2.510367\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:32 INFO 140624803325760] Epoch[26] Batch [10]#011Speed: 118.76 samples/sec#011loss=2.452132\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:32 INFO 140624803325760] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11961.373805999756, \"sum\": 11961.373805999756, \"min\": 11961.373805999756}}, \"EndTime\": 1538408012.552762, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408000.59116}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:32 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.013282883 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:32 INFO 140624803325760] #progress_metric: host=algo-3, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:33 INFO 140252856112960] Epoch[26] Batch[5] avg_epoch_loss=2.291771\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:33 INFO 140252856112960] Epoch[26] Batch [5]#011Speed: 151.52 samples/sec#011loss=2.291771\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:33 INFO 139663579178816] Epoch[27] Batch[5] avg_epoch_loss=2.471349\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:33 INFO 139663579178816] Epoch[27] Batch [5]#011Speed: 147.33 samples/sec#011loss=2.471349\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:34 INFO 140624803325760] Epoch[27] Batch[0] avg_epoch_loss=2.463954\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:37 INFO 140196492334912] Epoch[26] Batch[10] avg_epoch_loss=2.372286\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:37 INFO 140196492334912] Epoch[26] Batch [10]#011Speed: 119.72 samples/sec#011loss=2.244338\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:37 INFO 140196492334912] processed a total of 1342 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11801.561832427979, \"sum\": 11801.561832427979, \"min\": 11801.561832427979}}, \"EndTime\": 1538408017.177786, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408005.376001}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:37 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.712652981 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:37 INFO 140196492334912] #progress_metric: host=algo-4, completed 10 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:37 INFO 139663579178816] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10787.66393661499, \"sum\": 10787.66393661499, \"min\": 10787.66393661499}}, \"EndTime\": 1538408017.751917, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408006.964025}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:37 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.725918931 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:37 INFO 139663579178816] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:38 INFO 140252856112960] Epoch[26] Batch[10] avg_epoch_loss=2.500326\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:38 INFO 140252856112960] Epoch[26] Batch [10]#011Speed: 117.82 samples/sec#011loss=2.750593\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:38 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12092.067956924438, \"sum\": 12092.067956924438, \"min\": 12092.067956924438}}, \"EndTime\": 1538408018.648233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408006.555929}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:38 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.5903006 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:38 INFO 140252856112960] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:38 INFO 140196492334912] Epoch[27] Batch[0] avg_epoch_loss=2.187513\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:40 INFO 139663579178816] Epoch[28] Batch[0] avg_epoch_loss=2.370758\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:40 INFO 140252856112960] Epoch[27] Batch[0] avg_epoch_loss=2.040337\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:33:39 INFO 140624803325760] Epoch[27] Batch[5] avg_epoch_loss=2.257186\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:39 INFO 140624803325760] Epoch[27] Batch [5]#011Speed: 120.81 samples/sec#011loss=2.257186\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:44 INFO 140196492334912] Epoch[27] Batch[5] avg_epoch_loss=2.185365\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:44 INFO 140196492334912] Epoch[27] Batch [5]#011Speed: 119.70 samples/sec#011loss=2.185365\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:43 INFO 140624803325760] Epoch[27] Batch[10] avg_epoch_loss=2.003087\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:43 INFO 140624803325760] Epoch[27] Batch [10]#011Speed: 147.63 samples/sec#011loss=1.698168\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:43 INFO 140624803325760] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11261.396884918213, \"sum\": 11261.396884918213, \"min\": 11261.396884918213}}, \"EndTime\": 1538408023.81446, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408012.552847}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.259996742 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:44 INFO 139663579178816] Epoch[28] Batch[5] avg_epoch_loss=2.147920\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:44 INFO 139663579178816] Epoch[28] Batch [5]#011Speed: 150.24 samples/sec#011loss=2.147920\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:45 INFO 140252856112960] Epoch[27] Batch[5] avg_epoch_loss=2.284230\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:45 INFO 140252856112960] Epoch[27] Batch [5]#011Speed: 119.93 samples/sec#011loss=2.284230\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:46 INFO 140624803325760] Epoch[28] Batch[0] avg_epoch_loss=2.075808\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:48 INFO 140196492334912] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10878.273010253906, \"sum\": 10878.273010253906, \"min\": 10878.273010253906}}, \"EndTime\": 1538408028.056372, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408017.177869}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:48 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.653387888 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:48 INFO 140196492334912] #progress_metric: host=algo-4, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:48 INFO 139663579178816] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10514.3461227417, \"sum\": 10514.3461227417, \"min\": 10514.3461227417}}, \"EndTime\": 1538408028.266575, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408017.751988}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:48 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.595667481 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:48 INFO 139663579178816] #progress_metric: host=algo-2, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:49 INFO 140252856112960] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10734.071016311646, \"sum\": 10734.071016311646, \"min\": 10734.071016311646}}, \"EndTime\": 1538408029.382612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408018.648297}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:49 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.058487405 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:49 INFO 140252856112960] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:49 INFO 140196492334912] Epoch[28] Batch[0] avg_epoch_loss=2.056042\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:50 INFO 140624803325760] Epoch[28] Batch[5] avg_epoch_loss=1.954151\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:50 INFO 140624803325760] Epoch[28] Batch [5]#011Speed: 155.44 samples/sec#011loss=1.954151\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:50 INFO 139663579178816] Epoch[29] Batch[0] avg_epoch_loss=2.227391\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:50 INFO 140252856112960] Epoch[28] Batch[0] avg_epoch_loss=2.345061\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:54 INFO 140624803325760] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10251.797914505005, \"sum\": 10251.797914505005, \"min\": 10251.797914505005}}, \"EndTime\": 1538408034.066532, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408023.814526}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:54 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.392420048 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:54 INFO 140624803325760] #progress_metric: host=algo-3, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:54 INFO 139663579178816] Epoch[29] Batch[5] avg_epoch_loss=2.060424\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:54 INFO 139663579178816] Epoch[29] Batch [5]#011Speed: 154.42 samples/sec#011loss=2.060424\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:54 INFO 140196492334912] Epoch[28] Batch[5] avg_epoch_loss=2.187951\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:54 INFO 140196492334912] Epoch[28] Batch [5]#011Speed: 122.80 samples/sec#011loss=2.187951\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:33:56 INFO 140624803325760] Epoch[29] Batch[0] avg_epoch_loss=2.109718\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:56 INFO 140252856112960] Epoch[28] Batch[5] avg_epoch_loss=2.338295\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:33:56 INFO 140252856112960] Epoch[28] Batch [5]#011Speed: 120.10 samples/sec#011loss=2.338295\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:58 INFO 139663579178816] processed a total of 1205 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10292.935848236084, \"sum\": 10292.935848236084, \"min\": 10292.935848236084}}, \"EndTime\": 1538408038.559849, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408028.266661}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:58 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.069080661 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:33:58 INFO 139663579178816] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:59 INFO 140196492334912] Epoch[28] Batch[10] avg_epoch_loss=2.225680\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:59 INFO 140196492334912] Epoch[28] Batch [10]#011Speed: 149.73 samples/sec#011loss=2.270955\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:59 INFO 140196492334912] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11079.700946807861, \"sum\": 11079.700946807861, \"min\": 11079.700946807861}}, \"EndTime\": 1538408039.136377, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408028.056447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.879324453 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:33:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 11 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:00 INFO 140624803325760] Epoch[29] Batch[5] avg_epoch_loss=2.183470\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:00 INFO 140624803325760] Epoch[29] Batch [5]#011Speed: 155.28 samples/sec#011loss=2.183470\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:00 INFO 140252856112960] Epoch[28] Batch[10] avg_epoch_loss=2.293068\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:00 INFO 140252856112960] Epoch[28] Batch [10]#011Speed: 146.70 samples/sec#011loss=2.238796\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:00 INFO 140252856112960] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11251.502990722656, \"sum\": 11251.502990722656, \"min\": 11251.502990722656}}, \"EndTime\": 1538408040.634478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408029.382726}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:00 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.739185753 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:00 INFO 140252856112960] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:00 INFO 139663579178816] Epoch[30] Batch[0] avg_epoch_loss=2.300367\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:01 INFO 140196492334912] Epoch[29] Batch[0] avg_epoch_loss=2.364031\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:03 INFO 140252856112960] Epoch[29] Batch[0] avg_epoch_loss=2.191671\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:04 INFO 140624803325760] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10204.349994659424, \"sum\": 10204.349994659424, \"min\": 10204.349994659424}}, \"EndTime\": 1538408044.271188, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408034.066606}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:04 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.769434909 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:04 INFO 140624803325760] #progress_metric: host=algo-3, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:05 INFO 139663579178816] Epoch[30] Batch[5] avg_epoch_loss=2.249131\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:05 INFO 139663579178816] Epoch[30] Batch [5]#011Speed: 152.55 samples/sec#011loss=2.249131\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:05 INFO 140196492334912] Epoch[29] Batch[5] avg_epoch_loss=2.348091\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:05 INFO 140196492334912] Epoch[29] Batch [5]#011Speed: 155.97 samples/sec#011loss=2.348091\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:06 INFO 140624803325760] Epoch[30] Batch[0] avg_epoch_loss=1.986645\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:07 INFO 140252856112960] Epoch[29] Batch[5] avg_epoch_loss=2.196700\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:07 INFO 140252856112960] Epoch[29] Batch [5]#011Speed: 154.77 samples/sec#011loss=2.196700\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:34:09 INFO 140196492334912] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10236.187934875488, \"sum\": 10236.187934875488, \"min\": 10236.187934875488}}, \"EndTime\": 1538408049.372863, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408039.136447}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:09 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.872895399 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:09 INFO 140196492334912] #progress_metric: host=algo-4, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:11 INFO 140196492334912] Epoch[30] Batch[0] avg_epoch_loss=1.927061\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:10 INFO 139663579178816] Epoch[30] Batch[10] avg_epoch_loss=1.830015\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:10 INFO 139663579178816] Epoch[30] Batch [10]#011Speed: 115.68 samples/sec#011loss=1.327075\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:10 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12127.399921417236, \"sum\": 12127.399921417236, \"min\": 12127.399921417236}}, \"EndTime\": 1538408050.6876, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408038.559937}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=106.122141794 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:10 INFO 140624803325760] Epoch[30] Batch[5] avg_epoch_loss=2.163068\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:10 INFO 140624803325760] Epoch[30] Batch [5]#011Speed: 151.20 samples/sec#011loss=2.163068\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:12 INFO 140252856112960] Epoch[29] Batch[10] avg_epoch_loss=2.181909\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:12 INFO 140252856112960] Epoch[29] Batch [10]#011Speed: 120.66 samples/sec#011loss=2.164160\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:12 INFO 140252856112960] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11825.664043426514, \"sum\": 11825.664043426514, \"min\": 11825.664043426514}}, \"EndTime\": 1538408052.460497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408040.634546}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:12 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.212576271 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:12 INFO 140252856112960] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:12 INFO 139663579178816] Epoch[31] Batch[0] avg_epoch_loss=2.552280\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:14 INFO 140252856112960] Epoch[30] Batch[0] avg_epoch_loss=2.397516\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:15 INFO 140196492334912] Epoch[30] Batch[5] avg_epoch_loss=2.274905\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:15 INFO 140196492334912] Epoch[30] Batch [5]#011Speed: 155.17 samples/sec#011loss=2.274905\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:16 INFO 140624803325760] Epoch[30] Batch[10] avg_epoch_loss=1.929006\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:16 INFO 140624803325760] Epoch[30] Batch [10]#011Speed: 117.34 samples/sec#011loss=1.648132\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:16 INFO 140624803325760] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11989.3958568573, \"sum\": 11989.3958568573, \"min\": 11989.3958568573}}, \"EndTime\": 1538408056.260887, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408044.271261}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:16 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.010967691 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:16 INFO 140624803325760] #progress_metric: host=algo-3, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:17 INFO 139663579178816] Epoch[31] Batch[5] avg_epoch_loss=2.297952\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:17 INFO 139663579178816] Epoch[31] Batch [5]#011Speed: 120.36 samples/sec#011loss=2.297952\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:17 INFO 140624803325760] Epoch[31] Batch[0] avg_epoch_loss=1.977017\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:19 INFO 140252856112960] Epoch[30] Batch[5] avg_epoch_loss=2.307352\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:19 INFO 140252856112960] Epoch[30] Batch [5]#011Speed: 120.23 samples/sec#011loss=2.307352\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:19 INFO 140196492334912] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10224.165916442871, \"sum\": 10224.165916442871, \"min\": 10224.165916442871}}, \"EndTime\": 1538408059.597335, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408049.372938}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.23608604 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 12 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:21 INFO 139663579178816] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10819.399118423462, \"sum\": 10819.399118423462, \"min\": 10819.399118423462}}, \"EndTime\": 1538408061.507331, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408050.687683}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:21 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.809096959 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:21 INFO 139663579178816] #progress_metric: host=algo-2, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:23 INFO 140252856112960] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10743.23296546936, \"sum\": 10743.23296546936, \"min\": 10743.23296546936}}, \"EndTime\": 1538408063.204032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408052.460569}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:23 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.095800579 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:23 INFO 140252856112960] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:21 INFO 140196492334912] Epoch[31] Batch[0] avg_epoch_loss=2.293614\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:23 INFO 139663579178816] Epoch[32] Batch[0] avg_epoch_loss=2.264102\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:23 INFO 140624803325760] Epoch[31] Batch[5] avg_epoch_loss=2.024198\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:23 INFO 140624803325760] Epoch[31] Batch [5]#011Speed: 121.93 samples/sec#011loss=2.024198\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:24 INFO 140252856112960] Epoch[31] Batch[0] avg_epoch_loss=2.185419\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:26 INFO 140196492334912] Epoch[31] Batch[5] avg_epoch_loss=2.180744\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:26 INFO 140196492334912] Epoch[31] Batch [5]#011Speed: 152.42 samples/sec#011loss=2.180744\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:26 INFO 140624803325760] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10655.64489364624, \"sum\": 10655.64489364624, \"min\": 10655.64489364624}}, \"EndTime\": 1538408066.916852, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408056.26098}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.122521318 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 12 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:28 INFO 140624803325760] Epoch[32] Batch[0] avg_epoch_loss=2.216041\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:28 INFO 139663579178816] Epoch[32] Batch[5] avg_epoch_loss=2.195592\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:28 INFO 139663579178816] Epoch[32] Batch [5]#011Speed: 118.90 samples/sec#011loss=2.195592\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:29 INFO 140196492334912] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10312.615871429443, \"sum\": 10312.615871429443, \"min\": 10312.615871429443}}, \"EndTime\": 1538408069.910331, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408059.597412}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.330750925 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:30 INFO 140252856112960] Epoch[31] Batch[5] avg_epoch_loss=2.310856\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:30 INFO 140252856112960] Epoch[31] Batch [5]#011Speed: 120.55 samples/sec#011loss=2.310856\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:32 INFO 140196492334912] Epoch[32] Batch[0] avg_epoch_loss=2.487193\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:32 INFO 139663579178816] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10989.278793334961, \"sum\": 10989.278793334961, \"min\": 10989.278793334961}}, \"EndTime\": 1538408072.496959, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408061.507419}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:32 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.200864072 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:32 INFO 139663579178816] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:33 INFO 140252856112960] processed a total of 1207 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10761.842966079712, \"sum\": 10761.842966079712, \"min\": 10761.842966079712}}, \"EndTime\": 1538408073.966179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408063.204104}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:33 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.154363344 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:33 INFO 140252856112960] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:34:34 INFO 139663579178816] Epoch[33] Batch[0] avg_epoch_loss=2.171201\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:33 INFO 140624803325760] Epoch[32] Batch[5] avg_epoch_loss=2.345856\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:33 INFO 140624803325760] Epoch[32] Batch [5]#011Speed: 121.63 samples/sec#011loss=2.345856\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:35 INFO 140252856112960] Epoch[32] Batch[0] avg_epoch_loss=2.127245\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:36 INFO 140196492334912] Epoch[32] Batch[5] avg_epoch_loss=2.071802\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:36 INFO 140196492334912] Epoch[32] Batch [5]#011Speed: 154.84 samples/sec#011loss=2.071802\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:38 INFO 140624803325760] Epoch[32] Batch[10] avg_epoch_loss=2.534755\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:38 INFO 140624803325760] Epoch[32] Batch [10]#011Speed: 144.76 samples/sec#011loss=2.761435\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:38 INFO 140624803325760] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11354.028940200806, \"sum\": 11354.028940200806, \"min\": 11354.028940200806}}, \"EndTime\": 1538408078.271238, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408066.916951}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:38 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.174418076 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:38 INFO 140624803325760] #progress_metric: host=algo-3, completed 13 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:39 INFO 139663579178816] Epoch[33] Batch[5] avg_epoch_loss=2.225102\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:39 INFO 139663579178816] Epoch[33] Batch [5]#011Speed: 121.09 samples/sec#011loss=2.225102\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:40 INFO 140624803325760] Epoch[33] Batch[0] avg_epoch_loss=1.877982\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:40 INFO 140252856112960] Epoch[32] Batch[5] avg_epoch_loss=2.243638\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:40 INFO 140252856112960] Epoch[32] Batch [5]#011Speed: 120.83 samples/sec#011loss=2.243638\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:43 INFO 139663579178816] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10794.934034347534, \"sum\": 10794.934034347534, \"min\": 10794.934034347534}}, \"EndTime\": 1538408083.292247, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408072.497046}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:43 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.755599088 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:43 INFO 139663579178816] #progress_metric: host=algo-2, completed 13 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:44 INFO 140624803325760] Epoch[33] Batch[5] avg_epoch_loss=2.177478\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:44 INFO 140624803325760] Epoch[33] Batch [5]#011Speed: 154.46 samples/sec#011loss=2.177478\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:44 INFO 140252856112960] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10813.252925872803, \"sum\": 10813.252925872803, \"min\": 10813.252925872803}}, \"EndTime\": 1538408084.779736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408073.966253}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.488014082 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:41 INFO 140196492334912] Epoch[32] Batch[10] avg_epoch_loss=2.040574\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:41 INFO 140196492334912] Epoch[32] Batch [10]#011Speed: 119.45 samples/sec#011loss=2.003101\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:41 INFO 140196492334912] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11853.315114974976, \"sum\": 11853.315114974976, \"min\": 11853.315114974976}}, \"EndTime\": 1538408081.763967, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408069.910407}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.166742013 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:43 INFO 140196492334912] Epoch[33] Batch[0] avg_epoch_loss=2.261217\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:44 INFO 139663579178816] Epoch[34] Batch[0] avg_epoch_loss=2.318513\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:46 INFO 140252856112960] Epoch[33] Batch[0] avg_epoch_loss=2.184482\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:48 INFO 140196492334912] Epoch[33] Batch[5] avg_epoch_loss=2.133948\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:48 INFO 140196492334912] Epoch[33] Batch [5]#011Speed: 121.57 samples/sec#011loss=2.133948\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:48 INFO 140624803325760] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10591.57395362854, \"sum\": 10591.57395362854, \"min\": 10591.57395362854}}, \"EndTime\": 1538408088.863144, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408078.271322}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.488983343 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 13 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:50 INFO 139663579178816] Epoch[34] Batch[5] avg_epoch_loss=2.253278\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:50 INFO 139663579178816] Epoch[34] Batch [5]#011Speed: 121.79 samples/sec#011loss=2.253278\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:51 INFO 140624803325760] Epoch[34] Batch[0] avg_epoch_loss=2.092893\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:51 INFO 140252856112960] Epoch[33] Batch[5] avg_epoch_loss=2.185341\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:51 INFO 140252856112960] Epoch[33] Batch [5]#011Speed: 120.84 samples/sec#011loss=2.185341\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:52 INFO 140196492334912] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10689.878940582275, \"sum\": 10689.878940582275, \"min\": 10689.878940582275}}, \"EndTime\": 1538408092.454154, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408081.76404}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.680284176 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 13 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:54 INFO 139663579178816] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10755.589962005615, \"sum\": 10755.589962005615, \"min\": 10755.589962005615}}, \"EndTime\": 1538408094.048203, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408083.292342}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:54 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.448482554 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:54 INFO 139663579178816] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:55 INFO 140252856112960] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10782.42015838623, \"sum\": 10782.42015838623, \"min\": 10782.42015838623}}, \"EndTime\": 1538408095.562457, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408084.779808}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:55 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.392008641 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:55 INFO 140252856112960] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:53 INFO 140196492334912] Epoch[34] Batch[0] avg_epoch_loss=2.294711\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:55 INFO 140624803325760] Epoch[34] Batch[5] avg_epoch_loss=2.251659\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:55 INFO 140624803325760] Epoch[34] Batch [5]#011Speed: 151.02 samples/sec#011loss=2.251659\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:34:55 INFO 139663579178816] Epoch[35] Batch[0] avg_epoch_loss=2.207679\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:34:57 INFO 140252856112960] Epoch[34] Batch[0] avg_epoch_loss=2.208841\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:59 INFO 140196492334912] Epoch[34] Batch[5] avg_epoch_loss=2.265257\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:34:59 INFO 140196492334912] Epoch[34] Batch [5]#011Speed: 123.07 samples/sec#011loss=2.265257\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:59 INFO 140624803325760] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10343.84298324585, \"sum\": 10343.84298324585, \"min\": 10343.84298324585}}, \"EndTime\": 1538408099.207332, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408088.863231}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.583446758 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:34:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:00 INFO 139663579178816] Epoch[35] Batch[5] avg_epoch_loss=2.251653\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:00 INFO 139663579178816] Epoch[35] Batch [5]#011Speed: 120.69 samples/sec#011loss=2.251653\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:01 INFO 140624803325760] Epoch[35] Batch[0] avg_epoch_loss=2.496736\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:02 INFO 140196492334912] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10492.163896560669, \"sum\": 10492.163896560669, \"min\": 10492.163896560669}}, \"EndTime\": 1538408102.946625, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408092.454226}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:02 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.613561266 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:02 INFO 140196492334912] #progress_metric: host=algo-4, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:04 INFO 139663579178816] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10775.470972061157, \"sum\": 10775.470972061157, \"min\": 10775.470972061157}}, \"EndTime\": 1538408104.824038, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408094.048302}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:04 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.610671148 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:04 INFO 139663579178816] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:05 INFO 140624803325760] Epoch[35] Batch[5] avg_epoch_loss=2.266446\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:05 INFO 140624803325760] Epoch[35] Batch [5]#011Speed: 155.15 samples/sec#011loss=2.266446\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:35:02 INFO 140252856112960] Epoch[34] Batch[5] avg_epoch_loss=2.156048\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:02 INFO 140252856112960] Epoch[34] Batch [5]#011Speed: 120.65 samples/sec#011loss=2.156048\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:04 INFO 140196492334912] Epoch[35] Batch[0] avg_epoch_loss=2.101579\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:06 INFO 140252856112960] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10829.791069030762, \"sum\": 10829.791069030762, \"min\": 10829.791069030762}}, \"EndTime\": 1538408106.392603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408095.562528}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:06 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.621585439 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:06 INFO 140252856112960] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:06 INFO 139663579178816] Epoch[36] Batch[0] avg_epoch_loss=2.101382\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:08 INFO 140252856112960] Epoch[35] Batch[0] avg_epoch_loss=2.291391\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:09 INFO 140196492334912] Epoch[35] Batch[5] avg_epoch_loss=2.236021\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:09 INFO 140196492334912] Epoch[35] Batch [5]#011Speed: 123.56 samples/sec#011loss=2.236021\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:10 INFO 140624803325760] Epoch[35] Batch[10] avg_epoch_loss=2.478947\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:10 INFO 140624803325760] Epoch[35] Batch [10]#011Speed: 120.87 samples/sec#011loss=2.733948\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:10 INFO 140624803325760] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11784.260034561157, \"sum\": 11784.260034561157, \"min\": 11784.260034561157}}, \"EndTime\": 1538408110.991931, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408099.207418}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.079186386 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:11 INFO 139663579178816] Epoch[36] Batch[5] avg_epoch_loss=2.250561\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:11 INFO 139663579178816] Epoch[36] Batch [5]#011Speed: 120.28 samples/sec#011loss=2.250561\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:12 INFO 140624803325760] Epoch[36] Batch[0] avg_epoch_loss=2.148238\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:13 INFO 140196492334912] Epoch[35] Batch[10] avg_epoch_loss=2.233423\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:13 INFO 140196492334912] Epoch[35] Batch [10]#011Speed: 150.86 samples/sec#011loss=2.230304\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:13 INFO 140196492334912] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11006.947040557861, \"sum\": 11006.947040557861, \"min\": 11006.947040557861}}, \"EndTime\": 1538408113.953866, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408102.946682}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:13 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.559095806 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:13 INFO 140196492334912] #progress_metric: host=algo-4, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:16 INFO 139663579178816] Epoch[36] Batch[10] avg_epoch_loss=2.067816\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:16 INFO 139663579178816] Epoch[36] Batch [10]#011Speed: 148.99 samples/sec#011loss=1.848521\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:16 INFO 139663579178816] processed a total of 1345 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11300.91905593872, \"sum\": 11300.91905593872, \"min\": 11300.91905593872}}, \"EndTime\": 1538408116.125315, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408104.824136}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.015394848 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:13 INFO 140252856112960] Epoch[35] Batch[5] avg_epoch_loss=2.207148\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:13 INFO 140252856112960] Epoch[35] Batch [5]#011Speed: 123.68 samples/sec#011loss=2.207148\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:16 INFO 140196492334912] Epoch[36] Batch[0] avg_epoch_loss=2.134153\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:17 INFO 140624803325760] Epoch[36] Batch[5] avg_epoch_loss=2.127454\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:17 INFO 140624803325760] Epoch[36] Batch [5]#011Speed: 126.09 samples/sec#011loss=2.127454\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:17 INFO 140252856112960] Epoch[35] Batch[10] avg_epoch_loss=2.212251\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:17 INFO 140252856112960] Epoch[35] Batch [10]#011Speed: 152.94 samples/sec#011loss=2.218376\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:17 INFO 140252856112960] processed a total of 1358 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11032.427072525024, \"sum\": 11032.427072525024, \"min\": 11032.427072525024}}, \"EndTime\": 1538408117.425341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408106.392677}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=123.090532404 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:18 INFO 139663579178816] Epoch[37] Batch[0] avg_epoch_loss=2.241219\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:19 INFO 140252856112960] Epoch[36] Batch[0] avg_epoch_loss=2.325320\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:20 INFO 140196492334912] Epoch[36] Batch[5] avg_epoch_loss=2.266895\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:20 INFO 140196492334912] Epoch[36] Batch [5]#011Speed: 155.28 samples/sec#011loss=2.266895\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:21 INFO 140624803325760] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10386.943101882935, \"sum\": 10386.943101882935, \"min\": 10386.943101882935}}, \"EndTime\": 1538408121.3792, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408110.992015}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:21 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.689942507 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:21 INFO 140624803325760] #progress_metric: host=algo-3, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:22 INFO 139663579178816] Epoch[37] Batch[5] avg_epoch_loss=2.273406\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:22 INFO 139663579178816] Epoch[37] Batch [5]#011Speed: 154.53 samples/sec#011loss=2.273406\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:22 INFO 140624803325760] Epoch[37] Batch[0] avg_epoch_loss=2.366383\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:23 INFO 140252856112960] Epoch[36] Batch[5] avg_epoch_loss=2.082445\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:23 INFO 140252856112960] Epoch[36] Batch [5]#011Speed: 154.93 samples/sec#011loss=2.082445\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:25 INFO 140196492334912] Epoch[36] Batch[10] avg_epoch_loss=2.477170\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:25 INFO 140196492334912] Epoch[36] Batch [10]#011Speed: 119.31 samples/sec#011loss=2.729501\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:25 INFO 140196492334912] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11825.286149978638, \"sum\": 11825.286149978638, \"min\": 11825.286149978638}}, \"EndTime\": 1538408125.779442, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408113.953936}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:25 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.495317777 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:25 INFO 140196492334912] #progress_metric: host=algo-4, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:27 INFO 140196492334912] Epoch[37] Batch[0] avg_epoch_loss=2.026475\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:27 INFO 140252856112960] processed a total of 1171 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10223.654985427856, \"sum\": 10223.654985427856, \"min\": 10223.654985427856}}, \"EndTime\": 1538408127.64929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408117.425411}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:27 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.537038566 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:27 INFO 140252856112960] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:27 INFO 139663579178816] Epoch[37] Batch[10] avg_epoch_loss=2.329135\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:27 INFO 139663579178816] Epoch[37] Batch [10]#011Speed: 121.41 samples/sec#011loss=2.396010\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:27 INFO 139663579178816] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11802.380084991455, \"sum\": 11802.380084991455, \"min\": 11802.380084991455}}, \"EndTime\": 1538408127.92806, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408116.125413}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:27 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.807170939 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:27 INFO 139663579178816] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:27 INFO 140624803325760] Epoch[37] Batch[5] avg_epoch_loss=2.129022\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:27 INFO 140624803325760] Epoch[37] Batch [5]#011Speed: 127.30 samples/sec#011loss=2.129022\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:29 INFO 139663579178816] Epoch[38] Batch[0] avg_epoch_loss=1.979634\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:29 INFO 140252856112960] Epoch[37] Batch[0] avg_epoch_loss=2.218302\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:32 INFO 140196492334912] Epoch[37] Batch[5] avg_epoch_loss=2.095980\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:32 INFO 140196492334912] Epoch[37] Batch [5]#011Speed: 122.99 samples/sec#011loss=2.095980\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:32 INFO 140624803325760] Epoch[37] Batch[10] avg_epoch_loss=2.401276\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:32 INFO 140624803325760] Epoch[37] Batch [10]#011Speed: 144.62 samples/sec#011loss=2.727982\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:32 INFO 140624803325760] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11015.007019042969, \"sum\": 11015.007019042969, \"min\": 11015.007019042969}}, \"EndTime\": 1538408132.394516, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408121.379275}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:32 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.385605581 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:32 INFO 140624803325760] #progress_metric: host=algo-3, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:34 INFO 140252856112960] Epoch[37] Batch[5] avg_epoch_loss=2.327611\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:34 INFO 140252856112960] Epoch[37] Batch [5]#011Speed: 145.91 samples/sec#011loss=2.327611\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:35:36 INFO 140196492334912] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10479.304075241089, \"sum\": 10479.304075241089, \"min\": 10479.304075241089}}, \"EndTime\": 1538408136.25904, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408125.779519}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:36 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.376904247 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:36 INFO 140196492334912] #progress_metric: host=algo-4, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:34 INFO 139663579178816] Epoch[38] Batch[5] avg_epoch_loss=2.117144\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:34 INFO 139663579178816] Epoch[38] Batch [5]#011Speed: 123.30 samples/sec#011loss=2.117144\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:34 INFO 140624803325760] Epoch[38] Batch[0] avg_epoch_loss=2.232900\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:37 INFO 140196492334912] Epoch[38] Batch[0] avg_epoch_loss=2.588861\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:38 INFO 140252856112960] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10649.669170379639, \"sum\": 10649.669170379639, \"min\": 10649.669170379639}}, \"EndTime\": 1538408138.29927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408127.649368}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:38 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.157358642 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:38 INFO 140252856112960] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:38 INFO 139663579178816] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10559.66305732727, \"sum\": 10559.66305732727, \"min\": 10559.66305732727}}, \"EndTime\": 1538408138.488048, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408127.928144}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:38 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.225800462 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:38 INFO 139663579178816] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:38 INFO 140624803325760] Epoch[38] Batch[5] avg_epoch_loss=2.276376\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:38 INFO 140624803325760] Epoch[38] Batch [5]#011Speed: 156.33 samples/sec#011loss=2.276376\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:40 INFO 139663579178816] Epoch[39] Batch[0] avg_epoch_loss=1.872782\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:40 INFO 140252856112960] Epoch[38] Batch[0] avg_epoch_loss=2.368956\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:43 INFO 140196492334912] Epoch[38] Batch[5] avg_epoch_loss=2.183716\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:43 INFO 140196492334912] Epoch[38] Batch [5]#011Speed: 118.60 samples/sec#011loss=2.183716\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:44 INFO 140624803325760] Epoch[38] Batch[10] avg_epoch_loss=2.343647\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:44 INFO 140624803325760] Epoch[38] Batch [10]#011Speed: 121.59 samples/sec#011loss=2.424373\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:44 INFO 140624803325760] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11734.452962875366, \"sum\": 11734.452962875366, \"min\": 11734.452962875366}}, \"EndTime\": 1538408144.129254, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408132.394583}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:44 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.420314187 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:44 INFO 140624803325760] #progress_metric: host=algo-3, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:45 INFO 139663579178816] Epoch[39] Batch[5] avg_epoch_loss=2.231397\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:45 INFO 139663579178816] Epoch[39] Batch [5]#011Speed: 123.98 samples/sec#011loss=2.231397\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:45 INFO 140624803325760] Epoch[39] Batch[0] avg_epoch_loss=2.491409\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:45 INFO 140252856112960] Epoch[38] Batch[5] avg_epoch_loss=2.362417\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:45 INFO 140252856112960] Epoch[38] Batch [5]#011Speed: 145.44 samples/sec#011loss=2.362417\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:47 INFO 140196492334912] Epoch[38] Batch[10] avg_epoch_loss=2.249854\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:47 INFO 140196492334912] Epoch[38] Batch [10]#011Speed: 141.86 samples/sec#011loss=2.329219\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:47 INFO 140196492334912] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11605.196952819824, \"sum\": 11605.196952819824, \"min\": 11605.196952819824}}, \"EndTime\": 1538408147.864544, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408136.259117}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.276336484 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:49 INFO 139663579178816] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10555.695056915283, \"sum\": 10555.695056915283, \"min\": 10555.695056915283}}, \"EndTime\": 1538408149.044111, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408138.488137}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.617878308 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:50 INFO 140196492334912] Epoch[39] Batch[0] avg_epoch_loss=2.497594\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:50 INFO 140252856112960] Epoch[38] Batch[10] avg_epoch_loss=2.369027\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:50 INFO 140252856112960] Epoch[38] Batch [10]#011Speed: 115.93 samples/sec#011loss=2.376959\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:50 INFO 140252856112960] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12377.636909484863, \"sum\": 12377.636909484863, \"min\": 12377.636909484863}}, \"EndTime\": 1538408150.67722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408138.299347}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:50 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=103.653810054 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:50 INFO 140252856112960] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:50 INFO 139663579178816] Epoch[40] Batch[0] avg_epoch_loss=2.807021\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:50 INFO 140624803325760] Epoch[39] Batch[5] avg_epoch_loss=2.224264\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:50 INFO 140624803325760] Epoch[39] Batch [5]#011Speed: 123.90 samples/sec#011loss=2.224264\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:52 INFO 140252856112960] Epoch[39] Batch[0] avg_epoch_loss=2.712768\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:54 INFO 140196492334912] Epoch[39] Batch[5] avg_epoch_loss=2.302475\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:35:54 INFO 140196492334912] Epoch[39] Batch [5]#011Speed: 148.76 samples/sec#011loss=2.302475\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:54 INFO 140624803325760] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10484.469890594482, \"sum\": 10484.469890594482, \"min\": 10484.469890594482}}, \"EndTime\": 1538408154.614025, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408144.129336}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:54 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.890284647 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:54 INFO 140624803325760] #progress_metric: host=algo-3, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:55 INFO 139663579178816] Epoch[40] Batch[5] avg_epoch_loss=2.383644\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:35:55 INFO 139663579178816] Epoch[40] Batch [5]#011Speed: 123.22 samples/sec#011loss=2.383644\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:35:56 INFO 140624803325760] Epoch[40] Batch[0] avg_epoch_loss=2.561048\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:57 INFO 140252856112960] Epoch[39] Batch[5] avg_epoch_loss=2.395373\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:35:57 INFO 140252856112960] Epoch[39] Batch [5]#011Speed: 118.01 samples/sec#011loss=2.395373\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:00 INFO 139663579178816] Epoch[40] Batch[10] avg_epoch_loss=2.456191\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:00 INFO 139663579178816] Epoch[40] Batch [10]#011Speed: 150.95 samples/sec#011loss=2.543247\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:00 INFO 139663579178816] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11054.218053817749, \"sum\": 11054.218053817749, \"min\": 11054.218053817749}}, \"EndTime\": 1538408160.098688, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408149.044203}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:00 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.405189178 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:00 INFO 139663579178816] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:00 INFO 140196492334912] Epoch[39] Batch[10] avg_epoch_loss=2.142137\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:00 INFO 140196492334912] Epoch[39] Batch [10]#011Speed: 116.18 samples/sec#011loss=1.949731\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:00 INFO 140196492334912] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12185.415029525757, \"sum\": 12185.415029525757, \"min\": 12185.415029525757}}, \"EndTime\": 1538408160.05025, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408147.864611}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:00 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.53503274 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:00 INFO 140196492334912] #progress_metric: host=algo-4, completed 16 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:01 INFO 140624803325760] Epoch[40] Batch[5] avg_epoch_loss=2.280047\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:01 INFO 140624803325760] Epoch[40] Batch [5]#011Speed: 123.49 samples/sec#011loss=2.280047\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:01 INFO 140196492334912] Epoch[40] Batch[0] avg_epoch_loss=2.363453\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:36:02 INFO 140252856112960] Epoch[39] Batch[10] avg_epoch_loss=2.199580\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:02 INFO 140252856112960] Epoch[39] Batch [10]#011Speed: 144.22 samples/sec#011loss=1.964630\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:02 INFO 140252856112960] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11501.657009124756, \"sum\": 11501.657009124756, \"min\": 11501.657009124756}}, \"EndTime\": 1538408162.179186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408150.67729}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.634763948 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:02 INFO 139663579178816] Epoch[41] Batch[0] avg_epoch_loss=2.281615\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:04 INFO 140252856112960] Epoch[40] Batch[0] avg_epoch_loss=1.916929\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:05 INFO 140624803325760] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10513.433933258057, \"sum\": 10513.433933258057, \"min\": 10513.433933258057}}, \"EndTime\": 1538408165.127768, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408154.614106}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:05 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.277317901 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:05 INFO 140624803325760] #progress_metric: host=algo-3, completed 16 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:06 INFO 139663579178816] Epoch[41] Batch[5] avg_epoch_loss=2.220655\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:06 INFO 139663579178816] Epoch[41] Batch [5]#011Speed: 154.91 samples/sec#011loss=2.220655\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:06 INFO 140196492334912] Epoch[40] Batch[5] avg_epoch_loss=2.191355\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:06 INFO 140196492334912] Epoch[40] Batch [5]#011Speed: 121.52 samples/sec#011loss=2.191355\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:06 INFO 140624803325760] Epoch[41] Batch[0] avg_epoch_loss=2.715852\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:09 INFO 140252856112960] Epoch[40] Batch[5] avg_epoch_loss=2.097552\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:09 INFO 140252856112960] Epoch[40] Batch [5]#011Speed: 147.21 samples/sec#011loss=2.097552\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:10 INFO 139663579178816] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10300.472021102905, \"sum\": 10300.472021102905, \"min\": 10300.472021102905}}, \"EndTime\": 1538408170.3995, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408160.098769}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.381197086 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:10 INFO 140196492334912] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10677.569150924683, \"sum\": 10677.569150924683, \"min\": 10677.569150924683}}, \"EndTime\": 1538408170.728123, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408160.050327}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.066638991 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:12 INFO 140196492334912] Epoch[41] Batch[0] avg_epoch_loss=2.128044\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:11 INFO 140624803325760] Epoch[41] Batch[5] avg_epoch_loss=2.253708\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:11 INFO 140624803325760] Epoch[41] Batch [5]#011Speed: 124.14 samples/sec#011loss=2.253708\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:12 INFO 139663579178816] Epoch[42] Batch[0] avg_epoch_loss=2.242267\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:13 INFO 140252856112960] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10844.55394744873, \"sum\": 10844.55394744873, \"min\": 10844.55394744873}}, \"EndTime\": 1538408173.024038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408162.179257}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:13 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.895193593 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:13 INFO 140252856112960] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:15 INFO 140252856112960] Epoch[41] Batch[0] avg_epoch_loss=2.489897\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:16 INFO 140624803325760] Epoch[41] Batch[10] avg_epoch_loss=2.213323\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:16 INFO 140624803325760] Epoch[41] Batch [10]#011Speed: 152.89 samples/sec#011loss=2.164861\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:16 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11031.173944473267, \"sum\": 11031.173944473267, \"min\": 11031.173944473267}}, \"EndTime\": 1538408176.159242, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408165.127842}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:16 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.84107745 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:16 INFO 140624803325760] #progress_metric: host=algo-3, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:17 INFO 140196492334912] Epoch[41] Batch[5] avg_epoch_loss=2.163609\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:17 INFO 140196492334912] Epoch[41] Batch [5]#011Speed: 121.91 samples/sec#011loss=2.163609\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:17 INFO 139663579178816] Epoch[42] Batch[5] avg_epoch_loss=2.268330\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:17 INFO 139663579178816] Epoch[42] Batch [5]#011Speed: 148.82 samples/sec#011loss=2.268330\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:18 INFO 140624803325760] Epoch[42] Batch[0] avg_epoch_loss=2.104685\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:19 INFO 140252856112960] Epoch[41] Batch[5] avg_epoch_loss=2.142222\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:19 INFO 140252856112960] Epoch[41] Batch [5]#011Speed: 146.40 samples/sec#011loss=2.142222\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:21 INFO 140196492334912] Epoch[41] Batch[10] avg_epoch_loss=2.125724\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:21 INFO 140196492334912] Epoch[41] Batch [10]#011Speed: 149.03 samples/sec#011loss=2.080261\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:21 INFO 140196492334912] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11159.15298461914, \"sum\": 11159.15298461914, \"min\": 11159.15298461914}}, \"EndTime\": 1538408181.887599, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408170.728199}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:21 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.032796977 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:21 INFO 140196492334912] #progress_metric: host=algo-4, completed 16 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:22 INFO 140624803325760] Epoch[42] Batch[5] avg_epoch_loss=2.255450\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:22 INFO 140624803325760] Epoch[42] Batch [5]#011Speed: 156.98 samples/sec#011loss=2.255450\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:22 INFO 139663579178816] Epoch[42] Batch[10] avg_epoch_loss=2.436568\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:22 INFO 139663579178816] Epoch[42] Batch [10]#011Speed: 117.86 samples/sec#011loss=2.638455\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:22 INFO 139663579178816] processed a total of 1356 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12191.264152526855, \"sum\": 12191.264152526855, \"min\": 12191.264152526855}}, \"EndTime\": 1538408182.591115, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408170.399592}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:22 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.225317073 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:22 INFO 139663579178816] #progress_metric: host=algo-2, completed 17 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:24 INFO 139663579178816] Epoch[43] Batch[0] avg_epoch_loss=2.272201\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:24 INFO 140196492334912] Epoch[42] Batch[0] avg_epoch_loss=2.519405\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:25 INFO 140252856112960] Epoch[41] Batch[10] avg_epoch_loss=2.232102\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:25 INFO 140252856112960] Epoch[41] Batch [10]#011Speed: 116.37 samples/sec#011loss=2.339958\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:25 INFO 140252856112960] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12350.86989402771, \"sum\": 12350.86989402771, \"min\": 12350.86989402771}}, \"EndTime\": 1538408185.375213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408173.024113}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.712214568 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:26 INFO 140624803325760] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10146.31700515747, \"sum\": 10146.31700515747, \"min\": 10146.31700515747}}, \"EndTime\": 1538408186.305849, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408176.159311}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.280214991 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:27 INFO 140252856112960] Epoch[42] Batch[0] avg_epoch_loss=2.001384\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:28 INFO 140624803325760] Epoch[43] Batch[0] avg_epoch_loss=1.858847\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:36:28 INFO 140196492334912] Epoch[42] Batch[5] avg_epoch_loss=2.257169\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:28 INFO 140196492334912] Epoch[42] Batch [5]#011Speed: 150.33 samples/sec#011loss=2.257169\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:29 INFO 139663579178816] Epoch[43] Batch[5] avg_epoch_loss=2.183097\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:29 INFO 139663579178816] Epoch[43] Batch [5]#011Speed: 122.99 samples/sec#011loss=2.183097\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:32 INFO 140624803325760] Epoch[43] Batch[5] avg_epoch_loss=2.163809\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:32 INFO 140624803325760] Epoch[43] Batch [5]#011Speed: 157.63 samples/sec#011loss=2.163809\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:32 INFO 140252856112960] Epoch[42] Batch[5] avg_epoch_loss=2.289991\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:32 INFO 140252856112960] Epoch[42] Batch [5]#011Speed: 118.23 samples/sec#011loss=2.289991\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:33 INFO 139663579178816] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10511.999130249023, \"sum\": 10511.999130249023, \"min\": 10511.999130249023}}, \"EndTime\": 1538408193.103532, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408182.591279}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.290513035 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:33 INFO 140196492334912] Epoch[42] Batch[10] avg_epoch_loss=2.129725\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:33 INFO 140196492334912] Epoch[42] Batch [10]#011Speed: 120.49 samples/sec#011loss=1.976792\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:33 INFO 140196492334912] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11956.314086914062, \"sum\": 11956.314086914062, \"min\": 11956.314086914062}}, \"EndTime\": 1538408193.84421, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408181.88767}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:33 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.149982569 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:33 INFO 140196492334912] #progress_metric: host=algo-4, completed 17 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:34 INFO 139663579178816] Epoch[44] Batch[0] avg_epoch_loss=2.354459\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:35 INFO 140196492334912] Epoch[43] Batch[0] avg_epoch_loss=2.339176\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:36 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11185.028076171875, \"sum\": 11185.028076171875, \"min\": 11185.028076171875}}, \"EndTime\": 1538408196.560552, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408185.375287}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:36 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.185876709 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:36 INFO 140252856112960] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:37 INFO 140624803325760] Epoch[43] Batch[10] avg_epoch_loss=2.172570\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:37 INFO 140624803325760] Epoch[43] Batch [10]#011Speed: 124.64 samples/sec#011loss=2.183083\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:37 INFO 140624803325760] processed a total of 1395 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11547.410011291504, \"sum\": 11547.410011291504, \"min\": 11547.410011291504}}, \"EndTime\": 1538408197.853563, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408186.305924}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.805200617 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 17 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:39 INFO 140624803325760] Epoch[44] Batch[0] avg_epoch_loss=2.136489\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:38 INFO 140252856112960] Epoch[43] Batch[0] avg_epoch_loss=2.157509\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:40 INFO 139663579178816] Epoch[44] Batch[5] avg_epoch_loss=2.190177\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:40 INFO 139663579178816] Epoch[44] Batch [5]#011Speed: 119.54 samples/sec#011loss=2.190177\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:40 INFO 140196492334912] Epoch[43] Batch[5] avg_epoch_loss=2.209852\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:40 INFO 140196492334912] Epoch[43] Batch [5]#011Speed: 120.74 samples/sec#011loss=2.209852\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:43 INFO 140252856112960] Epoch[43] Batch[5] avg_epoch_loss=2.216657\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:43 INFO 140252856112960] Epoch[43] Batch [5]#011Speed: 118.82 samples/sec#011loss=2.216657\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:44 INFO 140196492334912] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10758.4969997406, \"sum\": 10758.4969997406, \"min\": 10758.4969997406}}, \"EndTime\": 1538408204.603018, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408193.844285}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:44 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.256545099 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:44 INFO 140196492334912] #progress_metric: host=algo-4, completed 17 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:44 INFO 139663579178816] Epoch[44] Batch[10] avg_epoch_loss=2.154827\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:44 INFO 139663579178816] Epoch[44] Batch [10]#011Speed: 145.67 samples/sec#011loss=2.112407\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:44 INFO 139663579178816] processed a total of 1336 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11421.393156051636, \"sum\": 11421.393156051636, \"min\": 11421.393156051636}}, \"EndTime\": 1538408204.525277, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408193.103641}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.972171884 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:45 INFO 140624803325760] Epoch[44] Batch[5] avg_epoch_loss=2.196735\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:45 INFO 140624803325760] Epoch[44] Batch [5]#011Speed: 112.91 samples/sec#011loss=2.196735\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:46 INFO 140196492334912] Epoch[44] Batch[0] avg_epoch_loss=2.388213\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:46 INFO 139663579178816] Epoch[45] Batch[0] avg_epoch_loss=2.041839\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:47 INFO 140252856112960] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10803.078889846802, \"sum\": 10803.078889846802, \"min\": 10803.078889846802}}, \"EndTime\": 1538408207.363969, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408196.560629}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:47 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.020695816 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:47 INFO 140252856112960] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:48 INFO 140624803325760] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11080.321073532104, \"sum\": 11080.321073532104, \"min\": 11080.321073532104}}, \"EndTime\": 1538408208.934194, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408197.853635}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.908994786 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:48 INFO 140252856112960] Epoch[44] Batch[0] avg_epoch_loss=2.344919\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:50 INFO 140624803325760] Epoch[45] Batch[0] avg_epoch_loss=1.879178\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:50 INFO 139663579178816] Epoch[45] Batch[5] avg_epoch_loss=2.147530\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:50 INFO 139663579178816] Epoch[45] Batch [5]#011Speed: 155.10 samples/sec#011loss=2.147530\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:51 INFO 140196492334912] Epoch[44] Batch[5] avg_epoch_loss=2.187805\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:51 INFO 140196492334912] Epoch[44] Batch [5]#011Speed: 119.83 samples/sec#011loss=2.187805\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:54 INFO 140252856112960] Epoch[44] Batch[5] avg_epoch_loss=2.258469\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:54 INFO 140252856112960] Epoch[44] Batch [5]#011Speed: 123.41 samples/sec#011loss=2.258469\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:54 INFO 139663579178816] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10324.804067611694, \"sum\": 10324.804067611694, \"min\": 10324.804067611694}}, \"EndTime\": 1538408214.850417, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408204.525361}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:54 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.293709324 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:54 INFO 139663579178816] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:56 INFO 140196492334912] Epoch[44] Batch[10] avg_epoch_loss=2.169502\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:56 INFO 140196492334912] Epoch[44] Batch [10]#011Speed: 144.59 samples/sec#011loss=2.147539\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:56 INFO 140196492334912] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11422.698020935059, \"sum\": 11422.698020935059, \"min\": 11422.698020935059}}, \"EndTime\": 1538408216.026026, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408204.603094}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:56 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.346285775 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:56 INFO 140196492334912] #progress_metric: host=algo-4, completed 18 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:55 INFO 140624803325760] Epoch[45] Batch[5] avg_epoch_loss=2.144845\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:55 INFO 140624803325760] Epoch[45] Batch [5]#011Speed: 122.81 samples/sec#011loss=2.144845\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:36:57 INFO 139663579178816] Epoch[46] Batch[0] avg_epoch_loss=1.934731\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:36:58 INFO 140196492334912] Epoch[45] Batch[0] avg_epoch_loss=2.394310\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:58 INFO 140252856112960] Epoch[44] Batch[10] avg_epoch_loss=2.325012\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:58 INFO 140252856112960] Epoch[44] Batch [10]#011Speed: 149.49 samples/sec#011loss=2.404863\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:58 INFO 140252856112960] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11086.596965789795, \"sum\": 11086.596965789795, \"min\": 11086.596965789795}}, \"EndTime\": 1538408218.450885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408207.364045}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:58 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.783066129 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:36:58 INFO 140252856112960] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:59 INFO 140624803325760] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10613.585948944092, \"sum\": 10613.585948944092, \"min\": 10613.585948944092}}, \"EndTime\": 1538408219.54811, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408208.934269}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.2217865 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:36:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 18 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:37:01 INFO 140624803325760] Epoch[46] Batch[0] avg_epoch_loss=1.949112\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:00 INFO 140252856112960] Epoch[45] Batch[0] avg_epoch_loss=2.169583\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:01 INFO 139663579178816] Epoch[46] Batch[5] avg_epoch_loss=2.190768\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:01 INFO 139663579178816] Epoch[46] Batch [5]#011Speed: 152.57 samples/sec#011loss=2.190768\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:02 INFO 140196492334912] Epoch[45] Batch[5] avg_epoch_loss=2.176479\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:02 INFO 140196492334912] Epoch[45] Batch [5]#011Speed: 152.64 samples/sec#011loss=2.176479\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:04 INFO 140252856112960] Epoch[45] Batch[5] avg_epoch_loss=2.237406\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:04 INFO 140252856112960] Epoch[45] Batch [5]#011Speed: 154.56 samples/sec#011loss=2.237406\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:06 INFO 140624803325760] Epoch[46] Batch[5] avg_epoch_loss=2.054941\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:06 INFO 140624803325760] Epoch[46] Batch [5]#011Speed: 123.16 samples/sec#011loss=2.054941\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:06 INFO 139663579178816] Epoch[46] Batch[10] avg_epoch_loss=1.887442\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:06 INFO 139663579178816] Epoch[46] Batch [10]#011Speed: 118.35 samples/sec#011loss=1.523451\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:06 INFO 139663579178816] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11984.52091217041, \"sum\": 11984.52091217041, \"min\": 11984.52091217041}}, \"EndTime\": 1538408226.835287, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408214.850508}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=107.721127537 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:08 INFO 140196492334912] Epoch[45] Batch[10] avg_epoch_loss=1.851452\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:08 INFO 140196492334912] Epoch[45] Batch [10]#011Speed: 118.50 samples/sec#011loss=1.461421\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:08 INFO 140196492334912] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11987.40005493164, \"sum\": 11987.40005493164, \"min\": 11987.40005493164}}, \"EndTime\": 1538408228.013717, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408216.026093}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:08 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.944483536 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:08 INFO 140196492334912] #progress_metric: host=algo-4, completed 18 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:08 INFO 139663579178816] Epoch[47] Batch[0] avg_epoch_loss=2.707148\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:09 INFO 140196492334912] Epoch[46] Batch[0] avg_epoch_loss=2.328981\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:10 INFO 140624803325760] Epoch[46] Batch[10] avg_epoch_loss=1.946195\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:10 INFO 140624803325760] Epoch[46] Batch [10]#011Speed: 149.54 samples/sec#011loss=1.815699\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:10 INFO 140624803325760] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11123.986959457397, \"sum\": 11123.986959457397, \"min\": 11123.986959457397}}, \"EndTime\": 1538408230.672507, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408219.548197}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.481358327 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:10 INFO 140252856112960] Epoch[45] Batch[10] avg_epoch_loss=2.227878\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:10 INFO 140252856112960] Epoch[45] Batch [10]#011Speed: 121.77 samples/sec#011loss=2.216445\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:10 INFO 140252856112960] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11779.73198890686, \"sum\": 11779.73198890686, \"min\": 11779.73198890686}}, \"EndTime\": 1538408230.230914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408218.450955}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.904795952 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:11 INFO 140252856112960] Epoch[46] Batch[0] avg_epoch_loss=2.122785\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:12 INFO 140624803325760] Epoch[47] Batch[0] avg_epoch_loss=2.290660\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:13 INFO 139663579178816] Epoch[47] Batch[5] avg_epoch_loss=2.348248\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:13 INFO 139663579178816] Epoch[47] Batch [5]#011Speed: 122.95 samples/sec#011loss=2.348248\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:14 INFO 140196492334912] Epoch[46] Batch[5] avg_epoch_loss=2.311076\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:14 INFO 140196492334912] Epoch[46] Batch [5]#011Speed: 119.59 samples/sec#011loss=2.311076\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:17 INFO 140624803325760] Epoch[47] Batch[5] avg_epoch_loss=2.125333\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:17 INFO 140624803325760] Epoch[47] Batch [5]#011Speed: 154.54 samples/sec#011loss=2.125333\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:17 INFO 140252856112960] Epoch[46] Batch[5] avg_epoch_loss=2.221643\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:17 INFO 140252856112960] Epoch[46] Batch [5]#011Speed: 123.12 samples/sec#011loss=2.221643\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:17 INFO 139663579178816] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10597.721099853516, \"sum\": 10597.721099853516, \"min\": 10597.721099853516}}, \"EndTime\": 1538408237.433343, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408226.835372}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.363754407 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 19 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:19 INFO 139663579178816] Epoch[48] Batch[0] avg_epoch_loss=2.401692\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:18 INFO 140196492334912] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10860.759973526001, \"sum\": 10860.759973526001, \"min\": 10860.759973526001}}, \"EndTime\": 1538408238.874811, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408228.013805}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:18 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.920465989 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:18 INFO 140196492334912] #progress_metric: host=algo-4, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:21 INFO 140252856112960] Epoch[46] Batch[10] avg_epoch_loss=1.925147\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:21 INFO 140252856112960] Epoch[46] Batch [10]#011Speed: 149.42 samples/sec#011loss=1.569352\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:21 INFO 140252856112960] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11154.10590171814, \"sum\": 11154.10590171814, \"min\": 11154.10590171814}}, \"EndTime\": 1538408241.385316, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408230.230981}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.430641291 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:20 INFO 140196492334912] Epoch[47] Batch[0] avg_epoch_loss=1.964169\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:20 INFO 140624803325760] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10274.333000183105, \"sum\": 10274.333000183105, \"min\": 10274.333000183105}}, \"EndTime\": 1538408240.947181, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408230.67259}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.899454611 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 19 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:23 INFO 140624803325760] Epoch[48] Batch[0] avg_epoch_loss=2.128348\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:23 INFO 140252856112960] Epoch[47] Batch[0] avg_epoch_loss=2.105675\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:24 INFO 139663579178816] Epoch[48] Batch[5] avg_epoch_loss=2.217105\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:24 INFO 139663579178816] Epoch[48] Batch [5]#011Speed: 122.11 samples/sec#011loss=2.217105\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:25 INFO 140196492334912] Epoch[47] Batch[5] avg_epoch_loss=2.193360\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:25 INFO 140196492334912] Epoch[47] Batch [5]#011Speed: 121.10 samples/sec#011loss=2.193360\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:27 INFO 140624803325760] Epoch[48] Batch[5] avg_epoch_loss=2.256463\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:27 INFO 140624803325760] Epoch[48] Batch [5]#011Speed: 154.36 samples/sec#011loss=2.256463\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:28 INFO 139663579178816] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10610.940933227539, \"sum\": 10610.940933227539, \"min\": 10610.940933227539}}, \"EndTime\": 1538408248.044644, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408237.433435}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:28 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.424515878 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:28 INFO 139663579178816] #progress_metric: host=algo-2, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:27 INFO 140252856112960] Epoch[47] Batch[5] avg_epoch_loss=2.181950\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:27 INFO 140252856112960] Epoch[47] Batch [5]#011Speed: 154.78 samples/sec#011loss=2.181950\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:29 INFO 139663579178816] Epoch[49] Batch[0] avg_epoch_loss=2.047797\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:30 INFO 140196492334912] Epoch[47] Batch[10] avg_epoch_loss=2.286179\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:30 INFO 140196492334912] Epoch[47] Batch [10]#011Speed: 149.68 samples/sec#011loss=2.397563\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:30 INFO 140196492334912] processed a total of 1345 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11215.234994888306, \"sum\": 11215.234994888306, \"min\": 11215.234994888306}}, \"EndTime\": 1538408250.090396, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408238.874901}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.92482838 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 19 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:31 INFO 140624803325760] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10224.45797920227, \"sum\": 10224.45797920227, \"min\": 10224.45797920227}}, \"EndTime\": 1538408251.171981, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408240.947266}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.374059238 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 19 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:37:32 INFO 140196492334912] Epoch[48] Batch[0] avg_epoch_loss=2.100509\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:33 INFO 140624803325760] Epoch[49] Batch[0] avg_epoch_loss=2.083270\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:33 INFO 140252856112960] Epoch[47] Batch[10] avg_epoch_loss=2.146202\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:33 INFO 140252856112960] Epoch[47] Batch [10]#011Speed: 120.43 samples/sec#011loss=2.103303\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:33 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11844.794034957886, \"sum\": 11844.794034957886, \"min\": 11844.794034957886}}, \"EndTime\": 1538408253.230409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408241.385385}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:33 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.836310693 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:33 INFO 140252856112960] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:34 INFO 139663579178816] Epoch[49] Batch[5] avg_epoch_loss=2.106287\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:34 INFO 139663579178816] Epoch[49] Batch [5]#011Speed: 121.88 samples/sec#011loss=2.106287\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:34 INFO 140252856112960] Epoch[48] Batch[0] avg_epoch_loss=1.893529\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:36 INFO 140196492334912] Epoch[48] Batch[5] avg_epoch_loss=2.179274\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:36 INFO 140196492334912] Epoch[48] Batch [5]#011Speed: 152.21 samples/sec#011loss=2.179274\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:37 INFO 140624803325760] Epoch[49] Batch[5] avg_epoch_loss=2.068823\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:37 INFO 140624803325760] Epoch[49] Batch [5]#011Speed: 154.77 samples/sec#011loss=2.068823\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:38 INFO 139663579178816] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10664.084911346436, \"sum\": 10664.084911346436, \"min\": 10664.084911346436}}, \"EndTime\": 1538408258.709083, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408248.044731}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:38 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.43273668 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:38 INFO 139663579178816] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:40 INFO 140252856112960] Epoch[48] Batch[5] avg_epoch_loss=2.151430\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:40 INFO 140252856112960] Epoch[48] Batch [5]#011Speed: 123.77 samples/sec#011loss=2.151430\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:44 INFO 140252856112960] Epoch[48] Batch[10] avg_epoch_loss=2.400896\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:44 INFO 140252856112960] Epoch[48] Batch [10]#011Speed: 150.83 samples/sec#011loss=2.700254\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:44 INFO 140252856112960] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11114.388942718506, \"sum\": 11114.388942718506, \"min\": 11114.388942718506}}, \"EndTime\": 1538408264.345151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408253.23048}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.673913466 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:40 INFO 139663579178816] Epoch[50] Batch[0] avg_epoch_loss=2.103363\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:41 INFO 140624803325760] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10246.394872665405, \"sum\": 10246.394872665405, \"min\": 10246.394872665405}}, \"EndTime\": 1538408261.418751, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408251.172068}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:41 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.16365741 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:41 INFO 140624803325760] #progress_metric: host=algo-3, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:42 INFO 140196492334912] Epoch[48] Batch[10] avg_epoch_loss=2.211949\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:42 INFO 140196492334912] Epoch[48] Batch [10]#011Speed: 118.54 samples/sec#011loss=2.251158\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:42 INFO 140196492334912] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11955.320835113525, \"sum\": 11955.320835113525, \"min\": 11955.320835113525}}, \"EndTime\": 1538408262.046049, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408250.090479}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:42 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.238836408 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:42 INFO 140196492334912] #progress_metric: host=algo-4, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:43 INFO 140196492334912] Epoch[49] Batch[0] avg_epoch_loss=2.226423\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:43 INFO 140624803325760] Epoch[50] Batch[0] avg_epoch_loss=2.430864\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:45 INFO 139663579178816] Epoch[50] Batch[5] avg_epoch_loss=2.130419\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:45 INFO 139663579178816] Epoch[50] Batch [5]#011Speed: 123.50 samples/sec#011loss=2.130419\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:46 INFO 140252856112960] Epoch[49] Batch[0] avg_epoch_loss=2.206422\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:47 INFO 140624803325760] Epoch[50] Batch[5] avg_epoch_loss=2.297198\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:47 INFO 140624803325760] Epoch[50] Batch [5]#011Speed: 153.60 samples/sec#011loss=2.297198\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:48 INFO 140196492334912] Epoch[49] Batch[5] avg_epoch_loss=2.073527\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:48 INFO 140196492334912] Epoch[49] Batch [5]#011Speed: 120.54 samples/sec#011loss=2.073527\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:49 INFO 139663579178816] Epoch[50] Batch[10] avg_epoch_loss=2.150921\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:49 INFO 139663579178816] Epoch[50] Batch [10]#011Speed: 149.79 samples/sec#011loss=2.175523\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:49 INFO 139663579178816] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11202.147960662842, \"sum\": 11202.147960662842, \"min\": 11202.147960662842}}, \"EndTime\": 1538408269.911591, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408258.709174}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.636618348 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:50 INFO 140252856112960] Epoch[49] Batch[5] avg_epoch_loss=2.220772\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:50 INFO 140252856112960] Epoch[49] Batch [5]#011Speed: 155.81 samples/sec#011loss=2.220772\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:52 INFO 140196492334912] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10793.81799697876, \"sum\": 10793.81799697876, \"min\": 10793.81799697876}}, \"EndTime\": 1538408272.840203, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408262.046143}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.176234046 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:52 INFO 139663579178816] Epoch[51] Batch[0] avg_epoch_loss=2.058707\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:53 INFO 140624803325760] Epoch[50] Batch[10] avg_epoch_loss=2.313803\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:53 INFO 140624803325760] Epoch[50] Batch [10]#011Speed: 120.85 samples/sec#011loss=2.333729\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:53 INFO 140624803325760] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11863.825798034668, \"sum\": 11863.825798034668, \"min\": 11863.825798034668}}, \"EndTime\": 1538408273.282922, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408261.418841}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.587094614 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:54 INFO 140196492334912] Epoch[50] Batch[0] avg_epoch_loss=2.057855\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:54 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10176.112174987793, \"sum\": 10176.112174987793, \"min\": 10176.112174987793}}, \"EndTime\": 1538408274.521561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408264.345218}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:54 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=123.22843113 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:54 INFO 140252856112960] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:37:54 INFO 140624803325760] Epoch[51] Batch[0] avg_epoch_loss=2.406797\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:56 INFO 139663579178816] Epoch[51] Batch[5] avg_epoch_loss=2.187166\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:37:56 INFO 139663579178816] Epoch[51] Batch [5]#011Speed: 154.64 samples/sec#011loss=2.187166\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:37:56 INFO 140252856112960] Epoch[50] Batch[0] avg_epoch_loss=2.137515\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:00 INFO 139663579178816] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10224.684953689575, \"sum\": 10224.684953689575, \"min\": 10224.684953689575}}, \"EndTime\": 1538408280.13661, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408269.911675}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:00 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.207650734 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:00 INFO 139663579178816] #progress_metric: host=algo-2, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:59 INFO 140196492334912] Epoch[50] Batch[5] avg_epoch_loss=2.172944\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:37:59 INFO 140196492334912] Epoch[50] Batch [5]#011Speed: 119.99 samples/sec#011loss=2.172944\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:00 INFO 140624803325760] Epoch[51] Batch[5] avg_epoch_loss=2.008427\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:00 INFO 140624803325760] Epoch[51] Batch [5]#011Speed: 123.54 samples/sec#011loss=2.008427\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:02 INFO 139663579178816] Epoch[52] Batch[0] avg_epoch_loss=2.008813\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:04 INFO 140196492334912] Epoch[50] Batch[10] avg_epoch_loss=2.035338\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:04 INFO 140196492334912] Epoch[50] Batch [10]#011Speed: 145.34 samples/sec#011loss=1.870211\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:04 INFO 140196492334912] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11400.779008865356, \"sum\": 11400.779008865356, \"min\": 11400.779008865356}}, \"EndTime\": 1538408284.241322, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408272.840289}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:04 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.81547655 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:04 INFO 140196492334912] #progress_metric: host=algo-4, completed 20 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:04 INFO 140624803325760] Epoch[51] Batch[10] avg_epoch_loss=2.099536\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:04 INFO 140624803325760] Epoch[51] Batch [10]#011Speed: 150.19 samples/sec#011loss=2.208867\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:04 INFO 140624803325760] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11074.94306564331, \"sum\": 11074.94306564331, \"min\": 11074.94306564331}}, \"EndTime\": 1538408284.358191, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408273.283007}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:04 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.026428124 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:04 INFO 140624803325760] #progress_metric: host=algo-3, completed 20 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:38:00 INFO 140252856112960] Epoch[50] Batch[5] avg_epoch_loss=1.996671\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:00 INFO 140252856112960] Epoch[50] Batch [5]#011Speed: 155.88 samples/sec#011loss=1.996671\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:06 INFO 140196492334912] Epoch[51] Batch[0] avg_epoch_loss=1.819067\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:06 INFO 140624803325760] Epoch[52] Batch[0] avg_epoch_loss=2.159172\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:06 INFO 140252856112960] Epoch[50] Batch[10] avg_epoch_loss=1.787299\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:06 INFO 140252856112960] Epoch[50] Batch [10]#011Speed: 121.72 samples/sec#011loss=1.536052\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:06 INFO 140252856112960] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11706.650972366333, \"sum\": 11706.650972366333, \"min\": 11706.650972366333}}, \"EndTime\": 1538408286.22852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408274.521636}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:06 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.473939193 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:06 INFO 140252856112960] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:06 INFO 139663579178816] Epoch[52] Batch[5] avg_epoch_loss=1.994018\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:06 INFO 139663579178816] Epoch[52] Batch [5]#011Speed: 154.43 samples/sec#011loss=1.994018\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:07 INFO 140252856112960] Epoch[51] Batch[0] avg_epoch_loss=1.870892\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:10 INFO 140624803325760] Epoch[52] Batch[5] avg_epoch_loss=2.206651\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:10 INFO 140624803325760] Epoch[52] Batch [5]#011Speed: 156.70 samples/sec#011loss=2.206651\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:10 INFO 140196492334912] Epoch[51] Batch[5] avg_epoch_loss=2.028306\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:10 INFO 140196492334912] Epoch[51] Batch [5]#011Speed: 152.18 samples/sec#011loss=2.028306\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:11 INFO 139663579178816] Epoch[52] Batch[10] avg_epoch_loss=2.101163\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:11 INFO 139663579178816] Epoch[52] Batch [10]#011Speed: 120.65 samples/sec#011loss=2.229736\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:11 INFO 139663579178816] processed a total of 1379 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11851.210117340088, \"sum\": 11851.210117340088, \"min\": 11851.210117340088}}, \"EndTime\": 1538408291.988159, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408280.136696}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:11 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.358180104 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:11 INFO 139663579178816] #progress_metric: host=algo-2, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:13 INFO 140252856112960] Epoch[51] Batch[5] avg_epoch_loss=2.105530\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:13 INFO 140252856112960] Epoch[51] Batch [5]#011Speed: 121.18 samples/sec#011loss=2.105530\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:13 INFO 139663579178816] Epoch[53] Batch[0] avg_epoch_loss=2.077730\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:16 INFO 140196492334912] Epoch[51] Batch[10] avg_epoch_loss=2.373502\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:16 INFO 140196492334912] Epoch[51] Batch [10]#011Speed: 117.11 samples/sec#011loss=2.787738\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:16 INFO 140196492334912] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11993.138074874878, \"sum\": 11993.138074874878, \"min\": 11993.138074874878}}, \"EndTime\": 1538408296.234823, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408284.241404}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:16 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.893274859 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:16 INFO 140196492334912] #progress_metric: host=algo-4, completed 20 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:16 INFO 140624803325760] Epoch[52] Batch[10] avg_epoch_loss=2.241216\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:16 INFO 140624803325760] Epoch[52] Batch [10]#011Speed: 120.91 samples/sec#011loss=2.282694\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:16 INFO 140624803325760] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11743.725061416626, \"sum\": 11743.725061416626, \"min\": 11743.725061416626}}, \"EndTime\": 1538408296.10224, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408284.35827}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:16 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.589245531 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:16 INFO 140624803325760] #progress_metric: host=algo-3, completed 21 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:17 INFO 140624803325760] Epoch[53] Batch[0] avg_epoch_loss=2.598294\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:17 INFO 140252856112960] Epoch[51] Batch[10] avg_epoch_loss=2.370045\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:17 INFO 140252856112960] Epoch[51] Batch [10]#011Speed: 147.28 samples/sec#011loss=2.687463\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:17 INFO 140252856112960] processed a total of 1337 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11273.832082748413, \"sum\": 11273.832082748413, \"min\": 11273.832082748413}}, \"EndTime\": 1538408297.502702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408286.2286}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.591618126 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:17 INFO 140196492334912] Epoch[52] Batch[0] avg_epoch_loss=2.315705\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:18 INFO 139663579178816] Epoch[53] Batch[5] avg_epoch_loss=2.365311\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:18 INFO 139663579178816] Epoch[53] Batch [5]#011Speed: 119.80 samples/sec#011loss=2.365311\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:19 INFO 140252856112960] Epoch[52] Batch[0] avg_epoch_loss=2.038080\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:22 INFO 139663579178816] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10842.988014221191, \"sum\": 10842.988014221191, \"min\": 10842.988014221191}}, \"EndTime\": 1538408302.831481, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408291.988244}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:22 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.954859375 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:22 INFO 139663579178816] #progress_metric: host=algo-2, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:23 INFO 140196492334912] Epoch[52] Batch[5] avg_epoch_loss=2.167443\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:23 INFO 140196492334912] Epoch[52] Batch [5]#011Speed: 117.51 samples/sec#011loss=2.167443\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:23 INFO 140624803325760] Epoch[53] Batch[5] avg_epoch_loss=2.250509\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:23 INFO 140624803325760] Epoch[53] Batch [5]#011Speed: 119.24 samples/sec#011loss=2.250509\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:23 INFO 140252856112960] Epoch[52] Batch[5] avg_epoch_loss=2.164423\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:23 INFO 140252856112960] Epoch[52] Batch [5]#011Speed: 151.89 samples/sec#011loss=2.164423\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:24 INFO 139663579178816] Epoch[54] Batch[0] avg_epoch_loss=2.056744\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:26 INFO 140624803325760] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10713.503122329712, \"sum\": 10713.503122329712, \"min\": 10713.503122329712}}, \"EndTime\": 1538408306.816071, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408296.102327}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.447491906 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:27 INFO 140196492334912] Epoch[52] Batch[10] avg_epoch_loss=2.202448\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:27 INFO 140196492334912] Epoch[52] Batch [10]#011Speed: 144.93 samples/sec#011loss=2.244453\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:27 INFO 140196492334912] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11476.922035217285, \"sum\": 11476.922035217285, \"min\": 11476.922035217285}}, \"EndTime\": 1538408307.712077, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408296.234915}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:27 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.26954536 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:27 INFO 140196492334912] #progress_metric: host=algo-4, completed 21 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:28 INFO 140624803325760] Epoch[54] Batch[0] avg_epoch_loss=2.341087\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:27 INFO 140252856112960] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10421.169996261597, \"sum\": 10421.169996261597, \"min\": 10421.169996261597}}, \"EndTime\": 1538408307.924176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408297.502774}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:27 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=122.153854059 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:27 INFO 140252856112960] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:29 INFO 139663579178816] Epoch[54] Batch[5] avg_epoch_loss=2.119859\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:29 INFO 139663579178816] Epoch[54] Batch [5]#011Speed: 121.58 samples/sec#011loss=2.119859\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:30 INFO 140196492334912] Epoch[53] Batch[0] avg_epoch_loss=2.168693\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:30 INFO 140252856112960] Epoch[53] Batch[0] avg_epoch_loss=1.926137\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:33 INFO 139663579178816] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10699.326038360596, \"sum\": 10699.326038360596, \"min\": 10699.326038360596}}, \"EndTime\": 1538408313.531209, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408302.83158}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.986587877 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:34 INFO 140252856112960] Epoch[53] Batch[5] avg_epoch_loss=1.944675\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:34 INFO 140252856112960] Epoch[53] Batch [5]#011Speed: 147.50 samples/sec#011loss=1.944675\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:38:33 INFO 140624803325760] Epoch[54] Batch[5] avg_epoch_loss=2.147828\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:33 INFO 140624803325760] Epoch[54] Batch [5]#011Speed: 121.93 samples/sec#011loss=2.147828\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:34 INFO 140196492334912] Epoch[53] Batch[5] avg_epoch_loss=2.208250\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:34 INFO 140196492334912] Epoch[53] Batch [5]#011Speed: 150.54 samples/sec#011loss=2.208250\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:35 INFO 139663579178816] Epoch[55] Batch[0] avg_epoch_loss=1.839947\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:37 INFO 140624803325760] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10626.161098480225, \"sum\": 10626.161098480225, \"min\": 10626.161098480225}}, \"EndTime\": 1538408317.442622, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408306.816142}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.103564042 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:38 INFO 140196492334912] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10500.05292892456, \"sum\": 10500.05292892456, \"min\": 10500.05292892456}}, \"EndTime\": 1538408318.212464, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408307.71216}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:38 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.426409805 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:38 INFO 140196492334912] #progress_metric: host=algo-4, completed 21 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:39 INFO 140624803325760] Epoch[55] Batch[0] avg_epoch_loss=2.333019\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:40 INFO 140196492334912] Epoch[54] Batch[0] avg_epoch_loss=2.127252\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:40 INFO 140252856112960] Epoch[53] Batch[10] avg_epoch_loss=2.101481\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:40 INFO 140252856112960] Epoch[53] Batch [10]#011Speed: 115.15 samples/sec#011loss=2.289649\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:40 INFO 140252856112960] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12351.047992706299, \"sum\": 12351.047992706299, \"min\": 12351.047992706299}}, \"EndTime\": 1538408320.275531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408307.924254}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=104.929330046 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:40 INFO 139663579178816] Epoch[55] Batch[5] avg_epoch_loss=2.080652\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:40 INFO 139663579178816] Epoch[55] Batch [5]#011Speed: 121.66 samples/sec#011loss=2.080652\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:41 INFO 140252856112960] Epoch[54] Batch[0] avg_epoch_loss=2.202150\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:44 INFO 139663579178816] Epoch[55] Batch[10] avg_epoch_loss=2.061924\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:44 INFO 139663579178816] Epoch[55] Batch [10]#011Speed: 149.39 samples/sec#011loss=2.039450\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:44 INFO 139663579178816] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11201.545000076294, \"sum\": 11201.545000076294, \"min\": 11201.545000076294}}, \"EndTime\": 1538408324.733114, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408313.531304}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.303915964 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:44 INFO 140624803325760] Epoch[55] Batch[5] avg_epoch_loss=2.157247\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:44 INFO 140624803325760] Epoch[55] Batch [5]#011Speed: 124.17 samples/sec#011loss=2.157247\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:44 INFO 140196492334912] Epoch[54] Batch[5] avg_epoch_loss=2.072208\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:44 INFO 140196492334912] Epoch[54] Batch [5]#011Speed: 149.33 samples/sec#011loss=2.072208\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:47 INFO 139663579178816] Epoch[56] Batch[0] avg_epoch_loss=2.501946\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:47 INFO 140252856112960] Epoch[54] Batch[5] avg_epoch_loss=2.231773\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:47 INFO 140252856112960] Epoch[54] Batch [5]#011Speed: 119.52 samples/sec#011loss=2.231773\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:47 INFO 140624803325760] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10497.520923614502, \"sum\": 10497.520923614502, \"min\": 10497.520923614502}}, \"EndTime\": 1538408327.940448, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408317.442693}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:47 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.312406996 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:47 INFO 140624803325760] #progress_metric: host=algo-3, completed 22 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:49 INFO 140624803325760] Epoch[56] Batch[0] avg_epoch_loss=1.666708\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:50 INFO 140196492334912] Epoch[54] Batch[10] avg_epoch_loss=2.053990\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:50 INFO 140196492334912] Epoch[54] Batch [10]#011Speed: 115.65 samples/sec#011loss=2.032129\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:50 INFO 140196492334912] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12224.009990692139, \"sum\": 12224.009990692139, \"min\": 12224.009990692139}}, \"EndTime\": 1538408330.436824, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408318.212551}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:50 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.51040507 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:50 INFO 140196492334912] #progress_metric: host=algo-4, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:51 INFO 140252856112960] Epoch[54] Batch[10] avg_epoch_loss=2.187857\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:51 INFO 140252856112960] Epoch[54] Batch [10]#011Speed: 145.90 samples/sec#011loss=2.135159\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:51 INFO 140252856112960] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11429.884910583496, \"sum\": 11429.884910583496, \"min\": 11429.884910583496}}, \"EndTime\": 1538408331.705731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408320.275619}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.598534928 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:51 INFO 139663579178816] Epoch[56] Batch[5] avg_epoch_loss=2.321789\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:51 INFO 139663579178816] Epoch[56] Batch [5]#011Speed: 149.88 samples/sec#011loss=2.321789\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:52 INFO 140196492334912] Epoch[55] Batch[0] avg_epoch_loss=2.496386\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:55 INFO 139663579178816] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10775.657892227173, \"sum\": 10775.657892227173, \"min\": 10775.657892227173}}, \"EndTime\": 1538408335.509107, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408324.733198}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:55 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.21674492 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:55 INFO 139663579178816] #progress_metric: host=algo-2, completed 22 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:54 INFO 140624803325760] Epoch[56] Batch[5] avg_epoch_loss=2.021571\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:54 INFO 140624803325760] Epoch[56] Batch [5]#011Speed: 123.24 samples/sec#011loss=2.021571\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:54 INFO 140252856112960] Epoch[55] Batch[0] avg_epoch_loss=1.915756\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:57 INFO 140196492334912] Epoch[55] Batch[5] avg_epoch_loss=2.122165\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:38:57 INFO 140196492334912] Epoch[55] Batch [5]#011Speed: 118.91 samples/sec#011loss=2.122165\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:38:57 INFO 139663579178816] Epoch[57] Batch[0] avg_epoch_loss=2.139616\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:58 INFO 140624803325760] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10535.418033599854, \"sum\": 10535.418033599854, \"min\": 10535.418033599854}}, \"EndTime\": 1538408338.476187, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408327.940521}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.215643563 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:38:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:58 INFO 140252856112960] Epoch[55] Batch[5] avg_epoch_loss=2.103687\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:38:58 INFO 140252856112960] Epoch[55] Batch [5]#011Speed: 147.68 samples/sec#011loss=2.103687\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:00 INFO 140624803325760] Epoch[57] Batch[0] avg_epoch_loss=2.383743\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:02 INFO 139663579178816] Epoch[57] Batch[5] avg_epoch_loss=2.016587\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:02 INFO 139663579178816] Epoch[57] Batch [5]#011Speed: 150.93 samples/sec#011loss=2.016587\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:01 INFO 140196492334912] Epoch[55] Batch[10] avg_epoch_loss=2.104720\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:01 INFO 140196492334912] Epoch[55] Batch [10]#011Speed: 142.53 samples/sec#011loss=2.083785\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:01 INFO 140196492334912] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11516.95704460144, \"sum\": 11516.95704460144, \"min\": 11516.95704460144}}, \"EndTime\": 1538408341.954134, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408330.436924}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:01 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.004602563 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:01 INFO 140196492334912] #progress_metric: host=algo-4, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:04 INFO 140196492334912] Epoch[56] Batch[0] avg_epoch_loss=2.068493\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:39:03 INFO 140252856112960] Epoch[55] Batch[10] avg_epoch_loss=2.149027\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:03 INFO 140252856112960] Epoch[55] Batch [10]#011Speed: 115.97 samples/sec#011loss=2.203435\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:03 INFO 140252856112960] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12240.357160568237, \"sum\": 12240.357160568237, \"min\": 12240.357160568237}}, \"EndTime\": 1538408343.946388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408331.705802}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:03 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=105.959965572 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:03 INFO 140252856112960] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:05 INFO 140624803325760] Epoch[57] Batch[5] avg_epoch_loss=2.167389\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:05 INFO 140624803325760] Epoch[57] Batch [5]#011Speed: 123.90 samples/sec#011loss=2.167389\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:05 INFO 140252856112960] Epoch[56] Batch[0] avg_epoch_loss=1.873622\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:05 INFO 139663579178816] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10459.851026535034, \"sum\": 10459.851026535034, \"min\": 10459.851026535034}}, \"EndTime\": 1538408345.969311, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408335.509199}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.74582681 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:08 INFO 139663579178816] Epoch[58] Batch[0] avg_epoch_loss=1.859913\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:08 INFO 140196492334912] Epoch[56] Batch[5] avg_epoch_loss=2.039873\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:08 INFO 140196492334912] Epoch[56] Batch [5]#011Speed: 150.46 samples/sec#011loss=2.039873\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:08 INFO 140624803325760] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10471.262216567993, \"sum\": 10471.262216567993, \"min\": 10471.262216567993}}, \"EndTime\": 1538408348.94782, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408338.47626}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.086600914 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 23 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:10 INFO 140624803325760] Epoch[58] Batch[0] avg_epoch_loss=1.964027\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:10 INFO 140252856112960] Epoch[56] Batch[5] avg_epoch_loss=2.088215\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:10 INFO 140252856112960] Epoch[56] Batch [5]#011Speed: 119.40 samples/sec#011loss=2.088215\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:14 INFO 140252856112960] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10837.575912475586, \"sum\": 10837.575912475586, \"min\": 10837.575912475586}}, \"EndTime\": 1538408354.78432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408343.946468}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.737312263 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:12 INFO 139663579178816] Epoch[58] Batch[5] avg_epoch_loss=1.990994\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:12 INFO 139663579178816] Epoch[58] Batch [5]#011Speed: 149.99 samples/sec#011loss=1.990994\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:14 INFO 140196492334912] Epoch[56] Batch[10] avg_epoch_loss=2.044984\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:14 INFO 140196492334912] Epoch[56] Batch [10]#011Speed: 115.78 samples/sec#011loss=2.051117\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:14 INFO 140196492334912] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12186.77020072937, \"sum\": 12186.77020072937, \"min\": 12186.77020072937}}, \"EndTime\": 1538408354.141232, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408341.954215}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=107.328397353 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 22 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:15 INFO 140624803325760] Epoch[58] Batch[5] avg_epoch_loss=2.096353\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:15 INFO 140624803325760] Epoch[58] Batch [5]#011Speed: 126.47 samples/sec#011loss=2.096353\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:15 INFO 140196492334912] Epoch[57] Batch[0] avg_epoch_loss=2.212672\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:16 INFO 140252856112960] Epoch[57] Batch[0] avg_epoch_loss=2.169046\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:18 INFO 139663579178816] Epoch[58] Batch[10] avg_epoch_loss=2.043778\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:18 INFO 139663579178816] Epoch[58] Batch [10]#011Speed: 118.47 samples/sec#011loss=2.107118\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:18 INFO 139663579178816] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12083.460092544556, \"sum\": 12083.460092544556, \"min\": 12083.460092544556}}, \"EndTime\": 1538408358.053118, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408345.969404}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:18 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=106.425335815 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:18 INFO 139663579178816] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:19 INFO 140624803325760] Epoch[58] Batch[10] avg_epoch_loss=1.920753\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:19 INFO 140624803325760] Epoch[58] Batch [10]#011Speed: 153.47 samples/sec#011loss=1.710033\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:19 INFO 140624803325760] processed a total of 1338 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10862.313985824585, \"sum\": 10862.313985824585, \"min\": 10862.313985824585}}, \"EndTime\": 1538408359.810438, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408348.947894}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:19 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.176995016 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:19 INFO 140624803325760] #progress_metric: host=algo-3, completed 23 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:19 INFO 139663579178816] Epoch[59] Batch[0] avg_epoch_loss=2.402578\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:21 INFO 140196492334912] Epoch[57] Batch[5] avg_epoch_loss=1.946657\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:21 INFO 140196492334912] Epoch[57] Batch [5]#011Speed: 118.98 samples/sec#011loss=1.946657\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:21 INFO 140252856112960] Epoch[57] Batch[5] avg_epoch_loss=2.261476\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:21 INFO 140252856112960] Epoch[57] Batch [5]#011Speed: 122.82 samples/sec#011loss=2.261476\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:22 INFO 140624803325760] Epoch[59] Batch[0] avg_epoch_loss=1.673749\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:24 INFO 140196492334912] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10834.824800491333, \"sum\": 10834.824800491333, \"min\": 10834.824800491333}}, \"EndTime\": 1538408364.976456, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408354.141315}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:24 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.33685829 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:24 INFO 140196492334912] #progress_metric: host=algo-4, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:25 INFO 140252856112960] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10624.290943145752, \"sum\": 10624.290943145752, \"min\": 10624.290943145752}}, \"EndTime\": 1538408365.408942, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408354.784395}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.383256323 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:26 INFO 140624803325760] Epoch[59] Batch[5] avg_epoch_loss=2.098062\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:26 INFO 140624803325760] Epoch[59] Batch [5]#011Speed: 152.88 samples/sec#011loss=2.098062\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:39:24 INFO 139663579178816] Epoch[59] Batch[5] avg_epoch_loss=2.186164\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:24 INFO 139663579178816] Epoch[59] Batch [5]#011Speed: 123.35 samples/sec#011loss=2.186164\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:26 INFO 140196492334912] Epoch[58] Batch[0] avg_epoch_loss=1.835608\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:27 INFO 140252856112960] Epoch[58] Batch[0] avg_epoch_loss=2.224946\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:29 INFO 139663579178816] Epoch[59] Batch[10] avg_epoch_loss=2.147505\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:29 INFO 139663579178816] Epoch[59] Batch [10]#011Speed: 147.87 samples/sec#011loss=2.101114\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:29 INFO 139663579178816] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11180.145978927612, \"sum\": 11180.145978927612, \"min\": 11180.145978927612}}, \"EndTime\": 1538408369.233591, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408358.053207}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.203149136 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:30 INFO 140624803325760] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10390.32506942749, \"sum\": 10390.32506942749, \"min\": 10390.32506942749}}, \"EndTime\": 1538408370.201061, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408359.810507}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:30 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.223172154 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:30 INFO 140624803325760] #progress_metric: host=algo-3, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:31 INFO 139663579178816] Epoch[60] Batch[0] avg_epoch_loss=2.049158\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:31 INFO 140196492334912] Epoch[58] Batch[5] avg_epoch_loss=2.095431\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:31 INFO 140196492334912] Epoch[58] Batch [5]#011Speed: 120.21 samples/sec#011loss=2.095431\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:32 INFO 140624803325760] Epoch[60] Batch[0] avg_epoch_loss=2.130152\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:32 INFO 140252856112960] Epoch[58] Batch[5] avg_epoch_loss=2.127131\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:32 INFO 140252856112960] Epoch[58] Batch [5]#011Speed: 123.54 samples/sec#011loss=2.127131\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:35 INFO 139663579178816] Epoch[60] Batch[5] avg_epoch_loss=2.119873\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:35 INFO 139663579178816] Epoch[60] Batch [5]#011Speed: 149.80 samples/sec#011loss=2.119873\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:36 INFO 140196492334912] Epoch[58] Batch[10] avg_epoch_loss=2.144977\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:36 INFO 140196492334912] Epoch[58] Batch [10]#011Speed: 144.94 samples/sec#011loss=2.204432\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:36 INFO 140196492334912] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11388.041973114014, \"sum\": 11388.041973114014, \"min\": 11388.041973114014}}, \"EndTime\": 1538408376.364847, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408364.976544}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:36 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.11951226 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:36 INFO 140196492334912] #progress_metric: host=algo-4, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:36 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10604.30097579956, \"sum\": 10604.30097579956, \"min\": 10604.30097579956}}, \"EndTime\": 1538408376.013567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408365.409016}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:36 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.252700592 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:36 INFO 140252856112960] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:36 INFO 140624803325760] Epoch[60] Batch[5] avg_epoch_loss=2.071543\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:36 INFO 140624803325760] Epoch[60] Batch [5]#011Speed: 148.90 samples/sec#011loss=2.071543\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:37 INFO 140252856112960] Epoch[59] Batch[0] avg_epoch_loss=1.950556\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:38 INFO 140196492334912] Epoch[59] Batch[0] avg_epoch_loss=2.114360\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:40 INFO 140624803325760] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10550.33802986145, \"sum\": 10550.33802986145, \"min\": 10550.33802986145}}, \"EndTime\": 1538408380.751706, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408370.201136}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.658405906 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:41 INFO 139663579178816] Epoch[60] Batch[10] avg_epoch_loss=2.121466\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:41 INFO 139663579178816] Epoch[60] Batch [10]#011Speed: 118.81 samples/sec#011loss=2.123377\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:41 INFO 139663579178816] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12073.246002197266, \"sum\": 12073.246002197266, \"min\": 12073.246002197266}}, \"EndTime\": 1538408381.307132, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408369.23366}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=107.922981245 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:42 INFO 140252856112960] Epoch[59] Batch[5] avg_epoch_loss=2.155960\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:42 INFO 140252856112960] Epoch[59] Batch [5]#011Speed: 122.13 samples/sec#011loss=2.155960\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:42 INFO 139663579178816] Epoch[61] Batch[0] avg_epoch_loss=1.965105\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:42 INFO 140196492334912] Epoch[59] Batch[5] avg_epoch_loss=2.215703\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:42 INFO 140196492334912] Epoch[59] Batch [5]#011Speed: 153.31 samples/sec#011loss=2.215703\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:46 INFO 140252856112960] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10610.821962356567, \"sum\": 10610.821962356567, \"min\": 10610.821962356567}}, \"EndTime\": 1538408386.624716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408376.013639}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.536067451 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:43 INFO 140624803325760] Epoch[61] Batch[0] avg_epoch_loss=2.145765\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:47 INFO 140624803325760] Epoch[61] Batch[5] avg_epoch_loss=2.151620\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:47 INFO 140624803325760] Epoch[61] Batch [5]#011Speed: 153.61 samples/sec#011loss=2.151620\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:48 INFO 139663579178816] Epoch[61] Batch[5] avg_epoch_loss=2.121406\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:48 INFO 139663579178816] Epoch[61] Batch [5]#011Speed: 119.64 samples/sec#011loss=2.121406\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:48 INFO 140196492334912] Epoch[59] Batch[10] avg_epoch_loss=2.337980\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:48 INFO 140196492334912] Epoch[59] Batch [10]#011Speed: 118.93 samples/sec#011loss=2.484712\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:48 INFO 140196492334912] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11997.467994689941, \"sum\": 11997.467994689941, \"min\": 11997.467994689941}}, \"EndTime\": 1538408388.362642, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408376.364929}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:48 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.772409268 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:48 INFO 140196492334912] #progress_metric: host=algo-4, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:48 INFO 140252856112960] Epoch[60] Batch[0] avg_epoch_loss=1.886425\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:49 INFO 140196492334912] Epoch[60] Batch[0] avg_epoch_loss=1.787323\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:51 INFO 140624803325760] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10368.531942367554, \"sum\": 10368.531942367554, \"min\": 10368.531942367554}}, \"EndTime\": 1538408391.120539, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408380.75178}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.591359122 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 24 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:52 INFO 139663579178816] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10852.163076400757, \"sum\": 10852.163076400757, \"min\": 10852.163076400757}}, \"EndTime\": 1538408392.15968, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408381.307271}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:52 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.828012463 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:52 INFO 139663579178816] #progress_metric: host=algo-2, completed 24 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:53 INFO 140624803325760] Epoch[62] Batch[0] avg_epoch_loss=2.168632\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:53 INFO 140252856112960] Epoch[60] Batch[5] avg_epoch_loss=2.272571\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:53 INFO 140252856112960] Epoch[60] Batch [5]#011Speed: 123.36 samples/sec#011loss=2.272571\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:57 INFO 140252856112960] Epoch[60] Batch[10] avg_epoch_loss=1.822895\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:57 INFO 140252856112960] Epoch[60] Batch [10]#011Speed: 149.95 samples/sec#011loss=1.283285\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:57 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11130.886793136597, \"sum\": 11130.886793136597, \"min\": 11130.886793136597}}, \"EndTime\": 1538408397.755914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408386.62479}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.880802985 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:39:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:39:53 INFO 139663579178816] Epoch[62] Batch[0] avg_epoch_loss=2.310906\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:55 INFO 140196492334912] Epoch[60] Batch[5] avg_epoch_loss=2.072087\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:55 INFO 140196492334912] Epoch[60] Batch [5]#011Speed: 119.78 samples/sec#011loss=2.072087\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:57 INFO 140624803325760] Epoch[62] Batch[5] avg_epoch_loss=2.142421\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:39:57 INFO 140624803325760] Epoch[62] Batch [5]#011Speed: 158.24 samples/sec#011loss=2.142421\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:59 INFO 139663579178816] Epoch[62] Batch[5] avg_epoch_loss=2.148562\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:39:59 INFO 139663579178816] Epoch[62] Batch [5]#011Speed: 121.32 samples/sec#011loss=2.148562\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:59 INFO 140196492334912] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10839.322090148926, \"sum\": 10839.322090148926, \"min\": 10839.322090148926}}, \"EndTime\": 1538408399.202291, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408388.362728}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.197604159 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:39:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:00 INFO 140252856112960] Epoch[61] Batch[0] avg_epoch_loss=2.288993\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:00 INFO 140196492334912] Epoch[61] Batch[0] avg_epoch_loss=2.685637\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:02 INFO 140624803325760] Epoch[62] Batch[10] avg_epoch_loss=1.904234\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:02 INFO 140624803325760] Epoch[62] Batch [10]#011Speed: 122.32 samples/sec#011loss=1.618409\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:02 INFO 140624803325760] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11712.162971496582, \"sum\": 11712.162971496582, \"min\": 11712.162971496582}}, \"EndTime\": 1538408402.833006, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408391.120613}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:02 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.226322002 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:02 INFO 140624803325760] #progress_metric: host=algo-3, completed 25 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:03 INFO 139663579178816] Epoch[62] Batch[10] avg_epoch_loss=2.284305\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:03 INFO 139663579178816] Epoch[62] Batch [10]#011Speed: 147.34 samples/sec#011loss=2.447197\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:03 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11235.340118408203, \"sum\": 11235.340118408203, \"min\": 11235.340118408203}}, \"EndTime\": 1538408403.395382, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408392.159772}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:03 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.102937356 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:03 INFO 139663579178816] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:04 INFO 140624803325760] Epoch[63] Batch[0] avg_epoch_loss=1.869798\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:04 INFO 140252856112960] Epoch[61] Batch[5] avg_epoch_loss=2.207590\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:04 INFO 140252856112960] Epoch[61] Batch [5]#011Speed: 151.57 samples/sec#011loss=2.207590\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:05 INFO 139663579178816] Epoch[63] Batch[0] avg_epoch_loss=2.059145\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:06 INFO 140196492334912] Epoch[61] Batch[5] avg_epoch_loss=2.249189\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:06 INFO 140196492334912] Epoch[61] Batch [5]#011Speed: 119.83 samples/sec#011loss=2.249189\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:08 INFO 140252856112960] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10465.309858322144, \"sum\": 10465.309858322144, \"min\": 10465.309858322144}}, \"EndTime\": 1538408408.221529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408397.755989}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:08 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.632050774 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:08 INFO 140252856112960] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:09 INFO 140624803325760] Epoch[63] Batch[5] avg_epoch_loss=2.048699\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:09 INFO 140624803325760] Epoch[63] Batch [5]#011Speed: 121.19 samples/sec#011loss=2.048699\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:09 INFO 139663579178816] Epoch[63] Batch[5] avg_epoch_loss=2.140542\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:09 INFO 139663579178816] Epoch[63] Batch [5]#011Speed: 152.81 samples/sec#011loss=2.140542\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:10 INFO 140196492334912] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10831.670999526978, \"sum\": 10831.670999526978, \"min\": 10831.670999526978}}, \"EndTime\": 1538408410.034308, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408399.202379}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.139515303 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:10 INFO 140252856112960] Epoch[62] Batch[0] avg_epoch_loss=2.327692\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:11 INFO 140196492334912] Epoch[62] Batch[0] avg_epoch_loss=2.255769\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:13 INFO 140624803325760] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10667.914152145386, \"sum\": 10667.914152145386, \"min\": 10667.914152145386}}, \"EndTime\": 1538408413.501221, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408402.833072}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:13 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.641279451 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:13 INFO 140624803325760] #progress_metric: host=algo-3, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:14 INFO 140252856112960] Epoch[62] Batch[5] avg_epoch_loss=2.062174\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:14 INFO 140252856112960] Epoch[62] Batch [5]#011Speed: 153.67 samples/sec#011loss=2.062174\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:15 INFO 139663579178816] Epoch[63] Batch[10] avg_epoch_loss=2.180413\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:15 INFO 139663579178816] Epoch[63] Batch [10]#011Speed: 120.47 samples/sec#011loss=2.228257\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:15 INFO 139663579178816] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11905.72190284729, \"sum\": 11905.72190284729, \"min\": 11905.72190284729}}, \"EndTime\": 1538408415.301442, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408403.395468}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:15 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.365884439 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:15 INFO 139663579178816] #progress_metric: host=algo-2, completed 25 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:16 INFO 139663579178816] Epoch[64] Batch[0] avg_epoch_loss=2.424821\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:15 INFO 140624803325760] Epoch[64] Batch[0] avg_epoch_loss=1.991442\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:17 INFO 140196492334912] Epoch[62] Batch[5] avg_epoch_loss=2.106372\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:17 INFO 140196492334912] Epoch[62] Batch [5]#011Speed: 118.24 samples/sec#011loss=2.106372\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:20 INFO 140624803325760] Epoch[64] Batch[5] avg_epoch_loss=2.050893\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:20 INFO 140624803325760] Epoch[64] Batch [5]#011Speed: 122.19 samples/sec#011loss=2.050893\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:20 INFO 140252856112960] Epoch[62] Batch[10] avg_epoch_loss=2.175698\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:20 INFO 140252856112960] Epoch[62] Batch [10]#011Speed: 119.03 samples/sec#011loss=2.311927\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:20 INFO 140252856112960] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11904.499053955078, \"sum\": 11904.499053955078, \"min\": 11904.499053955078}}, \"EndTime\": 1538408420.126335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408408.221606}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.4614567 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:21 INFO 140196492334912] Epoch[62] Batch[10] avg_epoch_loss=2.171612\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:21 INFO 140196492334912] Epoch[62] Batch [10]#011Speed: 144.08 samples/sec#011loss=2.249900\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:21 INFO 140196492334912] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11498.533010482788, \"sum\": 11498.533010482788, \"min\": 11498.533010482788}}, \"EndTime\": 1538408421.53319, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408410.034394}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:21 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.57850034 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:21 INFO 140196492334912] #progress_metric: host=algo-4, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:21 INFO 140252856112960] Epoch[63] Batch[0] avg_epoch_loss=1.926951\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:22 INFO 139663579178816] Epoch[64] Batch[5] avg_epoch_loss=2.103920\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:22 INFO 139663579178816] Epoch[64] Batch [5]#011Speed: 122.51 samples/sec#011loss=2.103920\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:23 INFO 140196492334912] Epoch[63] Batch[0] avg_epoch_loss=2.300575\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:24 INFO 140624803325760] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10840.546131134033, \"sum\": 10840.546131134033, \"min\": 10840.546131134033}}, \"EndTime\": 1538408424.342089, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408413.501296}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:24 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.151591822 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:24 INFO 140624803325760] #progress_metric: host=algo-3, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:28 INFO 140196492334912] Epoch[63] Batch[5] avg_epoch_loss=2.140946\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:28 INFO 140196492334912] Epoch[63] Batch [5]#011Speed: 150.60 samples/sec#011loss=2.140946\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:40:26 INFO 140624803325760] Epoch[65] Batch[0] avg_epoch_loss=1.965956\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:26 INFO 139663579178816] Epoch[64] Batch[10] avg_epoch_loss=1.986401\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:26 INFO 139663579178816] Epoch[64] Batch [10]#011Speed: 148.13 samples/sec#011loss=1.845378\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:26 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11220.249891281128, \"sum\": 11220.249891281128, \"min\": 11220.249891281128}}, \"EndTime\": 1538408426.522016, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408415.301527}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:26 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.573851367 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:26 INFO 139663579178816] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:27 INFO 140252856112960] Epoch[63] Batch[5] avg_epoch_loss=2.069936\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:27 INFO 140252856112960] Epoch[63] Batch [5]#011Speed: 121.02 samples/sec#011loss=2.069936\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:28 INFO 139663579178816] Epoch[65] Batch[0] avg_epoch_loss=2.159069\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:30 INFO 140252856112960] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10782.835006713867, \"sum\": 10782.835006713867, \"min\": 10782.835006713867}}, \"EndTime\": 1538408430.909481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408420.126405}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.552835683 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:31 INFO 140624803325760] Epoch[65] Batch[5] avg_epoch_loss=2.159866\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:31 INFO 140624803325760] Epoch[65] Batch [5]#011Speed: 120.29 samples/sec#011loss=2.159866\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:32 INFO 140196492334912] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10524.005889892578, \"sum\": 10524.005889892578, \"min\": 10524.005889892578}}, \"EndTime\": 1538408432.057582, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408421.53327}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:32 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.24504899 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:32 INFO 140196492334912] #progress_metric: host=algo-4, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:32 INFO 140252856112960] Epoch[64] Batch[0] avg_epoch_loss=2.357034\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:33 INFO 139663579178816] Epoch[65] Batch[5] avg_epoch_loss=2.130773\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:33 INFO 139663579178816] Epoch[65] Batch [5]#011Speed: 151.98 samples/sec#011loss=2.130773\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:34 INFO 140196492334912] Epoch[64] Batch[0] avg_epoch_loss=1.791669\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:35 INFO 140624803325760] Epoch[65] Batch[10] avg_epoch_loss=2.433073\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:35 INFO 140624803325760] Epoch[65] Batch [10]#011Speed: 147.37 samples/sec#011loss=2.760921\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:35 INFO 140624803325760] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11337.072134017944, \"sum\": 11337.072134017944, \"min\": 11337.072134017944}}, \"EndTime\": 1538408435.679467, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408424.342162}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:35 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.66702193 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:35 INFO 140624803325760] #progress_metric: host=algo-3, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:37 INFO 140252856112960] Epoch[64] Batch[5] avg_epoch_loss=2.199410\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:37 INFO 140252856112960] Epoch[64] Batch [5]#011Speed: 122.17 samples/sec#011loss=2.199410\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:38 INFO 139663579178816] Epoch[65] Batch[10] avg_epoch_loss=2.154221\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:38 INFO 139663579178816] Epoch[65] Batch [10]#011Speed: 121.50 samples/sec#011loss=2.182359\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:38 INFO 139663579178816] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11849.641799926758, \"sum\": 11849.641799926758, \"min\": 11849.641799926758}}, \"EndTime\": 1538408438.371958, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408426.522084}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:38 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.563458382 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:38 INFO 139663579178816] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:38 INFO 140196492334912] Epoch[64] Batch[5] avg_epoch_loss=2.239294\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:38 INFO 140196492334912] Epoch[64] Batch [5]#011Speed: 152.57 samples/sec#011loss=2.239294\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:38 INFO 140624803325760] Epoch[66] Batch[0] avg_epoch_loss=2.011517\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:40 INFO 139663579178816] Epoch[66] Batch[0] avg_epoch_loss=2.405293\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:42 INFO 140624803325760] Epoch[66] Batch[5] avg_epoch_loss=2.320768\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:42 INFO 140624803325760] Epoch[66] Batch [5]#011Speed: 157.64 samples/sec#011loss=2.320768\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:41 INFO 140252856112960] Epoch[64] Batch[10] avg_epoch_loss=1.969524\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:41 INFO 140252856112960] Epoch[64] Batch [10]#011Speed: 151.69 samples/sec#011loss=1.693662\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:41 INFO 140252856112960] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11080.603122711182, \"sum\": 11080.603122711182, \"min\": 11080.603122711182}}, \"EndTime\": 1538408441.990392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408430.909556}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:41 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.674706567 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:41 INFO 140252856112960] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:44 INFO 140196492334912] Epoch[64] Batch[10] avg_epoch_loss=2.195062\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:44 INFO 140196492334912] Epoch[64] Batch [10]#011Speed: 119.72 samples/sec#011loss=2.141982\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:44 INFO 140196492334912] processed a total of 1340 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11967.742919921875, \"sum\": 11967.742919921875, \"min\": 11967.742919921875}}, \"EndTime\": 1538408444.025664, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408432.057671}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:44 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.966389966 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:44 INFO 140196492334912] #progress_metric: host=algo-4, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:44 INFO 140252856112960] Epoch[65] Batch[0] avg_epoch_loss=2.180905\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:45 INFO 139663579178816] Epoch[66] Batch[5] avg_epoch_loss=2.417221\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:45 INFO 139663579178816] Epoch[66] Batch [5]#011Speed: 120.90 samples/sec#011loss=2.417221\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:45 INFO 140196492334912] Epoch[65] Batch[0] avg_epoch_loss=2.283978\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:46 INFO 140624803325760] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10326.474905014038, \"sum\": 10326.474905014038, \"min\": 10326.474905014038}}, \"EndTime\": 1538408446.006233, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408435.679536}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.886681214 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 26 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:48 INFO 140624803325760] Epoch[67] Batch[0] avg_epoch_loss=2.327637\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:49 INFO 139663579178816] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10767.454147338867, \"sum\": 10767.454147338867, \"min\": 10767.454147338867}}, \"EndTime\": 1538408449.139721, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408438.372039}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.296486032 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 26 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:40:48 INFO 140252856112960] Epoch[65] Batch[5] avg_epoch_loss=2.161195\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:48 INFO 140252856112960] Epoch[65] Batch [5]#011Speed: 154.59 samples/sec#011loss=2.161195\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:50 INFO 139663579178816] Epoch[67] Batch[0] avg_epoch_loss=1.711550\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:50 INFO 140196492334912] Epoch[65] Batch[5] avg_epoch_loss=2.177484\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:51 INFO 140196492334912] Epoch[65] Batch [5]#011Speed: 119.95 samples/sec#011loss=2.177484\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:52 INFO 140624803325760] Epoch[67] Batch[5] avg_epoch_loss=2.199855\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:52 INFO 140624803325760] Epoch[67] Batch [5]#011Speed: 156.88 samples/sec#011loss=2.199855\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:53 INFO 140252856112960] Epoch[65] Batch[10] avg_epoch_loss=2.195971\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:53 INFO 140252856112960] Epoch[65] Batch [10]#011Speed: 120.59 samples/sec#011loss=2.237703\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:53 INFO 140252856112960] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11793.86305809021, \"sum\": 11793.86305809021, \"min\": 11793.86305809021}}, \"EndTime\": 1538408453.784623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408441.990464}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:53 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.717112067 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:53 INFO 140252856112960] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:54 INFO 140196492334912] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10827.886819839478, \"sum\": 10827.886819839478, \"min\": 10827.886819839478}}, \"EndTime\": 1538408454.853908, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408444.025756}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:54 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.842445244 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:54 INFO 140196492334912] #progress_metric: host=algo-4, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:40:55 INFO 140252856112960] Epoch[66] Batch[0] avg_epoch_loss=2.102213\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:55 INFO 139663579178816] Epoch[67] Batch[5] avg_epoch_loss=1.974803\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:55 INFO 139663579178816] Epoch[67] Batch [5]#011Speed: 123.00 samples/sec#011loss=1.974803\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:40:56 INFO 140196492334912] Epoch[66] Batch[0] avg_epoch_loss=1.891371\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:56 INFO 140624803325760] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10285.937070846558, \"sum\": 10285.937070846558, \"min\": 10285.937070846558}}, \"EndTime\": 1538408456.292476, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408446.006309}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:56 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.871100486 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:56 INFO 140624803325760] #progress_metric: host=algo-3, completed 27 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:59 INFO 139663579178816] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10604.280948638916, \"sum\": 10604.280948638916, \"min\": 10604.280948638916}}, \"EndTime\": 1538408459.744354, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408449.139812}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:59 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.327128119 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:40:59 INFO 139663579178816] #progress_metric: host=algo-2, completed 27 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:40:58 INFO 140624803325760] Epoch[68] Batch[0] avg_epoch_loss=2.091433\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:00 INFO 140252856112960] Epoch[66] Batch[5] avg_epoch_loss=2.044875\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:00 INFO 140252856112960] Epoch[66] Batch [5]#011Speed: 123.48 samples/sec#011loss=2.044875\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:01 INFO 139663579178816] Epoch[68] Batch[0] avg_epoch_loss=2.149718\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:01 INFO 140196492334912] Epoch[66] Batch[5] avg_epoch_loss=2.060137\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:01 INFO 140196492334912] Epoch[66] Batch [5]#011Speed: 120.45 samples/sec#011loss=2.060137\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:02 INFO 140624803325760] Epoch[68] Batch[5] avg_epoch_loss=2.128574\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:02 INFO 140624803325760] Epoch[68] Batch [5]#011Speed: 159.43 samples/sec#011loss=2.128574\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:04 INFO 140252856112960] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10522.273778915405, \"sum\": 10522.273778915405, \"min\": 10522.273778915405}}, \"EndTime\": 1538408464.307203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408453.784694}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:04 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.649714057 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:04 INFO 140252856112960] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:05 INFO 140252856112960] Epoch[67] Batch[0] avg_epoch_loss=2.462371\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:06 INFO 140196492334912] Epoch[66] Batch[10] avg_epoch_loss=1.938018\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:06 INFO 140196492334912] Epoch[66] Batch [10]#011Speed: 148.73 samples/sec#011loss=1.791475\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:06 INFO 140196492334912] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11253.204822540283, \"sum\": 11253.204822540283, \"min\": 11253.204822540283}}, \"EndTime\": 1538408466.107459, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408454.853996}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.364984114 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 26 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:06 INFO 139663579178816] Epoch[68] Batch[5] avg_epoch_loss=2.088148\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:06 INFO 139663579178816] Epoch[68] Batch [5]#011Speed: 120.07 samples/sec#011loss=2.088148\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:10 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10815.834045410156, \"sum\": 10815.834045410156, \"min\": 10815.834045410156}}, \"EndTime\": 1538408470.560549, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408459.744455}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.06615649 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 27 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:07 INFO 140624803325760] Epoch[68] Batch[10] avg_epoch_loss=2.392242\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:07 INFO 140624803325760] Epoch[68] Batch [10]#011Speed: 122.05 samples/sec#011loss=2.708643\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:07 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11559.377908706665, \"sum\": 11559.377908706665, \"min\": 11559.377908706665}}, \"EndTime\": 1538408467.852158, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408456.292552}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:07 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.364798939 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:07 INFO 140624803325760] #progress_metric: host=algo-3, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:08 INFO 140196492334912] Epoch[67] Batch[0] avg_epoch_loss=2.049471\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:09 INFO 140624803325760] Epoch[69] Batch[0] avg_epoch_loss=2.633589\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:11 INFO 140252856112960] Epoch[67] Batch[5] avg_epoch_loss=2.256843\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:11 INFO 140252856112960] Epoch[67] Batch [5]#011Speed: 123.29 samples/sec#011loss=2.256843\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:12 INFO 139663579178816] Epoch[69] Batch[0] avg_epoch_loss=2.033961\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:12 INFO 140196492334912] Epoch[67] Batch[5] avg_epoch_loss=2.208265\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:12 INFO 140196492334912] Epoch[67] Batch [5]#011Speed: 152.80 samples/sec#011loss=2.208265\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:14 INFO 140624803325760] Epoch[69] Batch[5] avg_epoch_loss=2.133264\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:14 INFO 140624803325760] Epoch[69] Batch [5]#011Speed: 126.74 samples/sec#011loss=2.133264\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:14 INFO 140252856112960] processed a total of 1209 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10555.256128311157, \"sum\": 10555.256128311157, \"min\": 10555.256128311157}}, \"EndTime\": 1538408474.862779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408464.307277}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.538898557 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:16 INFO 140252856112960] Epoch[68] Batch[0] avg_epoch_loss=2.142361\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:18 INFO 140624803325760] Epoch[69] Batch[10] avg_epoch_loss=2.319798\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:18 INFO 140624803325760] Epoch[69] Batch [10]#011Speed: 152.10 samples/sec#011loss=2.543638\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:18 INFO 140624803325760] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10876.29222869873, \"sum\": 10876.29222869873, \"min\": 10876.29222869873}}, \"EndTime\": 1538408478.728827, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408467.852248}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.168459227 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 28 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:41:17 INFO 139663579178816] Epoch[69] Batch[5] avg_epoch_loss=2.025139\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:17 INFO 139663579178816] Epoch[69] Batch [5]#011Speed: 120.77 samples/sec#011loss=2.025139\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:18 INFO 140196492334912] Epoch[67] Batch[10] avg_epoch_loss=1.894688\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:18 INFO 140196492334912] Epoch[67] Batch [10]#011Speed: 117.53 samples/sec#011loss=1.518396\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:18 INFO 140196492334912] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12008.145093917847, \"sum\": 12008.145093917847, \"min\": 12008.145093917847}}, \"EndTime\": 1538408478.115937, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408466.107542}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:18 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.008753296 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:18 INFO 140196492334912] #progress_metric: host=algo-4, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:19 INFO 140196492334912] Epoch[68] Batch[0] avg_epoch_loss=2.396520\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:21 INFO 139663579178816] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10728.646039962769, \"sum\": 10728.646039962769, \"min\": 10728.646039962769}}, \"EndTime\": 1538408481.289552, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408470.560642}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:21 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.22957063 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:21 INFO 139663579178816] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:21 INFO 140624803325760] Epoch[70] Batch[0] avg_epoch_loss=2.066166\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:21 INFO 140252856112960] Epoch[68] Batch[5] avg_epoch_loss=2.067586\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:21 INFO 140252856112960] Epoch[68] Batch [5]#011Speed: 124.15 samples/sec#011loss=2.067586\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:22 INFO 139663579178816] Epoch[70] Batch[0] avg_epoch_loss=1.980624\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:25 INFO 140196492334912] Epoch[68] Batch[5] avg_epoch_loss=2.193742\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:25 INFO 140196492334912] Epoch[68] Batch [5]#011Speed: 119.09 samples/sec#011loss=2.193742\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:25 INFO 140624803325760] Epoch[70] Batch[5] avg_epoch_loss=2.238916\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:25 INFO 140624803325760] Epoch[70] Batch [5]#011Speed: 155.18 samples/sec#011loss=2.238916\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:25 INFO 140252856112960] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10765.458106994629, \"sum\": 10765.458106994629, \"min\": 10765.458106994629}}, \"EndTime\": 1538408485.628557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408474.862852}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.717568003 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:27 INFO 140252856112960] Epoch[69] Batch[0] avg_epoch_loss=1.993814\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:29 INFO 140196492334912] Epoch[68] Batch[10] avg_epoch_loss=1.935563\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:29 INFO 140196492334912] Epoch[68] Batch [10]#011Speed: 145.78 samples/sec#011loss=1.625747\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:29 INFO 140196492334912] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11387.351989746094, \"sum\": 11387.351989746094, \"min\": 11387.351989746094}}, \"EndTime\": 1538408489.503638, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408478.116035}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.370152927 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 27 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:28 INFO 139663579178816] Epoch[70] Batch[5] avg_epoch_loss=2.159785\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:28 INFO 139663579178816] Epoch[70] Batch [5]#011Speed: 121.38 samples/sec#011loss=2.159785\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:30 INFO 140624803325760] Epoch[70] Batch[10] avg_epoch_loss=2.303981\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:30 INFO 140624803325760] Epoch[70] Batch [10]#011Speed: 120.81 samples/sec#011loss=2.382059\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:30 INFO 140624803325760] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11806.329011917114, \"sum\": 11806.329011917114, \"min\": 11806.329011917114}}, \"EndTime\": 1538408490.535465, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408478.728899}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:30 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.177563055 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:30 INFO 140624803325760] #progress_metric: host=algo-3, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:32 INFO 139663579178816] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10734.010219573975, \"sum\": 10734.010219573975, \"min\": 10734.010219573975}}, \"EndTime\": 1538408492.023881, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408481.289634}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:32 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.382470281 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:32 INFO 139663579178816] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:31 INFO 140196492334912] Epoch[69] Batch[0] avg_epoch_loss=2.091995\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:32 INFO 140624803325760] Epoch[71] Batch[0] avg_epoch_loss=1.833441\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:32 INFO 140252856112960] Epoch[69] Batch[5] avg_epoch_loss=1.996249\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:32 INFO 140252856112960] Epoch[69] Batch [5]#011Speed: 118.39 samples/sec#011loss=1.996249\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:33 INFO 139663579178816] Epoch[71] Batch[0] avg_epoch_loss=2.053298\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:36 INFO 140196492334912] Epoch[69] Batch[5] avg_epoch_loss=2.026101\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:36 INFO 140196492334912] Epoch[69] Batch [5]#011Speed: 151.37 samples/sec#011loss=2.026101\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:37 INFO 140624803325760] Epoch[71] Batch[5] avg_epoch_loss=2.116652\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:37 INFO 140624803325760] Epoch[71] Batch [5]#011Speed: 121.43 samples/sec#011loss=2.116652\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:37 INFO 140252856112960] Epoch[69] Batch[10] avg_epoch_loss=1.764709\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:37 INFO 140252856112960] Epoch[69] Batch [10]#011Speed: 144.03 samples/sec#011loss=1.486861\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:37 INFO 140252856112960] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11524.533987045288, \"sum\": 11524.533987045288, \"min\": 11524.533987045288}}, \"EndTime\": 1538408497.153406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408485.628633}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:37 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.020842632 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:37 INFO 140252856112960] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:39 INFO 140252856112960] Epoch[70] Batch[0] avg_epoch_loss=2.379691\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:41 INFO 140624803325760] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10668.101072311401, \"sum\": 10668.101072311401, \"min\": 10668.101072311401}}, \"EndTime\": 1538408501.203895, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408490.535551}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:41 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.888477228 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:41 INFO 140624803325760] #progress_metric: host=algo-3, completed 28 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:41:38 INFO 139663579178816] Epoch[71] Batch[5] avg_epoch_loss=2.153151\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:38 INFO 139663579178816] Epoch[71] Batch [5]#011Speed: 121.68 samples/sec#011loss=2.153151\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:39 INFO 140196492334912] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10427.824020385742, \"sum\": 10427.824020385742, \"min\": 10427.824020385742}}, \"EndTime\": 1538408499.931794, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408489.50372}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:39 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.472698177 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:39 INFO 140196492334912] #progress_metric: host=algo-4, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:42 INFO 140196492334912] Epoch[70] Batch[0] avg_epoch_loss=2.255774\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:42 INFO 140624803325760] Epoch[72] Batch[0] avg_epoch_loss=2.374679\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:42 INFO 139663579178816] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10711.359024047852, \"sum\": 10711.359024047852, \"min\": 10711.359024047852}}, \"EndTime\": 1538408502.735575, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408492.02397}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:42 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.724138858 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:42 INFO 139663579178816] #progress_metric: host=algo-2, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:43 INFO 140252856112960] Epoch[70] Batch[5] avg_epoch_loss=2.219058\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:43 INFO 140252856112960] Epoch[70] Batch [5]#011Speed: 151.66 samples/sec#011loss=2.219058\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:44 INFO 139663579178816] Epoch[72] Batch[0] avg_epoch_loss=1.814426\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:46 INFO 140196492334912] Epoch[70] Batch[5] avg_epoch_loss=2.176964\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:46 INFO 140196492334912] Epoch[70] Batch [5]#011Speed: 151.10 samples/sec#011loss=2.176964\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:50 INFO 140196492334912] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10458.064079284668, \"sum\": 10458.064079284668, \"min\": 10458.064079284668}}, \"EndTime\": 1538408510.390195, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408499.93188}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:50 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.435869527 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:50 INFO 140196492334912] #progress_metric: host=algo-4, completed 28 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:48 INFO 140624803325760] Epoch[72] Batch[5] avg_epoch_loss=2.178313\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:48 INFO 140624803325760] Epoch[72] Batch [5]#011Speed: 124.10 samples/sec#011loss=2.178313\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:49 INFO 140252856112960] Epoch[70] Batch[10] avg_epoch_loss=2.281725\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:49 INFO 140252856112960] Epoch[70] Batch [10]#011Speed: 118.66 samples/sec#011loss=2.356926\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:49 INFO 140252856112960] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12094.093084335327, \"sum\": 12094.093084335327, \"min\": 12094.093084335327}}, \"EndTime\": 1538408509.24779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408497.153475}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:49 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.151032523 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:49 INFO 140252856112960] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:49 INFO 139663579178816] Epoch[72] Batch[5] avg_epoch_loss=2.229605\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:49 INFO 139663579178816] Epoch[72] Batch [5]#011Speed: 120.69 samples/sec#011loss=2.229605\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:50 INFO 140252856112960] Epoch[71] Batch[0] avg_epoch_loss=2.176252\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:52 INFO 140196492334912] Epoch[71] Batch[0] avg_epoch_loss=1.874328\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:52 INFO 140624803325760] Epoch[72] Batch[10] avg_epoch_loss=2.239693\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:52 INFO 140624803325760] Epoch[72] Batch [10]#011Speed: 153.65 samples/sec#011loss=2.313349\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:52 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10995.712995529175, \"sum\": 10995.712995529175, \"min\": 10995.712995529175}}, \"EndTime\": 1538408512.199967, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408501.204}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:52 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.408436031 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:52 INFO 140624803325760] #progress_metric: host=algo-3, completed 29 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:53 INFO 139663579178816] Epoch[72] Batch[10] avg_epoch_loss=2.283425\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:53 INFO 139663579178816] Epoch[72] Batch [10]#011Speed: 147.52 samples/sec#011loss=2.348008\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:53 INFO 139663579178816] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11253.674983978271, \"sum\": 11253.674983978271, \"min\": 11253.674983978271}}, \"EndTime\": 1538408513.98956, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408502.73566}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:53 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.938257209 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:53 INFO 139663579178816] #progress_metric: host=algo-2, completed 29 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:54 INFO 140624803325760] Epoch[73] Batch[0] avg_epoch_loss=2.068323\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:41:56 INFO 139663579178816] Epoch[73] Batch[0] avg_epoch_loss=2.371573\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:56 INFO 140252856112960] Epoch[71] Batch[5] avg_epoch_loss=2.230345\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:41:56 INFO 140252856112960] Epoch[71] Batch [5]#011Speed: 116.59 samples/sec#011loss=2.230345\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:56 INFO 140196492334912] Epoch[71] Batch[5] avg_epoch_loss=1.896572\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:41:56 INFO 140196492334912] Epoch[71] Batch [5]#011Speed: 151.46 samples/sec#011loss=1.896572\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:58 INFO 140624803325760] Epoch[73] Batch[5] avg_epoch_loss=2.031937\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:41:58 INFO 140624803325760] Epoch[73] Batch [5]#011Speed: 152.64 samples/sec#011loss=2.031937\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:00 INFO 140252856112960] Epoch[71] Batch[10] avg_epoch_loss=1.938333\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:00 INFO 140252856112960] Epoch[71] Batch [10]#011Speed: 144.91 samples/sec#011loss=1.587919\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:00 INFO 140252856112960] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11527.436017990112, \"sum\": 11527.436017990112, \"min\": 11527.436017990112}}, \"EndTime\": 1538408520.775538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408509.247858}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:00 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.161173982 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:00 INFO 140252856112960] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:00 INFO 140196492334912] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10492.894887924194, \"sum\": 10492.894887924194, \"min\": 10492.894887924194}}, \"EndTime\": 1538408520.88343, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408510.390283}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:00 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.173730441 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:00 INFO 140196492334912] #progress_metric: host=algo-4, completed 28 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:00 INFO 139663579178816] Epoch[73] Batch[5] avg_epoch_loss=2.165264\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:00 INFO 139663579178816] Epoch[73] Batch [5]#011Speed: 153.07 samples/sec#011loss=2.165264\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:02 INFO 140624803325760] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10517.56501197815, \"sum\": 10517.56501197815, \"min\": 10517.56501197815}}, \"EndTime\": 1538408522.717868, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408512.20005}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:02 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.08665441 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:02 INFO 140624803325760] #progress_metric: host=algo-3, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:03 INFO 140196492334912] Epoch[72] Batch[0] avg_epoch_loss=2.222579\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:03 INFO 140252856112960] Epoch[72] Batch[0] avg_epoch_loss=2.119967\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:05 INFO 140624803325760] Epoch[74] Batch[0] avg_epoch_loss=2.094008\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:05 INFO 139663579178816] Epoch[73] Batch[10] avg_epoch_loss=2.238243\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:05 INFO 139663579178816] Epoch[73] Batch [10]#011Speed: 119.72 samples/sec#011loss=2.325819\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:05 INFO 139663579178816] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11917.675971984863, \"sum\": 11917.675971984863, \"min\": 11917.675971984863}}, \"EndTime\": 1538408525.907591, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408513.989646}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.500065027 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:07 INFO 140196492334912] Epoch[72] Batch[5] avg_epoch_loss=2.213387\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:07 INFO 140196492334912] Epoch[72] Batch [5]#011Speed: 147.58 samples/sec#011loss=2.213387\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:07 INFO 140252856112960] Epoch[72] Batch[5] avg_epoch_loss=2.031915\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:07 INFO 140252856112960] Epoch[72] Batch [5]#011Speed: 153.11 samples/sec#011loss=2.031915\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:07 INFO 139663579178816] Epoch[74] Batch[0] avg_epoch_loss=1.829348\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:42:09 INFO 140624803325760] Epoch[74] Batch[5] avg_epoch_loss=2.113564\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:09 INFO 140624803325760] Epoch[74] Batch [5]#011Speed: 144.63 samples/sec#011loss=2.113564\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:11 INFO 140196492334912] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10680.16004562378, \"sum\": 10680.16004562378, \"min\": 10680.16004562378}}, \"EndTime\": 1538408531.563927, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408520.883518}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:11 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.379004949 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:11 INFO 140196492334912] #progress_metric: host=algo-4, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:11 INFO 140252856112960] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10406.605005264282, \"sum\": 10406.605005264282, \"min\": 10406.605005264282}}, \"EndTime\": 1538408531.182477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408520.775624}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:11 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.616119016 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:11 INFO 140252856112960] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:12 INFO 139663579178816] Epoch[74] Batch[5] avg_epoch_loss=2.179166\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:12 INFO 139663579178816] Epoch[74] Batch [5]#011Speed: 118.96 samples/sec#011loss=2.179166\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:13 INFO 140252856112960] Epoch[73] Batch[0] avg_epoch_loss=2.174574\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:13 INFO 140196492334912] Epoch[73] Batch[0] avg_epoch_loss=2.618166\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:15 INFO 140624803325760] Epoch[74] Batch[10] avg_epoch_loss=2.041042\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:15 INFO 140624803325760] Epoch[74] Batch [10]#011Speed: 116.90 samples/sec#011loss=1.954015\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:15 INFO 140624803325760] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12302.292108535767, \"sum\": 12302.292108535767, \"min\": 12302.292108535767}}, \"EndTime\": 1538408535.020512, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408522.717957}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:15 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=104.613554541 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:15 INFO 140624803325760] #progress_metric: host=algo-3, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:16 INFO 140624803325760] Epoch[75] Batch[0] avg_epoch_loss=1.682435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:17 INFO 139663579178816] Epoch[74] Batch[10] avg_epoch_loss=2.100015\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:17 INFO 139663579178816] Epoch[74] Batch [10]#011Speed: 144.41 samples/sec#011loss=2.005033\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:17 INFO 139663579178816] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11457.080125808716, \"sum\": 11457.080125808716, \"min\": 11457.080125808716}}, \"EndTime\": 1538408537.364999, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408525.907674}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.640271352 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:17 INFO 140252856112960] Epoch[73] Batch[5] avg_epoch_loss=2.173757\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:17 INFO 140252856112960] Epoch[73] Batch [5]#011Speed: 151.04 samples/sec#011loss=2.173757\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:18 INFO 140196492334912] Epoch[73] Batch[5] avg_epoch_loss=2.196163\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:18 INFO 140196492334912] Epoch[73] Batch [5]#011Speed: 151.26 samples/sec#011loss=2.196163\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:19 INFO 139663579178816] Epoch[75] Batch[0] avg_epoch_loss=1.727297\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:24 INFO 139663579178816] Epoch[75] Batch[5] avg_epoch_loss=2.175307\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:24 INFO 139663579178816] Epoch[75] Batch [5]#011Speed: 150.32 samples/sec#011loss=2.175307\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:21 INFO 140624803325760] Epoch[75] Batch[5] avg_epoch_loss=2.038944\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:21 INFO 140624803325760] Epoch[75] Batch [5]#011Speed: 122.86 samples/sec#011loss=2.038944\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:23 INFO 140196492334912] Epoch[73] Batch[10] avg_epoch_loss=2.377279\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:23 INFO 140196492334912] Epoch[73] Batch [10]#011Speed: 116.73 samples/sec#011loss=2.594619\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:23 INFO 140196492334912] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12039.974927902222, \"sum\": 12039.974927902222, \"min\": 12039.974927902222}}, \"EndTime\": 1538408543.604212, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408531.563997}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:23 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.387650822 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:23 INFO 140196492334912] #progress_metric: host=algo-4, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:23 INFO 140252856112960] Epoch[73] Batch[10] avg_epoch_loss=2.168801\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:23 INFO 140252856112960] Epoch[73] Batch [10]#011Speed: 120.05 samples/sec#011loss=2.162853\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:23 INFO 140252856112960] processed a total of 1347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12031.119108200073, \"sum\": 12031.119108200073, \"min\": 12031.119108200073}}, \"EndTime\": 1538408543.213992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408531.182565}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:23 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.958794323 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:23 INFO 140252856112960] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:24 INFO 140252856112960] Epoch[74] Batch[0] avg_epoch_loss=2.253245\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:25 INFO 140196492334912] Epoch[74] Batch[0] avg_epoch_loss=1.777844\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:26 INFO 140624803325760] Epoch[75] Batch[10] avg_epoch_loss=1.798573\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:26 INFO 140624803325760] Epoch[75] Batch [10]#011Speed: 151.86 samples/sec#011loss=1.510129\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:26 INFO 140624803325760] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11044.677019119263, \"sum\": 11044.677019119263, \"min\": 11044.677019119263}}, \"EndTime\": 1538408546.06553, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408535.020599}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.155098463 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:28 INFO 140624803325760] Epoch[76] Batch[0] avg_epoch_loss=2.649226\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:29 INFO 139663579178816] Epoch[75] Batch[10] avg_epoch_loss=2.171984\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:29 INFO 139663579178816] Epoch[75] Batch [10]#011Speed: 119.89 samples/sec#011loss=2.167995\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:29 INFO 139663579178816] processed a total of 1343 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12013.569116592407, \"sum\": 12013.569116592407, \"min\": 12013.569116592407}}, \"EndTime\": 1538408549.3789, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408537.365084}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.789087356 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:30 INFO 140196492334912] Epoch[74] Batch[5] avg_epoch_loss=2.237023\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:30 INFO 140196492334912] Epoch[74] Batch [5]#011Speed: 118.63 samples/sec#011loss=2.237023\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:31 INFO 139663579178816] Epoch[76] Batch[0] avg_epoch_loss=2.811311\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:34 INFO 140196492334912] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10950.796127319336, \"sum\": 10950.796127319336, \"min\": 10950.796127319336}}, \"EndTime\": 1538408554.555353, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408543.604312}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:34 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.606598325 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:34 INFO 140196492334912] #progress_metric: host=algo-4, completed 30 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:42:30 INFO 140252856112960] Epoch[74] Batch[5] avg_epoch_loss=2.297945\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:30 INFO 140252856112960] Epoch[74] Batch [5]#011Speed: 121.25 samples/sec#011loss=2.297945\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:32 INFO 140624803325760] Epoch[76] Batch[5] avg_epoch_loss=2.533968\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:32 INFO 140624803325760] Epoch[76] Batch [5]#011Speed: 152.71 samples/sec#011loss=2.533968\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:34 INFO 140252856112960] Epoch[74] Batch[10] avg_epoch_loss=2.092243\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:34 INFO 140252856112960] Epoch[74] Batch [10]#011Speed: 150.00 samples/sec#011loss=1.845401\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:34 INFO 140252856112960] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11164.024114608765, \"sum\": 11164.024114608765, \"min\": 11164.024114608765}}, \"EndTime\": 1538408554.378319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408543.214055}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:34 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.041977907 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:34 INFO 140252856112960] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:36 INFO 139663579178816] Epoch[76] Batch[5] avg_epoch_loss=2.497441\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:36 INFO 139663579178816] Epoch[76] Batch [5]#011Speed: 123.88 samples/sec#011loss=2.497441\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:36 INFO 140196492334912] Epoch[75] Batch[0] avg_epoch_loss=2.159260\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:36 INFO 140624803325760] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10414.808988571167, \"sum\": 10414.808988571167, \"min\": 10414.808988571167}}, \"EndTime\": 1538408556.480678, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408546.065614}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:36 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.77168624 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:36 INFO 140624803325760] #progress_metric: host=algo-3, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:36 INFO 140252856112960] Epoch[75] Batch[0] avg_epoch_loss=2.133928\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:38 INFO 140624803325760] Epoch[77] Batch[0] avg_epoch_loss=2.045142\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:40 INFO 140252856112960] Epoch[75] Batch[5] avg_epoch_loss=2.059880\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:40 INFO 140252856112960] Epoch[75] Batch [5]#011Speed: 153.12 samples/sec#011loss=2.059880\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:40 INFO 139663579178816] Epoch[76] Batch[10] avg_epoch_loss=2.164030\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:40 INFO 139663579178816] Epoch[76] Batch [10]#011Speed: 149.35 samples/sec#011loss=1.763936\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:40 INFO 139663579178816] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11120.939016342163, \"sum\": 11120.939016342163, \"min\": 11120.939016342163}}, \"EndTime\": 1538408560.500242, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408549.378985}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:40 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.344844929 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:40 INFO 139663579178816] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:41 INFO 140196492334912] Epoch[75] Batch[5] avg_epoch_loss=1.996387\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:41 INFO 140196492334912] Epoch[75] Batch [5]#011Speed: 117.50 samples/sec#011loss=1.996387\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:42 INFO 139663579178816] Epoch[77] Batch[0] avg_epoch_loss=1.768158\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:42 INFO 140624803325760] Epoch[77] Batch[5] avg_epoch_loss=1.961573\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:42 INFO 140624803325760] Epoch[77] Batch [5]#011Speed: 155.57 samples/sec#011loss=1.961573\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:44 INFO 140252856112960] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10353.826999664307, \"sum\": 10353.826999664307, \"min\": 10353.826999664307}}, \"EndTime\": 1538408564.732445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408554.37839}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=122.755224861 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:46 INFO 140196492334912] Epoch[75] Batch[10] avg_epoch_loss=1.990491\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:46 INFO 140196492334912] Epoch[75] Batch [10]#011Speed: 144.88 samples/sec#011loss=1.983416\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:46 INFO 140196492334912] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11535.699129104614, \"sum\": 11535.699129104614, \"min\": 11535.699129104614}}, \"EndTime\": 1538408566.0914, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408554.555446}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:46 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.946265081 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:46 INFO 140196492334912] #progress_metric: host=algo-4, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:46 INFO 140624803325760] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10170.320987701416, \"sum\": 10170.320987701416, \"min\": 10170.320987701416}}, \"EndTime\": 1538408566.651341, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408556.480765}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=125.854781593 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 31 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:46 INFO 139663579178816] Epoch[77] Batch[5] avg_epoch_loss=1.958775\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:46 INFO 139663579178816] Epoch[77] Batch [5]#011Speed: 156.75 samples/sec#011loss=1.958775\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:47 INFO 140252856112960] Epoch[76] Batch[0] avg_epoch_loss=2.009969\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:48 INFO 140196492334912] Epoch[76] Batch[0] avg_epoch_loss=2.288607\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:49 INFO 140624803325760] Epoch[78] Batch[0] avg_epoch_loss=2.455481\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:50 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10212.966918945312, \"sum\": 10212.966918945312, \"min\": 10212.966918945312}}, \"EndTime\": 1538408570.713545, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408560.500327}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:50 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.035479569 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:50 INFO 139663579178816] #progress_metric: host=algo-2, completed 31 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:53 INFO 140624803325760] Epoch[78] Batch[5] avg_epoch_loss=2.271774\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:53 INFO 140624803325760] Epoch[78] Batch [5]#011Speed: 155.08 samples/sec#011loss=2.271774\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:51 INFO 140252856112960] Epoch[76] Batch[5] avg_epoch_loss=2.237398\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:51 INFO 140252856112960] Epoch[76] Batch [5]#011Speed: 152.58 samples/sec#011loss=2.237398\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:52 INFO 140196492334912] Epoch[76] Batch[5] avg_epoch_loss=2.291614\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:52 INFO 140196492334912] Epoch[76] Batch [5]#011Speed: 149.60 samples/sec#011loss=2.291614\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:53 INFO 139663579178816] Epoch[78] Batch[0] avg_epoch_loss=2.434072\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:56 INFO 140196492334912] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10570.381879806519, \"sum\": 10570.381879806519, \"min\": 10570.381879806519}}, \"EndTime\": 1538408576.662118, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408566.091485}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:56 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.672515863 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:56 INFO 140196492334912] #progress_metric: host=algo-4, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:56 INFO 140252856112960] Epoch[76] Batch[10] avg_epoch_loss=2.006411\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:56 INFO 140252856112960] Epoch[76] Batch [10]#011Speed: 119.27 samples/sec#011loss=1.729225\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:56 INFO 140252856112960] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11919.013977050781, \"sum\": 11919.013977050781, \"min\": 11919.013977050781}}, \"EndTime\": 1538408576.651789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408564.732521}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:56 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.242816879 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:56 INFO 140252856112960] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:57 INFO 139663579178816] Epoch[78] Batch[5] avg_epoch_loss=2.277732\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:42:57 INFO 139663579178816] Epoch[78] Batch [5]#011Speed: 155.75 samples/sec#011loss=2.277732\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:58 INFO 140624803325760] Epoch[78] Batch[10] avg_epoch_loss=2.217825\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:58 INFO 140624803325760] Epoch[78] Batch [10]#011Speed: 122.39 samples/sec#011loss=2.153087\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:58 INFO 140624803325760] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11741.43099784851, \"sum\": 11741.43099784851, \"min\": 11741.43099784851}}, \"EndTime\": 1538408578.393113, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408566.651427}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.358048362 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:42:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:42:58 INFO 140252856112960] Epoch[77] Batch[0] avg_epoch_loss=2.137546\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:42:58 INFO 140196492334912] Epoch[77] Batch[0] avg_epoch_loss=2.292871\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:00 INFO 140624803325760] Epoch[79] Batch[0] avg_epoch_loss=1.946592\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:03 INFO 140196492334912] Epoch[77] Batch[5] avg_epoch_loss=2.129469\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:03 INFO 140196492334912] Epoch[77] Batch [5]#011Speed: 150.00 samples/sec#011loss=2.129469\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:43:02 INFO 139663579178816] Epoch[78] Batch[10] avg_epoch_loss=2.301226\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:02 INFO 139663579178816] Epoch[78] Batch [10]#011Speed: 120.78 samples/sec#011loss=2.329419\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:02 INFO 139663579178816] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11780.853033065796, \"sum\": 11780.853033065796, \"min\": 11780.853033065796}}, \"EndTime\": 1538408582.494736, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408570.713636}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:02 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.923147225 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:02 INFO 139663579178816] #progress_metric: host=algo-2, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:03 INFO 140252856112960] Epoch[77] Batch[5] avg_epoch_loss=2.165469\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:03 INFO 140252856112960] Epoch[77] Batch [5]#011Speed: 121.05 samples/sec#011loss=2.165469\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:04 INFO 139663579178816] Epoch[79] Batch[0] avg_epoch_loss=2.067524\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:05 INFO 140624803325760] Epoch[79] Batch[5] avg_epoch_loss=2.078198\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:05 INFO 140624803325760] Epoch[79] Batch [5]#011Speed: 122.67 samples/sec#011loss=2.078198\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:07 INFO 140252856112960] Epoch[77] Batch[10] avg_epoch_loss=2.271732\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:07 INFO 140252856112960] Epoch[77] Batch [10]#011Speed: 147.30 samples/sec#011loss=2.399247\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:07 INFO 140252856112960] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11271.501064300537, \"sum\": 11271.501064300537, \"min\": 11271.501064300537}}, \"EndTime\": 1538408587.923625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408576.651865}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:07 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.487413227 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:07 INFO 140252856112960] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:09 INFO 139663579178816] Epoch[79] Batch[5] avg_epoch_loss=1.997574\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:09 INFO 139663579178816] Epoch[79] Batch [5]#011Speed: 120.45 samples/sec#011loss=1.997574\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:08 INFO 140196492334912] Epoch[77] Batch[10] avg_epoch_loss=2.008955\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:08 INFO 140196492334912] Epoch[77] Batch [10]#011Speed: 114.76 samples/sec#011loss=1.864339\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:08 INFO 140196492334912] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12166.214942932129, \"sum\": 12166.214942932129, \"min\": 12166.214942932129}}, \"EndTime\": 1538408588.828686, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408576.662205}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:08 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.701409171 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:08 INFO 140196492334912] #progress_metric: host=algo-4, completed 31 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:09 INFO 140624803325760] Epoch[79] Batch[10] avg_epoch_loss=2.016785\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:09 INFO 140624803325760] Epoch[79] Batch [10]#011Speed: 150.28 samples/sec#011loss=1.943089\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:09 INFO 140624803325760] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11144.771099090576, \"sum\": 11144.771099090576, \"min\": 11144.771099090576}}, \"EndTime\": 1538408589.538215, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408578.393196}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:09 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.811736153 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:09 INFO 140624803325760] #progress_metric: host=algo-3, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:10 INFO 140196492334912] Epoch[78] Batch[0] avg_epoch_loss=1.807588\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:10 INFO 140252856112960] Epoch[78] Batch[0] avg_epoch_loss=2.094171\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:11 INFO 140624803325760] Epoch[80] Batch[0] avg_epoch_loss=2.013058\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:13 INFO 139663579178816] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10730.103969573975, \"sum\": 10730.103969573975, \"min\": 10730.103969573975}}, \"EndTime\": 1538408593.225135, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408582.494805}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.188808259 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:14 INFO 140252856112960] Epoch[78] Batch[5] avg_epoch_loss=2.071360\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:14 INFO 140252856112960] Epoch[78] Batch [5]#011Speed: 154.83 samples/sec#011loss=2.071360\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:14 INFO 139663579178816] Epoch[80] Batch[0] avg_epoch_loss=1.961972\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:15 INFO 140196492334912] Epoch[78] Batch[5] avg_epoch_loss=1.971116\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:15 INFO 140196492334912] Epoch[78] Batch [5]#011Speed: 118.61 samples/sec#011loss=1.971116\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:16 INFO 140624803325760] Epoch[80] Batch[5] avg_epoch_loss=2.131035\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:16 INFO 140624803325760] Epoch[80] Batch [5]#011Speed: 154.44 samples/sec#011loss=2.131035\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:18 INFO 140252856112960] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10312.82091140747, \"sum\": 10312.82091140747, \"min\": 10312.82091140747}}, \"EndTime\": 1538408598.236742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408587.923694}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:18 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.946479224 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:18 INFO 140252856112960] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:20 INFO 139663579178816] Epoch[80] Batch[5] avg_epoch_loss=2.076809\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:20 INFO 139663579178816] Epoch[80] Batch [5]#011Speed: 121.21 samples/sec#011loss=2.076809\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:20 INFO 140196492334912] Epoch[78] Batch[10] avg_epoch_loss=2.390166\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:20 INFO 140196492334912] Epoch[78] Batch [10]#011Speed: 142.72 samples/sec#011loss=2.893026\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:20 INFO 140196492334912] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11490.601062774658, \"sum\": 11490.601062774658, \"min\": 11490.601062774658}}, \"EndTime\": 1538408600.319626, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408588.828769}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:20 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.090402953 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:20 INFO 140196492334912] #progress_metric: host=algo-4, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:20 INFO 140252856112960] Epoch[79] Batch[0] avg_epoch_loss=1.857655\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:21 INFO 140624803325760] Epoch[80] Batch[10] avg_epoch_loss=2.295130\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:21 INFO 140624803325760] Epoch[80] Batch [10]#011Speed: 119.54 samples/sec#011loss=2.492043\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:21 INFO 140624803325760] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11867.104053497314, \"sum\": 11867.104053497314, \"min\": 11867.104053497314}}, \"EndTime\": 1538408601.405656, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408589.538301}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:21 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.292565049 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:21 INFO 140624803325760] #progress_metric: host=algo-3, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:22 INFO 140196492334912] Epoch[79] Batch[0] avg_epoch_loss=2.106038\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:22 INFO 140624803325760] Epoch[81] Batch[0] avg_epoch_loss=2.289366\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:24 INFO 140252856112960] Epoch[79] Batch[5] avg_epoch_loss=2.171595\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:24 INFO 140252856112960] Epoch[79] Batch [5]#011Speed: 153.56 samples/sec#011loss=2.171595\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:24 INFO 139663579178816] Epoch[80] Batch[10] avg_epoch_loss=1.882765\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:24 INFO 139663579178816] Epoch[80] Batch [10]#011Speed: 145.25 samples/sec#011loss=1.649914\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:24 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11346.133947372437, \"sum\": 11346.133947372437, \"min\": 11346.133947372437}}, \"EndTime\": 1538408604.571592, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408593.225206}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:24 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.429458446 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:24 INFO 139663579178816] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:26 INFO 139663579178816] Epoch[81] Batch[0] avg_epoch_loss=2.788522\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:26 INFO 140196492334912] Epoch[79] Batch[5] avg_epoch_loss=2.328020\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:26 INFO 140196492334912] Epoch[79] Batch [5]#011Speed: 152.50 samples/sec#011loss=2.328020\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:28 INFO 140624803325760] Epoch[81] Batch[5] avg_epoch_loss=2.228531\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:28 INFO 140624803325760] Epoch[81] Batch [5]#011Speed: 122.34 samples/sec#011loss=2.228531\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:30 INFO 140252856112960] Epoch[79] Batch[10] avg_epoch_loss=2.289389\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:30 INFO 140252856112960] Epoch[79] Batch [10]#011Speed: 118.40 samples/sec#011loss=2.430740\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:30 INFO 140252856112960] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11948.490858078003, \"sum\": 11948.490858078003, \"min\": 11948.490858078003}}, \"EndTime\": 1538408610.185536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408598.236816}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.468773308 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:31 INFO 139663579178816] Epoch[81] Batch[5] avg_epoch_loss=2.128515\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:31 INFO 139663579178816] Epoch[81] Batch [5]#011Speed: 150.04 samples/sec#011loss=2.128515\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:31 INFO 140252856112960] Epoch[80] Batch[0] avg_epoch_loss=2.139060\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:32 INFO 140624803325760] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10629.371881484985, \"sum\": 10629.371881484985, \"min\": 10629.371881484985}}, \"EndTime\": 1538408612.035356, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408601.40574}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:32 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.586805322 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:32 INFO 140624803325760] #progress_metric: host=algo-3, completed 32 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:35 INFO 139663579178816] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10456.753969192505, \"sum\": 10456.753969192505, \"min\": 10456.753969192505}}, \"EndTime\": 1538408615.02869, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408604.571677}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:35 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.103950182 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:35 INFO 139663579178816] #progress_metric: host=algo-2, completed 32 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:43:32 INFO 140196492334912] Epoch[79] Batch[10] avg_epoch_loss=2.428449\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:32 INFO 140196492334912] Epoch[79] Batch [10]#011Speed: 117.23 samples/sec#011loss=2.548964\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:32 INFO 140196492334912] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12038.702011108398, \"sum\": 12038.702011108398, \"min\": 12038.702011108398}}, \"EndTime\": 1538408612.358655, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408600.319708}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:32 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.980743562 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:32 INFO 140196492334912] #progress_metric: host=algo-4, completed 32 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:33 INFO 140624803325760] Epoch[82] Batch[0] avg_epoch_loss=1.705055\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:33 INFO 140196492334912] Epoch[80] Batch[0] avg_epoch_loss=2.017328\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:37 INFO 139663579178816] Epoch[82] Batch[0] avg_epoch_loss=2.043333\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:37 INFO 140252856112960] Epoch[80] Batch[5] avg_epoch_loss=2.201875\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:37 INFO 140252856112960] Epoch[80] Batch [5]#011Speed: 121.66 samples/sec#011loss=2.201875\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:38 INFO 140624803325760] Epoch[82] Batch[5] avg_epoch_loss=2.056858\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:38 INFO 140624803325760] Epoch[82] Batch [5]#011Speed: 120.99 samples/sec#011loss=2.056858\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:39 INFO 140196492334912] Epoch[80] Batch[5] avg_epoch_loss=2.029725\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:39 INFO 140196492334912] Epoch[80] Batch [5]#011Speed: 119.01 samples/sec#011loss=2.029725\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:40 INFO 140252856112960] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10709.003925323486, \"sum\": 10709.003925323486, \"min\": 10709.003925323486}}, \"EndTime\": 1538408620.894854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408610.185623}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.442851314 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:42 INFO 140624803325760] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10663.74921798706, \"sum\": 10663.74921798706, \"min\": 10663.74921798706}}, \"EndTime\": 1538408622.69944, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408612.035438}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:42 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.874830769 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:42 INFO 140624803325760] #progress_metric: host=algo-3, completed 33 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:41 INFO 139663579178816] Epoch[82] Batch[5] avg_epoch_loss=2.071479\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:41 INFO 139663579178816] Epoch[82] Batch [5]#011Speed: 151.11 samples/sec#011loss=2.071479\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:42 INFO 140252856112960] Epoch[81] Batch[0] avg_epoch_loss=2.052833\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:43 INFO 140196492334912] Epoch[80] Batch[10] avg_epoch_loss=2.154090\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:43 INFO 140196492334912] Epoch[80] Batch [10]#011Speed: 144.28 samples/sec#011loss=2.303328\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:43 INFO 140196492334912] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11437.422037124634, \"sum\": 11437.422037124634, \"min\": 11437.422037124634}}, \"EndTime\": 1538408623.796405, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408612.358736}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:43 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.485863302 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:43 INFO 140196492334912] #progress_metric: host=algo-4, completed 32 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:44 INFO 140624803325760] Epoch[83] Batch[0] avg_epoch_loss=2.562443\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:45 INFO 139663579178816] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10422.19877243042, \"sum\": 10422.19877243042, \"min\": 10422.19877243042}}, \"EndTime\": 1538408625.451244, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408615.028779}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:45 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.399548342 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:45 INFO 139663579178816] #progress_metric: host=algo-2, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:46 INFO 140196492334912] Epoch[81] Batch[0] avg_epoch_loss=1.895898\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:47 INFO 140252856112960] Epoch[81] Batch[5] avg_epoch_loss=2.255439\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:47 INFO 140252856112960] Epoch[81] Batch [5]#011Speed: 121.32 samples/sec#011loss=2.255439\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:47 INFO 139663579178816] Epoch[83] Batch[0] avg_epoch_loss=1.950977\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:49 INFO 140624803325760] Epoch[83] Batch[5] avg_epoch_loss=2.340438\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:49 INFO 140624803325760] Epoch[83] Batch [5]#011Speed: 121.42 samples/sec#011loss=2.340438\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:50 INFO 140196492334912] Epoch[81] Batch[5] avg_epoch_loss=2.117621\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:50 INFO 140196492334912] Epoch[81] Batch [5]#011Speed: 148.56 samples/sec#011loss=2.117621\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:52 INFO 139663579178816] Epoch[83] Batch[5] avg_epoch_loss=2.048906\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:52 INFO 139663579178816] Epoch[83] Batch [5]#011Speed: 142.36 samples/sec#011loss=2.048906\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:53 INFO 140624803325760] Epoch[83] Batch[10] avg_epoch_loss=1.931453\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:53 INFO 140624803325760] Epoch[83] Batch [10]#011Speed: 148.05 samples/sec#011loss=1.440670\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:53 INFO 140624803325760] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11200.906991958618, \"sum\": 11200.906991958618, \"min\": 11200.906991958618}}, \"EndTime\": 1538408633.900679, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408622.699509}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.364482389 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:54 INFO 140196492334912] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10573.271989822388, \"sum\": 10573.271989822388, \"min\": 10573.271989822388}}, \"EndTime\": 1538408634.370014, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408623.796489}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:54 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.490997139 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:54 INFO 140196492334912] #progress_metric: host=algo-4, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:52 INFO 140252856112960] Epoch[81] Batch[10] avg_epoch_loss=2.153929\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:52 INFO 140252856112960] Epoch[81] Batch [10]#011Speed: 149.69 samples/sec#011loss=2.032118\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:52 INFO 140252856112960] processed a total of 1353 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11227.581024169922, \"sum\": 11227.581024169922, \"min\": 11227.581024169922}}, \"EndTime\": 1538408632.122751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408620.89493}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:52 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.505706918 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:52 INFO 140252856112960] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:54 INFO 140252856112960] Epoch[82] Batch[0] avg_epoch_loss=1.938196\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:43:56 INFO 140196492334912] Epoch[82] Batch[0] avg_epoch_loss=2.204046\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:43:56 INFO 140624803325760] Epoch[84] Batch[0] avg_epoch_loss=2.086883\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:57 INFO 139663579178816] Epoch[83] Batch[10] avg_epoch_loss=2.221777\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:57 INFO 139663579178816] Epoch[83] Batch [10]#011Speed: 115.25 samples/sec#011loss=2.429222\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:57 INFO 139663579178816] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12417.66905784607, \"sum\": 12417.66905784607, \"min\": 12417.66905784607}}, \"EndTime\": 1538408637.869281, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408625.451336}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=104.607859808 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:58 INFO 140252856112960] Epoch[82] Batch[5] avg_epoch_loss=2.063260\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:43:58 INFO 140252856112960] Epoch[82] Batch [5]#011Speed: 153.60 samples/sec#011loss=2.063260\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:43:59 INFO 139663579178816] Epoch[84] Batch[0] avg_epoch_loss=2.143026\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:00 INFO 140624803325760] Epoch[84] Batch[5] avg_epoch_loss=2.157333\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:00 INFO 140624803325760] Epoch[84] Batch [5]#011Speed: 154.73 samples/sec#011loss=2.157333\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:01 INFO 140196492334912] Epoch[82] Batch[5] avg_epoch_loss=2.108690\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:01 INFO 140196492334912] Epoch[82] Batch [5]#011Speed: 150.70 samples/sec#011loss=2.108690\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:02 INFO 140252856112960] processed a total of 1213 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10340.826988220215, \"sum\": 10340.826988220215, \"min\": 10340.826988220215}}, \"EndTime\": 1538408642.46387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408632.122821}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.300759121 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:44:04 INFO 140624803325760] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10362.175941467285, \"sum\": 10362.175941467285, \"min\": 10362.175941467285}}, \"EndTime\": 1538408644.263185, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408633.900761}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:04 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.752538954 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:04 INFO 140624803325760] #progress_metric: host=algo-3, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:04 INFO 140252856112960] Epoch[83] Batch[0] avg_epoch_loss=2.749252\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:04 INFO 139663579178816] Epoch[84] Batch[5] avg_epoch_loss=2.186270\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:04 INFO 139663579178816] Epoch[84] Batch [5]#011Speed: 117.34 samples/sec#011loss=2.186270\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:06 INFO 140196492334912] Epoch[82] Batch[10] avg_epoch_loss=2.120607\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:06 INFO 140196492334912] Epoch[82] Batch [10]#011Speed: 116.56 samples/sec#011loss=2.134908\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:06 INFO 140196492334912] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12163.702011108398, \"sum\": 12163.702011108398, \"min\": 12163.702011108398}}, \"EndTime\": 1538408646.534054, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408634.370103}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=107.943010794 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 33 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:06 INFO 140624803325760] Epoch[85] Batch[0] avg_epoch_loss=2.126165\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:08 INFO 140196492334912] Epoch[83] Batch[0] avg_epoch_loss=2.112598\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:09 INFO 139663579178816] Epoch[84] Batch[10] avg_epoch_loss=2.108011\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:09 INFO 139663579178816] Epoch[84] Batch [10]#011Speed: 145.05 samples/sec#011loss=2.014101\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:09 INFO 139663579178816] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11475.391149520874, \"sum\": 11475.391149520874, \"min\": 11475.391149520874}}, \"EndTime\": 1538408649.345006, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408637.869373}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:09 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.674652362 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:09 INFO 139663579178816] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:09 INFO 140252856112960] Epoch[83] Batch[5] avg_epoch_loss=2.274121\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:09 INFO 140252856112960] Epoch[83] Batch [5]#011Speed: 152.20 samples/sec#011loss=2.274121\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:10 INFO 140624803325760] Epoch[85] Batch[5] avg_epoch_loss=2.042152\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:10 INFO 140624803325760] Epoch[85] Batch [5]#011Speed: 152.84 samples/sec#011loss=2.042152\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:11 INFO 139663579178816] Epoch[85] Batch[0] avg_epoch_loss=2.116264\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:15 INFO 139663579178816] Epoch[85] Batch[5] avg_epoch_loss=2.075340\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:15 INFO 139663579178816] Epoch[85] Batch [5]#011Speed: 150.55 samples/sec#011loss=2.075340\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:13 INFO 140196492334912] Epoch[83] Batch[5] avg_epoch_loss=2.188595\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:13 INFO 140196492334912] Epoch[83] Batch [5]#011Speed: 116.14 samples/sec#011loss=2.188595\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:14 INFO 140252856112960] Epoch[83] Batch[10] avg_epoch_loss=2.229838\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:14 INFO 140252856112960] Epoch[83] Batch [10]#011Speed: 118.85 samples/sec#011loss=2.176699\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:14 INFO 140252856112960] processed a total of 1356 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12003.818035125732, \"sum\": 12003.818035125732, \"min\": 12003.818035125732}}, \"EndTime\": 1538408654.467988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408642.463946}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.963097984 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:16 INFO 140624803325760] Epoch[85] Batch[10] avg_epoch_loss=2.015454\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:16 INFO 140624803325760] Epoch[85] Batch [10]#011Speed: 116.91 samples/sec#011loss=1.983417\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:16 INFO 140624803325760] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12042.890071868896, \"sum\": 12042.890071868896, \"min\": 12042.890071868896}}, \"EndTime\": 1538408656.30642, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408644.263275}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:16 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=107.946390472 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:16 INFO 140624803325760] #progress_metric: host=algo-3, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:16 INFO 140252856112960] Epoch[84] Batch[0] avg_epoch_loss=2.348071\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:17 INFO 140624803325760] Epoch[86] Batch[0] avg_epoch_loss=1.875646\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:18 INFO 140196492334912] Epoch[83] Batch[10] avg_epoch_loss=2.203803\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:18 INFO 140196492334912] Epoch[83] Batch [10]#011Speed: 140.82 samples/sec#011loss=2.222052\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:18 INFO 140196492334912] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11709.435939788818, \"sum\": 11709.435939788818, \"min\": 11709.435939788818}}, \"EndTime\": 1538408658.243816, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408646.534137}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:18 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.618218115 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:18 INFO 140196492334912] #progress_metric: host=algo-4, completed 33 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:19 INFO 139663579178816] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10491.12582206726, \"sum\": 10491.12582206726, \"min\": 10491.12582206726}}, \"EndTime\": 1538408659.836467, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408649.345091}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:19 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.432792429 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:19 INFO 139663579178816] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:20 INFO 140196492334912] Epoch[84] Batch[0] avg_epoch_loss=2.356812\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:21 INFO 140252856112960] Epoch[84] Batch[5] avg_epoch_loss=2.121562\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:21 INFO 140252856112960] Epoch[84] Batch [5]#011Speed: 119.71 samples/sec#011loss=2.121562\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:22 INFO 139663579178816] Epoch[86] Batch[0] avg_epoch_loss=1.752480\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:25 INFO 140252856112960] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10797.000169754028, \"sum\": 10797.000169754028, \"min\": 10797.000169754028}}, \"EndTime\": 1538408665.265277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408654.468058}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.234773042 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:23 INFO 140624803325760] Epoch[86] Batch[5] avg_epoch_loss=1.987145\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:23 INFO 140624803325760] Epoch[86] Batch [5]#011Speed: 120.29 samples/sec#011loss=1.987145\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:24 INFO 140196492334912] Epoch[84] Batch[5] avg_epoch_loss=2.072792\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:24 INFO 140196492334912] Epoch[84] Batch [5]#011Speed: 149.09 samples/sec#011loss=2.072792\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:26 INFO 140252856112960] Epoch[85] Batch[0] avg_epoch_loss=2.038333\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:26 INFO 139663579178816] Epoch[86] Batch[5] avg_epoch_loss=2.087616\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:26 INFO 139663579178816] Epoch[86] Batch [5]#011Speed: 149.56 samples/sec#011loss=2.087616\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:27 INFO 140624803325760] Epoch[86] Batch[10] avg_epoch_loss=2.228548\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:27 INFO 140624803325760] Epoch[86] Batch [10]#011Speed: 149.40 samples/sec#011loss=2.518231\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:27 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11254.798889160156, \"sum\": 11254.798889160156, \"min\": 11254.798889160156}}, \"EndTime\": 1538408667.561549, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408656.306504}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:27 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.459732037 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:27 INFO 140624803325760] #progress_metric: host=algo-3, completed 34 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:29 INFO 140624803325760] Epoch[87] Batch[0] avg_epoch_loss=1.998246\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:30 INFO 139663579178816] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10585.12806892395, \"sum\": 10585.12806892395, \"min\": 10585.12806892395}}, \"EndTime\": 1538408670.421936, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408659.836554}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:30 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.222367177 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:30 INFO 139663579178816] #progress_metric: host=algo-2, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:30 INFO 140196492334912] Epoch[84] Batch[10] avg_epoch_loss=2.059133\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:30 INFO 140196492334912] Epoch[84] Batch [10]#011Speed: 115.27 samples/sec#011loss=2.042742\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:30 INFO 140196492334912] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12218.921899795532, \"sum\": 12218.921899795532, \"min\": 12218.921899795532}}, \"EndTime\": 1538408670.463068, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408658.243896}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.490895235 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:32 INFO 140196492334912] Epoch[85] Batch[0] avg_epoch_loss=1.676612\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:32 INFO 140252856112960] Epoch[85] Batch[5] avg_epoch_loss=2.172476\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:32 INFO 140252856112960] Epoch[85] Batch [5]#011Speed: 119.92 samples/sec#011loss=2.172476\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:36 INFO 140252856112960] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10852.799892425537, \"sum\": 10852.799892425537, \"min\": 10852.799892425537}}, \"EndTime\": 1538408676.118403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408665.265354}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:36 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.821376335 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:36 INFO 140252856112960] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:44:32 INFO 139663579178816] Epoch[87] Batch[0] avg_epoch_loss=2.098401\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:34 INFO 140624803325760] Epoch[87] Batch[5] avg_epoch_loss=2.016731\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:34 INFO 140624803325760] Epoch[87] Batch [5]#011Speed: 153.47 samples/sec#011loss=2.016731\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:37 INFO 139663579178816] Epoch[87] Batch[5] avg_epoch_loss=2.244349\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:37 INFO 139663579178816] Epoch[87] Batch [5]#011Speed: 146.26 samples/sec#011loss=2.244349\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:37 INFO 140196492334912] Epoch[85] Batch[5] avg_epoch_loss=1.978378\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:37 INFO 140196492334912] Epoch[85] Batch [5]#011Speed: 117.40 samples/sec#011loss=1.978378\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:37 INFO 140252856112960] Epoch[86] Batch[0] avg_epoch_loss=1.812019\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:39 INFO 140624803325760] Epoch[87] Batch[10] avg_epoch_loss=1.930336\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:39 INFO 140624803325760] Epoch[87] Batch [10]#011Speed: 119.82 samples/sec#011loss=1.826662\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:39 INFO 140624803325760] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11901.59296989441, \"sum\": 11901.59296989441, \"min\": 11901.59296989441}}, \"EndTime\": 1538408679.463469, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408667.561631}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:39 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.572281781 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:39 INFO 140624803325760] #progress_metric: host=algo-3, completed 35 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:41 INFO 140624803325760] Epoch[88] Batch[0] avg_epoch_loss=1.650386\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:42 INFO 140196492334912] Epoch[85] Batch[10] avg_epoch_loss=1.716106\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:42 INFO 140196492334912] Epoch[85] Batch [10]#011Speed: 142.81 samples/sec#011loss=1.401379\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:42 INFO 140196492334912] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11590.555906295776, \"sum\": 11590.555906295776, \"min\": 11590.555906295776}}, \"EndTime\": 1538408682.053959, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408670.463162}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:42 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.210049147 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:42 INFO 140196492334912] #progress_metric: host=algo-4, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:43 INFO 140252856112960] Epoch[86] Batch[5] avg_epoch_loss=2.028429\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:43 INFO 140252856112960] Epoch[86] Batch [5]#011Speed: 119.67 samples/sec#011loss=2.028429\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:44 INFO 140196492334912] Epoch[86] Batch[0] avg_epoch_loss=2.133729\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:46 INFO 140252856112960] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10746.876955032349, \"sum\": 10746.876955032349, \"min\": 10746.876955032349}}, \"EndTime\": 1538408686.865714, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408676.118484}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.079618821 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:42 INFO 139663579178816] Epoch[87] Batch[10] avg_epoch_loss=2.258298\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:42 INFO 139663579178816] Epoch[87] Batch [10]#011Speed: 117.45 samples/sec#011loss=2.275036\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:42 INFO 139663579178816] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12183.685064315796, \"sum\": 12183.685064315796, \"min\": 12183.685064315796}}, \"EndTime\": 1538408682.605965, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408670.422026}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:42 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=107.10932738 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:42 INFO 139663579178816] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:44 INFO 139663579178816] Epoch[88] Batch[0] avg_epoch_loss=2.100359\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:46 INFO 140624803325760] Epoch[88] Batch[5] avg_epoch_loss=2.052404\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:46 INFO 140624803325760] Epoch[88] Batch [5]#011Speed: 121.97 samples/sec#011loss=2.052404\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:48 INFO 140196492334912] Epoch[86] Batch[5] avg_epoch_loss=1.985870\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:48 INFO 140196492334912] Epoch[86] Batch [5]#011Speed: 149.55 samples/sec#011loss=1.985870\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:48 INFO 140252856112960] Epoch[87] Batch[0] avg_epoch_loss=2.421928\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:49 INFO 139663579178816] Epoch[88] Batch[5] avg_epoch_loss=2.051334\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:49 INFO 139663579178816] Epoch[88] Batch [5]#011Speed: 122.73 samples/sec#011loss=2.051334\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:50 INFO 140624803325760] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10599.700927734375, \"sum\": 10599.700927734375, \"min\": 10599.700927734375}}, \"EndTime\": 1538408690.063496, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408679.463551}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:50 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.813229806 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:50 INFO 140624803325760] #progress_metric: host=algo-3, completed 35 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:51 INFO 140624803325760] Epoch[89] Batch[0] avg_epoch_loss=2.077405\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:52 INFO 140196492334912] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10604.387998580933, \"sum\": 10604.387998580933, \"min\": 10604.387998580933}}, \"EndTime\": 1538408692.658671, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408682.05404}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.874253419 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 34 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:53 INFO 139663579178816] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10814.704179763794, \"sum\": 10814.704179763794, \"min\": 10814.704179763794}}, \"EndTime\": 1538408693.421001, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408682.606052}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:53 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.65707683 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:53 INFO 139663579178816] #progress_metric: host=algo-2, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:53 INFO 140252856112960] Epoch[87] Batch[5] avg_epoch_loss=2.139852\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:53 INFO 140252856112960] Epoch[87] Batch [5]#011Speed: 121.39 samples/sec#011loss=2.139852\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:44:55 INFO 139663579178816] Epoch[89] Batch[0] avg_epoch_loss=2.340552\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:55 INFO 140196492334912] Epoch[87] Batch[0] avg_epoch_loss=2.180893\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:56 INFO 140624803325760] Epoch[89] Batch[5] avg_epoch_loss=2.085034\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:44:56 INFO 140624803325760] Epoch[89] Batch [5]#011Speed: 123.23 samples/sec#011loss=2.085034\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:58 INFO 140252856112960] Epoch[87] Batch[10] avg_epoch_loss=2.183829\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:58 INFO 140252856112960] Epoch[87] Batch [10]#011Speed: 148.82 samples/sec#011loss=2.236601\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:58 INFO 140252856112960] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11197.21794128418, \"sum\": 11197.21794128418, \"min\": 11197.21794128418}}, \"EndTime\": 1538408698.063237, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408686.865788}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:58 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.67031824 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:44:58 INFO 140252856112960] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:59 INFO 140196492334912] Epoch[87] Batch[5] avg_epoch_loss=2.147945\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:44:59 INFO 140196492334912] Epoch[87] Batch [5]#011Speed: 150.07 samples/sec#011loss=2.147945\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:00 INFO 139663579178816] Epoch[89] Batch[5] avg_epoch_loss=2.214101\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:00 INFO 139663579178816] Epoch[89] Batch [5]#011Speed: 119.17 samples/sec#011loss=2.214101\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:00 INFO 140252856112960] Epoch[88] Batch[0] avg_epoch_loss=2.028812\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:01 INFO 140624803325760] Epoch[89] Batch[10] avg_epoch_loss=2.061089\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:01 INFO 140624803325760] Epoch[89] Batch [10]#011Speed: 151.61 samples/sec#011loss=2.032355\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:01 INFO 140624803325760] processed a total of 1351 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11059.78012084961, \"sum\": 11059.78012084961, \"min\": 11059.78012084961}}, \"EndTime\": 1538408701.123633, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408690.063582}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:01 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.152947392 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:01 INFO 140624803325760] #progress_metric: host=algo-3, completed 36 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:03 INFO 140624803325760] Epoch[90] Batch[0] avg_epoch_loss=2.588578\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:45:04 INFO 140252856112960] Epoch[88] Batch[5] avg_epoch_loss=2.124728\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:04 INFO 140252856112960] Epoch[88] Batch [5]#011Speed: 150.24 samples/sec#011loss=2.124728\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:04 INFO 139663579178816] Epoch[89] Batch[10] avg_epoch_loss=1.899794\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:04 INFO 139663579178816] Epoch[89] Batch [10]#011Speed: 143.07 samples/sec#011loss=1.522627\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:04 INFO 139663579178816] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11556.242942810059, \"sum\": 11556.242942810059, \"min\": 11556.242942810059}}, \"EndTime\": 1538408704.977619, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408693.421113}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:04 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.62672923 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:04 INFO 139663579178816] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:04 INFO 140196492334912] Epoch[87] Batch[10] avg_epoch_loss=2.219774\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:04 INFO 140196492334912] Epoch[87] Batch [10]#011Speed: 114.34 samples/sec#011loss=2.305968\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:04 INFO 140196492334912] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12216.268062591553, \"sum\": 12216.268062591553, \"min\": 12216.268062591553}}, \"EndTime\": 1538408704.875287, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408692.65876}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:04 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.350220743 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:04 INFO 140196492334912] #progress_metric: host=algo-4, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:06 INFO 140196492334912] Epoch[88] Batch[0] avg_epoch_loss=2.026222\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:07 INFO 139663579178816] Epoch[90] Batch[0] avg_epoch_loss=2.250614\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:07 INFO 140624803325760] Epoch[90] Batch[5] avg_epoch_loss=2.081398\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:07 INFO 140624803325760] Epoch[90] Batch [5]#011Speed: 153.41 samples/sec#011loss=2.081398\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:10 INFO 140252856112960] Epoch[88] Batch[10] avg_epoch_loss=1.741828\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:10 INFO 140252856112960] Epoch[88] Batch [10]#011Speed: 116.37 samples/sec#011loss=1.282348\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:10 INFO 140252856112960] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12083.724975585938, \"sum\": 12083.724975585938, \"min\": 12083.724975585938}}, \"EndTime\": 1538408710.147244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408698.063303}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.009355232 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:11 INFO 140624803325760] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10315.198183059692, \"sum\": 10315.198183059692, \"min\": 10315.198183059692}}, \"EndTime\": 1538408711.439162, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408701.123715}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:11 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.505406493 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:11 INFO 140624803325760] #progress_metric: host=algo-3, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:11 INFO 140252856112960] Epoch[89] Batch[0] avg_epoch_loss=2.131244\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:11 INFO 139663579178816] Epoch[90] Batch[5] avg_epoch_loss=2.168593\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:11 INFO 139663579178816] Epoch[90] Batch [5]#011Speed: 155.03 samples/sec#011loss=2.168593\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:11 INFO 140196492334912] Epoch[88] Batch[5] avg_epoch_loss=2.013662\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:11 INFO 140196492334912] Epoch[88] Batch [5]#011Speed: 117.41 samples/sec#011loss=2.013662\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:13 INFO 140624803325760] Epoch[91] Batch[0] avg_epoch_loss=2.285611\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:15 INFO 140196492334912] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11046.10800743103, \"sum\": 11046.10800743103, \"min\": 11046.10800743103}}, \"EndTime\": 1538408715.921726, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408704.875371}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:15 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.432139572 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:15 INFO 140196492334912] #progress_metric: host=algo-4, completed 35 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:17 INFO 140624803325760] Epoch[91] Batch[5] avg_epoch_loss=1.945100\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:17 INFO 140624803325760] Epoch[91] Batch [5]#011Speed: 153.69 samples/sec#011loss=1.945100\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:16 INFO 139663579178816] Epoch[90] Batch[10] avg_epoch_loss=2.089024\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:16 INFO 139663579178816] Epoch[90] Batch [10]#011Speed: 123.54 samples/sec#011loss=1.993541\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:16 INFO 139663579178816] processed a total of 1407 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11761.75594329834, \"sum\": 11761.75594329834, \"min\": 11761.75594329834}}, \"EndTime\": 1538408716.739709, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408704.977704}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.623702902 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:17 INFO 140196492334912] Epoch[89] Batch[0] avg_epoch_loss=2.297374\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:17 INFO 140252856112960] Epoch[89] Batch[5] avg_epoch_loss=1.994780\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:17 INFO 140252856112960] Epoch[89] Batch [5]#011Speed: 120.43 samples/sec#011loss=1.994780\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:18 INFO 139663579178816] Epoch[91] Batch[0] avg_epoch_loss=1.805677\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:21 INFO 140252856112960] Epoch[89] Batch[10] avg_epoch_loss=2.265199\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:21 INFO 140252856112960] Epoch[89] Batch [10]#011Speed: 148.20 samples/sec#011loss=2.589702\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:21 INFO 140252856112960] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11295.819997787476, \"sum\": 11295.819997787476, \"min\": 11295.819997787476}}, \"EndTime\": 1538408721.443376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408710.147323}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.741573485 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:22 INFO 140196492334912] Epoch[89] Batch[5] avg_epoch_loss=2.174618\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:22 INFO 140196492334912] Epoch[89] Batch [5]#011Speed: 119.44 samples/sec#011loss=2.174618\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:23 INFO 140624803325760] Epoch[91] Batch[10] avg_epoch_loss=2.191244\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:23 INFO 140624803325760] Epoch[91] Batch [10]#011Speed: 120.31 samples/sec#011loss=2.486616\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:23 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11825.919151306152, \"sum\": 11825.919151306152, \"min\": 11825.919151306152}}, \"EndTime\": 1538408723.265425, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408711.439255}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:23 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.095978495 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:23 INFO 140624803325760] #progress_metric: host=algo-3, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:23 INFO 140252856112960] Epoch[90] Batch[0] avg_epoch_loss=2.096755\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:24 INFO 140624803325760] Epoch[92] Batch[0] avg_epoch_loss=2.083674\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:26 INFO 140196492334912] processed a total of 1203 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10802.423000335693, \"sum\": 10802.423000335693, \"min\": 10802.423000335693}}, \"EndTime\": 1538408726.724503, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408715.921826}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:26 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.362564651 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:26 INFO 140196492334912] #progress_metric: host=algo-4, completed 36 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:45:23 INFO 139663579178816] Epoch[91] Batch[5] avg_epoch_loss=2.125684\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:23 INFO 139663579178816] Epoch[91] Batch [5]#011Speed: 124.08 samples/sec#011loss=2.125684\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:27 INFO 139663579178816] Epoch[91] Batch[10] avg_epoch_loss=2.101517\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:27 INFO 139663579178816] Epoch[91] Batch [10]#011Speed: 151.76 samples/sec#011loss=2.072516\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:27 INFO 139663579178816] processed a total of 1337 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11048.366785049438, \"sum\": 11048.366785049438, \"min\": 11048.366785049438}}, \"EndTime\": 1538408727.788403, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408716.739795}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:27 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.011990186 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:27 INFO 139663579178816] #progress_metric: host=algo-2, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:28 INFO 140196492334912] Epoch[90] Batch[0] avg_epoch_loss=1.940932\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:28 INFO 140252856112960] Epoch[90] Batch[5] avg_epoch_loss=2.115186\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:28 INFO 140252856112960] Epoch[90] Batch [5]#011Speed: 151.70 samples/sec#011loss=2.115186\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:30 INFO 139663579178816] Epoch[92] Batch[0] avg_epoch_loss=1.969319\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:30 INFO 140624803325760] Epoch[92] Batch[5] avg_epoch_loss=2.200074\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:30 INFO 140624803325760] Epoch[92] Batch [5]#011Speed: 122.24 samples/sec#011loss=2.200074\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:31 INFO 140252856112960] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10465.133905410767, \"sum\": 10465.133905410767, \"min\": 10465.133905410767}}, \"EndTime\": 1538408731.908806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408721.443447}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:31 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.391863531 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:31 INFO 140252856112960] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:33 INFO 140196492334912] Epoch[90] Batch[5] avg_epoch_loss=2.156193\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:33 INFO 140196492334912] Epoch[90] Batch [5]#011Speed: 118.81 samples/sec#011loss=2.156193\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:34 INFO 139663579178816] Epoch[92] Batch[5] avg_epoch_loss=2.151941\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:34 INFO 139663579178816] Epoch[92] Batch [5]#011Speed: 154.42 samples/sec#011loss=2.151941\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:34 INFO 140624803325760] Epoch[92] Batch[10] avg_epoch_loss=1.980149\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:34 INFO 140624803325760] Epoch[92] Batch [10]#011Speed: 150.13 samples/sec#011loss=1.716238\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:34 INFO 140624803325760] processed a total of 1357 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11132.64799118042, \"sum\": 11132.64799118042, \"min\": 11132.64799118042}}, \"EndTime\": 1538408734.398379, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408723.265515}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:34 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.892352714 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:34 INFO 140624803325760] #progress_metric: host=algo-3, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:34 INFO 140252856112960] Epoch[91] Batch[0] avg_epoch_loss=2.249718\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:38 INFO 140252856112960] Epoch[91] Batch[5] avg_epoch_loss=1.968885\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:38 INFO 140252856112960] Epoch[91] Batch [5]#011Speed: 152.17 samples/sec#011loss=1.968885\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:36 INFO 140624803325760] Epoch[93] Batch[0] avg_epoch_loss=2.339685\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:38 INFO 140196492334912] Epoch[90] Batch[10] avg_epoch_loss=1.772945\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:38 INFO 140196492334912] Epoch[90] Batch [10]#011Speed: 142.00 samples/sec#011loss=1.313049\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:38 INFO 140196492334912] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11501.065969467163, \"sum\": 11501.065969467163, \"min\": 11501.065969467163}}, \"EndTime\": 1538408738.225923, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408726.72459}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:38 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.118729973 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:38 INFO 140196492334912] #progress_metric: host=algo-4, completed 36 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:39 INFO 139663579178816] Epoch[92] Batch[10] avg_epoch_loss=2.236313\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:39 INFO 139663579178816] Epoch[92] Batch [10]#011Speed: 121.54 samples/sec#011loss=2.337559\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:39 INFO 139663579178816] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11771.610975265503, \"sum\": 11771.610975265503, \"min\": 11771.610975265503}}, \"EndTime\": 1538408739.560341, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408727.788486}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:39 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.708207781 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:39 INFO 139663579178816] #progress_metric: host=algo-2, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:40 INFO 140196492334912] Epoch[91] Batch[0] avg_epoch_loss=2.146893\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:40 INFO 140624803325760] Epoch[93] Batch[5] avg_epoch_loss=2.025617\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:40 INFO 140624803325760] Epoch[93] Batch [5]#011Speed: 154.26 samples/sec#011loss=2.025617\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:41 INFO 139663579178816] Epoch[93] Batch[0] avg_epoch_loss=1.889092\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:43 INFO 140252856112960] Epoch[91] Batch[10] avg_epoch_loss=1.992200\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:43 INFO 140252856112960] Epoch[91] Batch [10]#011Speed: 118.95 samples/sec#011loss=2.020179\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:43 INFO 140252856112960] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11988.84105682373, \"sum\": 11988.84105682373, \"min\": 11988.84105682373}}, \"EndTime\": 1538408743.897957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408731.908883}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:43 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.599892231 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:43 INFO 140252856112960] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:44 INFO 140196492334912] Epoch[91] Batch[5] avg_epoch_loss=2.022807\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:44 INFO 140196492334912] Epoch[91] Batch [5]#011Speed: 147.92 samples/sec#011loss=2.022807\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:45 INFO 140252856112960] Epoch[92] Batch[0] avg_epoch_loss=2.341006\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:48 INFO 140196492334912] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10575.525999069214, \"sum\": 10575.525999069214, \"min\": 10575.525999069214}}, \"EndTime\": 1538408748.801774, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408738.226006}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:48 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.413632224 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:48 INFO 140196492334912] #progress_metric: host=algo-4, completed 36 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:46 INFO 139663579178816] Epoch[93] Batch[5] avg_epoch_loss=1.988537\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:46 INFO 139663579178816] Epoch[93] Batch [5]#011Speed: 121.46 samples/sec#011loss=1.988537\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:46 INFO 140624803325760] Epoch[93] Batch[10] avg_epoch_loss=1.989886\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:46 INFO 140624803325760] Epoch[93] Batch [10]#011Speed: 118.02 samples/sec#011loss=1.947009\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:46 INFO 140624803325760] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11894.5951461792, \"sum\": 11894.5951461792, \"min\": 11894.5951461792}}, \"EndTime\": 1538408746.29331, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408734.398463}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=107.862929832 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 37 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:47 INFO 140624803325760] Epoch[94] Batch[0] avg_epoch_loss=1.755054\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:50 INFO 139663579178816] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10812.144041061401, \"sum\": 10812.144041061401, \"min\": 10812.144041061401}}, \"EndTime\": 1538408750.372821, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408739.560427}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:50 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.516715306 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:50 INFO 139663579178816] #progress_metric: host=algo-2, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:50 INFO 140252856112960] Epoch[92] Batch[5] avg_epoch_loss=2.086020\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:50 INFO 140252856112960] Epoch[92] Batch [5]#011Speed: 120.59 samples/sec#011loss=2.086020\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:51 INFO 140196492334912] Epoch[92] Batch[0] avg_epoch_loss=2.403010\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:52 INFO 139663579178816] Epoch[94] Batch[0] avg_epoch_loss=2.021883\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:53 INFO 140624803325760] Epoch[94] Batch[5] avg_epoch_loss=2.072112\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:53 INFO 140624803325760] Epoch[94] Batch [5]#011Speed: 119.13 samples/sec#011loss=2.072112\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:54 INFO 140252856112960] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10755.784034729004, \"sum\": 10755.784034729004, \"min\": 10755.784034729004}}, \"EndTime\": 1538408754.65405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408743.898038}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:54 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.074808009 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:45:54 INFO 140252856112960] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:55 INFO 140196492334912] Epoch[92] Batch[5] avg_epoch_loss=2.039227\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:55 INFO 140196492334912] Epoch[92] Batch [5]#011Speed: 150.32 samples/sec#011loss=2.039227\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:57 INFO 140624803325760] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10856.810092926025, \"sum\": 10856.810092926025, \"min\": 10856.810092926025}}, \"EndTime\": 1538408757.150451, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408746.293399}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:57 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.133964937 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:57 INFO 140624803325760] #progress_metric: host=algo-3, completed 38 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:45:56 INFO 140252856112960] Epoch[93] Batch[0] avg_epoch_loss=1.898906\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:57 INFO 139663579178816] Epoch[94] Batch[5] avg_epoch_loss=2.167933\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:45:57 INFO 139663579178816] Epoch[94] Batch [5]#011Speed: 120.09 samples/sec#011loss=2.167933\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:45:58 INFO 140624803325760] Epoch[95] Batch[0] avg_epoch_loss=2.781043\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:59 INFO 140196492334912] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10517.732858657837, \"sum\": 10517.732858657837, \"min\": 10517.732858657837}}, \"EndTime\": 1538408759.319854, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408748.801867}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.465118138 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:45:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:01 INFO 140196492334912] Epoch[93] Batch[0] avg_epoch_loss=2.204038\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:01 INFO 140252856112960] Epoch[93] Batch[5] avg_epoch_loss=2.057335\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:01 INFO 140252856112960] Epoch[93] Batch [5]#011Speed: 120.22 samples/sec#011loss=2.057335\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:01 INFO 139663579178816] Epoch[94] Batch[10] avg_epoch_loss=1.878623\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:01 INFO 139663579178816] Epoch[94] Batch [10]#011Speed: 143.72 samples/sec#011loss=1.531451\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:01 INFO 139663579178816] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11419.126987457275, \"sum\": 11419.126987457275, \"min\": 11419.126987457275}}, \"EndTime\": 1538408761.792309, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408750.37292}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.616843603 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:04 INFO 139663579178816] Epoch[95] Batch[0] avg_epoch_loss=1.935374\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:04 INFO 140624803325760] Epoch[95] Batch[5] avg_epoch_loss=2.206982\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:04 INFO 140624803325760] Epoch[95] Batch [5]#011Speed: 121.79 samples/sec#011loss=2.206982\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:05 INFO 140252856112960] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10766.479969024658, \"sum\": 10766.479969024658, \"min\": 10766.479969024658}}, \"EndTime\": 1538408765.420837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408754.654122}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:05 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.421941784 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:05 INFO 140252856112960] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:08 INFO 140624803325760] Epoch[95] Batch[10] avg_epoch_loss=2.171853\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:08 INFO 140624803325760] Epoch[95] Batch [10]#011Speed: 151.55 samples/sec#011loss=2.129699\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:08 INFO 140624803325760] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11121.246099472046, \"sum\": 11121.246099472046, \"min\": 11121.246099472046}}, \"EndTime\": 1538408768.271996, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408757.150522}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.499903379 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:05 INFO 140196492334912] Epoch[93] Batch[5] avg_epoch_loss=2.153335\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:05 INFO 140196492334912] Epoch[93] Batch [5]#011Speed: 151.26 samples/sec#011loss=2.153335\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:07 INFO 140252856112960] Epoch[94] Batch[0] avg_epoch_loss=1.730271\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:08 INFO 139663579178816] Epoch[95] Batch[5] avg_epoch_loss=2.150154\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:08 INFO 139663579178816] Epoch[95] Batch [5]#011Speed: 154.26 samples/sec#011loss=2.150154\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:10 INFO 140624803325760] Epoch[96] Batch[0] avg_epoch_loss=2.197292\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:11 INFO 140196492334912] Epoch[93] Batch[10] avg_epoch_loss=2.113574\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:11 INFO 140196492334912] Epoch[93] Batch [10]#011Speed: 118.06 samples/sec#011loss=2.065861\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:11 INFO 140196492334912] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12060.850858688354, \"sum\": 12060.850858688354, \"min\": 12060.850858688354}}, \"EndTime\": 1538408771.381058, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408759.319942}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:11 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.117188126 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:11 INFO 140196492334912] #progress_metric: host=algo-4, completed 37 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:12 INFO 139663579178816] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10340.056896209717, \"sum\": 10340.056896209717, \"min\": 10340.056896209717}}, \"EndTime\": 1538408772.132694, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408761.792393}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:12 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.305514524 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:12 INFO 139663579178816] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:12 INFO 140252856112960] Epoch[94] Batch[5] avg_epoch_loss=1.997054\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:12 INFO 140252856112960] Epoch[94] Batch [5]#011Speed: 119.74 samples/sec#011loss=1.997054\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:12 INFO 140196492334912] Epoch[94] Batch[0] avg_epoch_loss=1.648298\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:14 INFO 139663579178816] Epoch[96] Batch[0] avg_epoch_loss=2.272545\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:14 INFO 140624803325760] Epoch[96] Batch[5] avg_epoch_loss=1.986640\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:14 INFO 140624803325760] Epoch[96] Batch [5]#011Speed: 155.93 samples/sec#011loss=1.986640\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:16 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10853.775978088379, \"sum\": 10853.775978088379, \"min\": 10853.775978088379}}, \"EndTime\": 1538408776.274922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408765.420908}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:16 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.534688992 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:16 INFO 140252856112960] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:18 INFO 139663579178816] Epoch[96] Batch[5] avg_epoch_loss=2.085419\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:18 INFO 139663579178816] Epoch[96] Batch [5]#011Speed: 155.55 samples/sec#011loss=2.085419\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:17 INFO 140252856112960] Epoch[95] Batch[0] avg_epoch_loss=1.925119\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:18 INFO 140196492334912] Epoch[94] Batch[5] avg_epoch_loss=2.005000\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:18 INFO 140196492334912] Epoch[94] Batch [5]#011Speed: 116.56 samples/sec#011loss=2.005000\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:20 INFO 140624803325760] Epoch[96] Batch[10] avg_epoch_loss=1.969351\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:20 INFO 140624803325760] Epoch[96] Batch [10]#011Speed: 121.42 samples/sec#011loss=1.948603\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:20 INFO 140624803325760] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11753.346920013428, \"sum\": 11753.346920013428, \"min\": 11753.346920013428}}, \"EndTime\": 1538408780.025639, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408768.272064}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.541709277 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 38 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:21 INFO 140624803325760] Epoch[97] Batch[0] avg_epoch_loss=1.946437\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:22 INFO 140196492334912] Epoch[94] Batch[10] avg_epoch_loss=2.061958\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:22 INFO 140196492334912] Epoch[94] Batch [10]#011Speed: 143.76 samples/sec#011loss=2.130307\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:22 INFO 140196492334912] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11560.028076171875, \"sum\": 11560.028076171875, \"min\": 11560.028076171875}}, \"EndTime\": 1538408782.941412, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408771.381141}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:22 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.060416518 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:22 INFO 140196492334912] #progress_metric: host=algo-4, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:23 INFO 140252856112960] Epoch[95] Batch[5] avg_epoch_loss=2.053110\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:23 INFO 140252856112960] Epoch[95] Batch [5]#011Speed: 120.40 samples/sec#011loss=2.053110\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:23 INFO 139663579178816] Epoch[96] Batch[10] avg_epoch_loss=1.928295\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:23 INFO 139663579178816] Epoch[96] Batch [10]#011Speed: 120.82 samples/sec#011loss=1.739747\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:23 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11717.948198318481, \"sum\": 11717.948198318481, \"min\": 11717.948198318481}}, \"EndTime\": 1538408783.85094, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408772.132768}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:23 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.830748227 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:23 INFO 139663579178816] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:25 INFO 139663579178816] Epoch[97] Batch[0] avg_epoch_loss=1.898314\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:25 INFO 140196492334912] Epoch[95] Batch[0] avg_epoch_loss=2.067774\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:27 INFO 140252856112960] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10925.91118812561, \"sum\": 10925.91118812561, \"min\": 10925.91118812561}}, \"EndTime\": 1538408787.201158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408776.274992}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:27 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.785392807 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:27 INFO 140252856112960] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:46:27 INFO 140624803325760] Epoch[97] Batch[5] avg_epoch_loss=2.000150\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:27 INFO 140624803325760] Epoch[97] Batch [5]#011Speed: 117.53 samples/sec#011loss=2.000150\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:28 INFO 140252856112960] Epoch[96] Batch[0] avg_epoch_loss=1.983119\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:29 INFO 140196492334912] Epoch[95] Batch[5] avg_epoch_loss=2.020173\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:29 INFO 140196492334912] Epoch[95] Batch [5]#011Speed: 148.25 samples/sec#011loss=2.020173\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:30 INFO 139663579178816] Epoch[97] Batch[5] avg_epoch_loss=2.182891\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:30 INFO 139663579178816] Epoch[97] Batch [5]#011Speed: 123.66 samples/sec#011loss=2.182891\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:31 INFO 140624803325760] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11031.606912612915, \"sum\": 11031.606912612915, \"min\": 11031.606912612915}}, \"EndTime\": 1538408791.057547, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408780.025708}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.406038321 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 39 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:32 INFO 140624803325760] Epoch[98] Batch[0] avg_epoch_loss=2.124208\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:34 INFO 140252856112960] Epoch[96] Batch[5] avg_epoch_loss=2.284744\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:34 INFO 140252856112960] Epoch[96] Batch [5]#011Speed: 119.23 samples/sec#011loss=2.284744\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:34 INFO 139663579178816] Epoch[97] Batch[10] avg_epoch_loss=2.129664\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:34 INFO 139663579178816] Epoch[97] Batch [10]#011Speed: 150.33 samples/sec#011loss=2.065791\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:34 INFO 139663579178816] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11010.241985321045, \"sum\": 11010.241985321045, \"min\": 11010.241985321045}}, \"EndTime\": 1538408794.861507, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408783.850992}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:34 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.52677569 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:34 INFO 139663579178816] #progress_metric: host=algo-2, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:35 INFO 140196492334912] Epoch[95] Batch[10] avg_epoch_loss=1.776458\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:35 INFO 140196492334912] Epoch[95] Batch [10]#011Speed: 115.26 samples/sec#011loss=1.483999\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:35 INFO 140196492334912] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12302.87480354309, \"sum\": 12302.87480354309, \"min\": 12302.87480354309}}, \"EndTime\": 1538408795.244657, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408782.941534}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.584106436 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:36 INFO 140196492334912] Epoch[96] Batch[0] avg_epoch_loss=2.161213\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:38 INFO 140252856112960] Epoch[96] Batch[10] avg_epoch_loss=2.136717\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:38 INFO 140252856112960] Epoch[96] Batch [10]#011Speed: 148.22 samples/sec#011loss=1.959085\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:38 INFO 140252856112960] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11368.136882781982, \"sum\": 11368.136882781982, \"min\": 11368.136882781982}}, \"EndTime\": 1538408798.569605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408787.201237}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:38 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.386066238 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:38 INFO 140252856112960] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:37 INFO 139663579178816] Epoch[98] Batch[0] avg_epoch_loss=2.271086\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:37 INFO 140624803325760] Epoch[98] Batch[5] avg_epoch_loss=2.037374\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:37 INFO 140624803325760] Epoch[98] Batch [5]#011Speed: 121.18 samples/sec#011loss=2.037374\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:40 INFO 140252856112960] Epoch[97] Batch[0] avg_epoch_loss=1.658610\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:41 INFO 139663579178816] Epoch[98] Batch[5] avg_epoch_loss=1.998037\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:41 INFO 139663579178816] Epoch[98] Batch [5]#011Speed: 154.94 samples/sec#011loss=1.998037\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:41 INFO 140624803325760] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10693.19200515747, \"sum\": 10693.19200515747, \"min\": 10693.19200515747}}, \"EndTime\": 1538408801.751062, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408791.057624}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:41 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.326834338 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:41 INFO 140624803325760] #progress_metric: host=algo-3, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:42 INFO 140196492334912] Epoch[96] Batch[5] avg_epoch_loss=2.149751\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:42 INFO 140196492334912] Epoch[96] Batch [5]#011Speed: 117.98 samples/sec#011loss=2.149751\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:43 INFO 140624803325760] Epoch[99] Batch[0] avg_epoch_loss=1.924099\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:45 INFO 140252856112960] Epoch[97] Batch[5] avg_epoch_loss=1.992478\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:45 INFO 140252856112960] Epoch[97] Batch [5]#011Speed: 149.45 samples/sec#011loss=1.992478\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:46 INFO 140196492334912] Epoch[96] Batch[10] avg_epoch_loss=1.730771\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:46 INFO 140196492334912] Epoch[96] Batch [10]#011Speed: 143.24 samples/sec#011loss=1.227995\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:46 INFO 140196492334912] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11464.499950408936, \"sum\": 11464.499950408936, \"min\": 11464.499950408936}}, \"EndTime\": 1538408806.709538, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408795.244733}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:46 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.519483384 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:46 INFO 140196492334912] #progress_metric: host=algo-4, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:49 INFO 140252856112960] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10565.836906433105, \"sum\": 10565.836906433105, \"min\": 10565.836906433105}}, \"EndTime\": 1538408809.135735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408798.569674}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:49 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.049288666 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:49 INFO 140252856112960] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:46 INFO 139663579178816] Epoch[98] Batch[10] avg_epoch_loss=2.063523\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:46 INFO 139663579178816] Epoch[98] Batch [10]#011Speed: 122.83 samples/sec#011loss=2.142106\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:46 INFO 139663579178816] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11741.179943084717, \"sum\": 11741.179943084717, \"min\": 11741.179943084717}}, \"EndTime\": 1538408806.602986, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408794.861576}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:46 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.893719723 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:46 INFO 139663579178816] #progress_metric: host=algo-2, completed 39 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:48 INFO 139663579178816] Epoch[99] Batch[0] avg_epoch_loss=2.008331\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:48 INFO 140624803325760] Epoch[99] Batch[5] avg_epoch_loss=2.073679\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:48 INFO 140624803325760] Epoch[99] Batch [5]#011Speed: 122.84 samples/sec#011loss=2.073679\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:49 INFO 140196492334912] Epoch[97] Batch[0] avg_epoch_loss=1.988062\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:51 INFO 140252856112960] Epoch[98] Batch[0] avg_epoch_loss=2.257193\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:52 INFO 140624803325760] Epoch[99] Batch[10] avg_epoch_loss=2.001407\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:52 INFO 140624803325760] Epoch[99] Batch [10]#011Speed: 149.67 samples/sec#011loss=1.914680\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:52 INFO 140624803325760] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11160.362005233765, \"sum\": 11160.362005233765, \"min\": 11160.362005233765}}, \"EndTime\": 1538408812.911781, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408801.751148}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:52 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.543171732 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:52 INFO 140624803325760] #progress_metric: host=algo-3, completed 40 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:53 INFO 139663579178816] Epoch[99] Batch[5] avg_epoch_loss=1.973091\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:53 INFO 139663579178816] Epoch[99] Batch [5]#011Speed: 123.08 samples/sec#011loss=1.973091\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:53 INFO 140196492334912] Epoch[97] Batch[5] avg_epoch_loss=2.110938\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:53 INFO 140196492334912] Epoch[97] Batch [5]#011Speed: 150.08 samples/sec#011loss=2.110938\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:55 INFO 140624803325760] Epoch[100] Batch[0] avg_epoch_loss=1.869024\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:55 INFO 140252856112960] Epoch[98] Batch[5] avg_epoch_loss=2.079000\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:55 INFO 140252856112960] Epoch[98] Batch [5]#011Speed: 150.44 samples/sec#011loss=2.079000\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:57 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10486.0680103302, \"sum\": 10486.0680103302, \"min\": 10486.0680103302}}, \"EndTime\": 1538408817.089411, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408806.603056}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.779400859 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:46:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:59 INFO 140624803325760] Epoch[100] Batch[5] avg_epoch_loss=2.100478\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:46:59 INFO 140624803325760] Epoch[100] Batch [5]#011Speed: 144.69 samples/sec#011loss=2.100478\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:46:58 INFO 139663579178816] Epoch[100] Batch[0] avg_epoch_loss=1.774816\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:58 INFO 140196492334912] Epoch[97] Batch[10] avg_epoch_loss=2.400229\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:58 INFO 140196492334912] Epoch[97] Batch [10]#011Speed: 114.80 samples/sec#011loss=2.747378\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:58 INFO 140196492334912] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12263.192892074585, \"sum\": 12263.192892074585, \"min\": 12263.192892074585}}, \"EndTime\": 1538408818.973068, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408806.709621}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:58 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.865507724 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:46:58 INFO 140196492334912] #progress_metric: host=algo-4, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:59 INFO 140252856112960] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10615.89002609253, \"sum\": 10615.89002609253, \"min\": 10615.89002609253}}, \"EndTime\": 1538408819.751936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408809.135807}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:59 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.768337182 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:46:59 INFO 140252856112960] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:00 INFO 140196492334912] Epoch[98] Batch[0] avg_epoch_loss=2.006873\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:02 INFO 140252856112960] Epoch[99] Batch[0] avg_epoch_loss=2.068220\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:03 INFO 140624803325760] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10680.98497390747, \"sum\": 10680.98497390747, \"min\": 10680.98497390747}}, \"EndTime\": 1538408823.593106, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408812.911868}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:03 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.62462345 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:03 INFO 140624803325760] #progress_metric: host=algo-3, completed 40 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:03 INFO 139663579178816] Epoch[100] Batch[5] avg_epoch_loss=1.944992\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:03 INFO 139663579178816] Epoch[100] Batch [5]#011Speed: 123.51 samples/sec#011loss=1.944992\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:06 INFO 140196492334912] Epoch[98] Batch[5] avg_epoch_loss=2.111871\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:06 INFO 140196492334912] Epoch[98] Batch [5]#011Speed: 116.27 samples/sec#011loss=2.111871\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:06 INFO 140624803325760] Epoch[101] Batch[0] avg_epoch_loss=1.924880\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:06 INFO 140252856112960] Epoch[99] Batch[5] avg_epoch_loss=2.064058\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:06 INFO 140252856112960] Epoch[99] Batch [5]#011Speed: 150.69 samples/sec#011loss=2.064058\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:08 INFO 139663579178816] Epoch[100] Batch[10] avg_epoch_loss=2.247980\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:08 INFO 139663579178816] Epoch[100] Batch [10]#011Speed: 152.98 samples/sec#011loss=2.611565\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:08 INFO 139663579178816] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10987.629890441895, \"sum\": 10987.629890441895, \"min\": 10987.629890441895}}, \"EndTime\": 1538408828.07734, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408817.089483}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:08 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.495792204 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:08 INFO 139663579178816] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:10 INFO 140196492334912] Epoch[98] Batch[10] avg_epoch_loss=2.034079\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:10 INFO 140196492334912] Epoch[98] Batch [10]#011Speed: 143.32 samples/sec#011loss=1.940729\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:10 INFO 140196492334912] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11598.989963531494, \"sum\": 11598.989963531494, \"min\": 11598.989963531494}}, \"EndTime\": 1538408830.572424, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408818.973158}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.198299428 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 39 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:10 INFO 139663579178816] Epoch[101] Batch[0] avg_epoch_loss=2.189559\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:10 INFO 140624803325760] Epoch[101] Batch[5] avg_epoch_loss=2.022142\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:10 INFO 140624803325760] Epoch[101] Batch [5]#011Speed: 150.88 samples/sec#011loss=2.022142\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:11 INFO 140252856112960] Epoch[99] Batch[10] avg_epoch_loss=1.791757\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:11 INFO 140252856112960] Epoch[99] Batch [10]#011Speed: 117.38 samples/sec#011loss=1.464997\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:11 INFO 140252856112960] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12136.101007461548, \"sum\": 12136.101007461548, \"min\": 12136.101007461548}}, \"EndTime\": 1538408831.888397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408819.752028}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:11 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.95240877 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:11 INFO 140252856112960] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:13 INFO 140196492334912] Epoch[99] Batch[0] avg_epoch_loss=2.117460\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:13 INFO 140252856112960] Epoch[100] Batch[0] avg_epoch_loss=2.145415\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:14 INFO 139663579178816] Epoch[101] Batch[5] avg_epoch_loss=2.189540\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:14 INFO 139663579178816] Epoch[101] Batch [5]#011Speed: 155.80 samples/sec#011loss=2.189540\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:15 INFO 140624803325760] Epoch[101] Batch[10] avg_epoch_loss=2.154803\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:15 INFO 140624803325760] Epoch[101] Batch [10]#011Speed: 116.37 samples/sec#011loss=2.313996\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:15 INFO 140624803325760] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12201.308012008667, \"sum\": 12201.308012008667, \"min\": 12201.308012008667}}, \"EndTime\": 1538408835.794758, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408823.593195}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:15 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=105.643319112 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:15 INFO 140624803325760] #progress_metric: host=algo-3, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:17 INFO 140196492334912] Epoch[99] Batch[5] avg_epoch_loss=2.211871\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:17 INFO 140196492334912] Epoch[99] Batch [5]#011Speed: 148.36 samples/sec#011loss=2.211871\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:17 INFO 140624803325760] Epoch[102] Batch[0] avg_epoch_loss=2.148083\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:18 INFO 139663579178816] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10164.751052856445, \"sum\": 10164.751052856445, \"min\": 10164.751052856445}}, \"EndTime\": 1538408838.242387, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408828.077409}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:18 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.496974682 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:18 INFO 139663579178816] #progress_metric: host=algo-2, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:18 INFO 140252856112960] Epoch[100] Batch[5] avg_epoch_loss=2.187392\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:18 INFO 140252856112960] Epoch[100] Batch [5]#011Speed: 119.13 samples/sec#011loss=2.187392\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:20 INFO 139663579178816] Epoch[102] Batch[0] avg_epoch_loss=2.054267\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:22 INFO 140624803325760] Epoch[102] Batch[5] avg_epoch_loss=2.059169\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:22 INFO 140624803325760] Epoch[102] Batch [5]#011Speed: 122.47 samples/sec#011loss=2.059169\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:22 INFO 140252856112960] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10841.38798713684, \"sum\": 10841.38798713684, \"min\": 10841.38798713684}}, \"EndTime\": 1538408842.730115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408831.888483}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:22 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.22005933 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:22 INFO 140252856112960] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:22 INFO 140196492334912] Epoch[99] Batch[10] avg_epoch_loss=2.398443\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:22 INFO 140196492334912] Epoch[99] Batch [10]#011Speed: 114.58 samples/sec#011loss=2.622329\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:22 INFO 140196492334912] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12331.284999847412, \"sum\": 12331.284999847412, \"min\": 12331.284999847412}}, \"EndTime\": 1538408842.904038, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408830.572506}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:22 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.935291852 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:22 INFO 140196492334912] #progress_metric: host=algo-4, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:24 INFO 140196492334912] Epoch[100] Batch[0] avg_epoch_loss=2.289440\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:24 INFO 140252856112960] Epoch[101] Batch[0] avg_epoch_loss=1.964315\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:24 INFO 139663579178816] Epoch[102] Batch[5] avg_epoch_loss=1.991659\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:24 INFO 139663579178816] Epoch[102] Batch [5]#011Speed: 156.49 samples/sec#011loss=1.991659\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:26 INFO 140624803325760] Epoch[102] Batch[10] avg_epoch_loss=2.306752\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:26 INFO 140624803325760] Epoch[102] Batch [10]#011Speed: 149.24 samples/sec#011loss=2.603852\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:26 INFO 140624803325760] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11173.925161361694, \"sum\": 11173.925161361694, \"min\": 11173.925161361694}}, \"EndTime\": 1538408846.969012, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408835.794844}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.846781216 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 41 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:47:29 INFO 140624803325760] Epoch[103] Batch[0] avg_epoch_loss=2.028792\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:29 INFO 140252856112960] Epoch[101] Batch[5] avg_epoch_loss=2.072435\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:29 INFO 140252856112960] Epoch[101] Batch [5]#011Speed: 120.59 samples/sec#011loss=2.072435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:30 INFO 139663579178816] Epoch[102] Batch[10] avg_epoch_loss=1.934070\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:30 INFO 139663579178816] Epoch[102] Batch [10]#011Speed: 116.68 samples/sec#011loss=1.864964\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:30 INFO 139663579178816] processed a total of 1340 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11921.005010604858, \"sum\": 11921.005010604858, \"min\": 11921.005010604858}}, \"EndTime\": 1538408850.163701, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408838.242462}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:30 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.405600417 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:30 INFO 139663579178816] #progress_metric: host=algo-2, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:30 INFO 140196492334912] Epoch[100] Batch[5] avg_epoch_loss=2.037934\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:30 INFO 140196492334912] Epoch[100] Batch [5]#011Speed: 116.02 samples/sec#011loss=2.037934\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:31 INFO 139663579178816] Epoch[103] Batch[0] avg_epoch_loss=1.855116\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:33 INFO 140624803325760] Epoch[103] Batch[5] avg_epoch_loss=2.113671\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:33 INFO 140624803325760] Epoch[103] Batch [5]#011Speed: 156.82 samples/sec#011loss=2.113671\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:33 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10733.886003494263, \"sum\": 10733.886003494263, \"min\": 10733.886003494263}}, \"EndTime\": 1538408853.464306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408842.730191}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:33 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.94301932 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:33 INFO 140252856112960] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:34 INFO 140196492334912] Epoch[100] Batch[10] avg_epoch_loss=2.004057\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:34 INFO 140196492334912] Epoch[100] Batch [10]#011Speed: 141.24 samples/sec#011loss=1.963403\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:34 INFO 140196492334912] processed a total of 1374 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11751.216888427734, \"sum\": 11751.216888427734, \"min\": 11751.216888427734}}, \"EndTime\": 1538408854.655574, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408842.904119}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:34 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.922806067 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:34 INFO 140196492334912] #progress_metric: host=algo-4, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:35 INFO 140252856112960] Epoch[102] Batch[0] avg_epoch_loss=1.907316\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:37 INFO 139663579178816] Epoch[103] Batch[5] avg_epoch_loss=2.019924\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:37 INFO 139663579178816] Epoch[103] Batch [5]#011Speed: 120.05 samples/sec#011loss=2.019924\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:37 INFO 140196492334912] Epoch[101] Batch[0] avg_epoch_loss=2.016176\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:41 INFO 139663579178816] Epoch[103] Batch[10] avg_epoch_loss=1.947097\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:41 INFO 139663579178816] Epoch[103] Batch [10]#011Speed: 147.69 samples/sec#011loss=1.859705\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:41 INFO 139663579178816] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11345.234155654907, \"sum\": 11345.234155654907, \"min\": 11345.234155654907}}, \"EndTime\": 1538408861.509222, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408850.16377}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.408285569 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:41 INFO 140196492334912] Epoch[101] Batch[5] avg_epoch_loss=1.982528\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:41 INFO 140196492334912] Epoch[101] Batch [5]#011Speed: 149.46 samples/sec#011loss=1.982528\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:37 INFO 140624803325760] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10175.897121429443, \"sum\": 10175.897121429443, \"min\": 10175.897121429443}}, \"EndTime\": 1538408857.145245, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408846.969097}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.508293375 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 41 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:39 INFO 140624803325760] Epoch[104] Batch[0] avg_epoch_loss=2.278429\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:40 INFO 140252856112960] Epoch[102] Batch[5] avg_epoch_loss=2.031256\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:40 INFO 140252856112960] Epoch[102] Batch [5]#011Speed: 120.28 samples/sec#011loss=2.031256\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:43 INFO 140624803325760] Epoch[104] Batch[5] avg_epoch_loss=1.934609\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:43 INFO 140624803325760] Epoch[104] Batch [5]#011Speed: 156.36 samples/sec#011loss=1.934609\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:43 INFO 139663579178816] Epoch[104] Batch[0] avg_epoch_loss=1.873798\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:44 INFO 140252856112960] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10768.379926681519, \"sum\": 10768.379926681519, \"min\": 10768.379926681519}}, \"EndTime\": 1538408864.23299, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408853.464381}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.72945965 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:45 INFO 140196492334912] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10585.123062133789, \"sum\": 10585.123062133789, \"min\": 10585.123062133789}}, \"EndTime\": 1538408865.241029, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408854.655658}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:45 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.521979672 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:45 INFO 140196492334912] #progress_metric: host=algo-4, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:45 INFO 140252856112960] Epoch[103] Batch[0] avg_epoch_loss=2.019449\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:47 INFO 140196492334912] Epoch[102] Batch[0] avg_epoch_loss=2.660131\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:48 INFO 139663579178816] Epoch[104] Batch[5] avg_epoch_loss=1.812115\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:48 INFO 139663579178816] Epoch[104] Batch [5]#011Speed: 149.80 samples/sec#011loss=1.812115\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:48 INFO 140624803325760] Epoch[104] Batch[10] avg_epoch_loss=1.679910\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:48 INFO 140624803325760] Epoch[104] Batch [10]#011Speed: 121.83 samples/sec#011loss=1.374272\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:48 INFO 140624803325760] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11737.586975097656, \"sum\": 11737.586975097656, \"min\": 11737.586975097656}}, \"EndTime\": 1538408868.883175, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408857.145332}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.41326583 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 42 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:50 INFO 140624803325760] Epoch[105] Batch[0] avg_epoch_loss=1.823557\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:51 INFO 140252856112960] Epoch[103] Batch[5] avg_epoch_loss=2.020236\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:51 INFO 140252856112960] Epoch[103] Batch [5]#011Speed: 119.61 samples/sec#011loss=2.020236\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:52 INFO 139663579178816] processed a total of 1221 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10571.52795791626, \"sum\": 10571.52795791626, \"min\": 10571.52795791626}}, \"EndTime\": 1538408872.081042, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408861.509289}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:52 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.497689886 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:52 INFO 139663579178816] #progress_metric: host=algo-2, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:51 INFO 140196492334912] Epoch[102] Batch[5] avg_epoch_loss=2.143061\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:51 INFO 140196492334912] Epoch[102] Batch [5]#011Speed: 149.88 samples/sec#011loss=2.143061\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:54 INFO 139663579178816] Epoch[105] Batch[0] avg_epoch_loss=1.992696\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:55 INFO 140624803325760] Epoch[105] Batch[5] avg_epoch_loss=2.082830\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:55 INFO 140624803325760] Epoch[105] Batch [5]#011Speed: 124.42 samples/sec#011loss=2.082830\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:55 INFO 140252856112960] Epoch[103] Batch[10] avg_epoch_loss=1.991576\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:55 INFO 140252856112960] Epoch[103] Batch [10]#011Speed: 148.22 samples/sec#011loss=1.957185\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:55 INFO 140252856112960] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11324.444055557251, \"sum\": 11324.444055557251, \"min\": 11324.444055557251}}, \"EndTime\": 1538408875.557738, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408864.233064}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:55 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.384367837 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:55 INFO 140252856112960] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:57 INFO 140196492334912] Epoch[102] Batch[10] avg_epoch_loss=1.823959\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:57 INFO 140196492334912] Epoch[102] Batch [10]#011Speed: 114.21 samples/sec#011loss=1.441037\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:57 INFO 140196492334912] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12266.085863113403, \"sum\": 12266.085863113403, \"min\": 12266.085863113403}}, \"EndTime\": 1538408877.507451, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408865.241115}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:57 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.514709007 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:57 INFO 140196492334912] #progress_metric: host=algo-4, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:47:57 INFO 140252856112960] Epoch[104] Batch[0] avg_epoch_loss=2.003465\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:47:59 INFO 140196492334912] Epoch[103] Batch[0] avg_epoch_loss=2.194982\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:59 INFO 140624803325760] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10477.542877197266, \"sum\": 10477.542877197266, \"min\": 10477.542877197266}}, \"EndTime\": 1538408879.361053, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408868.883259}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.869672136 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:47:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 42 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:47:58 INFO 139663579178816] Epoch[105] Batch[5] avg_epoch_loss=2.027792\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:47:58 INFO 139663579178816] Epoch[105] Batch [5]#011Speed: 150.15 samples/sec#011loss=2.027792\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:00 INFO 140624803325760] Epoch[106] Batch[0] avg_epoch_loss=1.719639\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:02 INFO 140252856112960] Epoch[104] Batch[5] avg_epoch_loss=1.998234\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:02 INFO 140252856112960] Epoch[104] Batch [5]#011Speed: 150.09 samples/sec#011loss=1.998234\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:04 INFO 139663579178816] Epoch[105] Batch[10] avg_epoch_loss=2.258264\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:04 INFO 139663579178816] Epoch[105] Batch [10]#011Speed: 116.20 samples/sec#011loss=2.534831\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:04 INFO 139663579178816] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12146.088123321533, \"sum\": 12146.088123321533, \"min\": 12146.088123321533}}, \"EndTime\": 1538408884.227433, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408872.081119}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:04 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=105.465152553 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:04 INFO 139663579178816] #progress_metric: host=algo-2, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:04 INFO 140196492334912] Epoch[103] Batch[5] avg_epoch_loss=2.082627\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:04 INFO 140196492334912] Epoch[103] Batch [5]#011Speed: 116.81 samples/sec#011loss=2.082627\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:05 INFO 139663579178816] Epoch[106] Batch[0] avg_epoch_loss=1.724430\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:06 INFO 140624803325760] Epoch[106] Batch[5] avg_epoch_loss=1.848556\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:06 INFO 140624803325760] Epoch[106] Batch [5]#011Speed: 124.00 samples/sec#011loss=1.848556\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:07 INFO 140252856112960] Epoch[104] Batch[10] avg_epoch_loss=2.198471\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:07 INFO 140252856112960] Epoch[104] Batch [10]#011Speed: 118.05 samples/sec#011loss=2.438755\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:07 INFO 140252856112960] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12084.823846817017, \"sum\": 12084.823846817017, \"min\": 12084.823846817017}}, \"EndTime\": 1538408887.642862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408875.557806}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:07 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.564821804 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:07 INFO 140252856112960] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:09 INFO 140624803325760] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10538.748979568481, \"sum\": 10538.748979568481, \"min\": 10538.748979568481}}, \"EndTime\": 1538408889.90016, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408879.36114}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:09 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.13393791 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:09 INFO 140624803325760] #progress_metric: host=algo-3, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:09 INFO 140252856112960] Epoch[105] Batch[0] avg_epoch_loss=1.958007\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:09 INFO 140196492334912] Epoch[103] Batch[10] avg_epoch_loss=1.991374\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:09 INFO 140196492334912] Epoch[103] Batch [10]#011Speed: 144.03 samples/sec#011loss=1.881869\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:09 INFO 140196492334912] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11564.260005950928, \"sum\": 11564.260005950928, \"min\": 11564.260005950928}}, \"EndTime\": 1538408889.072045, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408877.507539}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:09 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.624729329 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:09 INFO 140196492334912] #progress_metric: host=algo-4, completed 41 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:11 INFO 139663579178816] Epoch[106] Batch[5] avg_epoch_loss=1.884159\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:11 INFO 139663579178816] Epoch[106] Batch [5]#011Speed: 119.67 samples/sec#011loss=1.884159\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:11 INFO 140196492334912] Epoch[104] Batch[0] avg_epoch_loss=1.777650\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:11 INFO 140624803325760] Epoch[107] Batch[0] avg_epoch_loss=1.629096\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:14 INFO 140252856112960] Epoch[105] Batch[5] avg_epoch_loss=1.960111\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:14 INFO 140252856112960] Epoch[105] Batch [5]#011Speed: 119.24 samples/sec#011loss=1.960111\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:15 INFO 139663579178816] processed a total of 1199 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10921.504974365234, \"sum\": 10921.504974365234, \"min\": 10921.504974365234}}, \"EndTime\": 1538408895.149247, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408884.227503}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:15 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.782026627 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:15 INFO 139663579178816] #progress_metric: host=algo-2, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:15 INFO 140196492334912] Epoch[104] Batch[5] avg_epoch_loss=2.019777\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:15 INFO 140196492334912] Epoch[104] Batch [5]#011Speed: 149.86 samples/sec#011loss=2.019777\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:16 INFO 140624803325760] Epoch[107] Batch[5] avg_epoch_loss=1.939221\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:16 INFO 140624803325760] Epoch[107] Batch [5]#011Speed: 125.02 samples/sec#011loss=1.939221\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:16 INFO 139663579178816] Epoch[107] Batch[0] avg_epoch_loss=2.260668\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:18 INFO 140252856112960] Epoch[105] Batch[10] avg_epoch_loss=1.931191\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:18 INFO 140252856112960] Epoch[105] Batch [10]#011Speed: 147.54 samples/sec#011loss=1.896487\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:18 INFO 140252856112960] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11336.357831954956, \"sum\": 11336.357831954956, \"min\": 11336.357831954956}}, \"EndTime\": 1538408898.979525, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408887.642945}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:18 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.203532154 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:18 INFO 140252856112960] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:19 INFO 140196492334912] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10549.379825592041, \"sum\": 10549.379825592041, \"min\": 10549.379825592041}}, \"EndTime\": 1538408899.621759, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408889.072128}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.972212515 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 42 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:20 INFO 140624803325760] Epoch[107] Batch[10] avg_epoch_loss=2.036750\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:20 INFO 140624803325760] Epoch[107] Batch [10]#011Speed: 153.47 samples/sec#011loss=2.153784\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:20 INFO 140624803325760] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10925.852060317993, \"sum\": 10925.852060317993, \"min\": 10925.852060317993}}, \"EndTime\": 1538408900.826396, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408889.90025}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.250477985 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 43 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:48:21 INFO 140252856112960] Epoch[106] Batch[0] avg_epoch_loss=1.966357\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:21 INFO 139663579178816] Epoch[107] Batch[5] avg_epoch_loss=2.005453\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:21 INFO 139663579178816] Epoch[107] Batch [5]#011Speed: 123.30 samples/sec#011loss=2.005453\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:22 INFO 140196492334912] Epoch[105] Batch[0] avg_epoch_loss=2.184206\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:23 INFO 140624803325760] Epoch[108] Batch[0] avg_epoch_loss=2.174900\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:25 INFO 140252856112960] Epoch[106] Batch[5] avg_epoch_loss=2.114317\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:25 INFO 140252856112960] Epoch[106] Batch [5]#011Speed: 150.86 samples/sec#011loss=2.114317\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:26 INFO 139663579178816] Epoch[107] Batch[10] avg_epoch_loss=2.168096\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:26 INFO 139663579178816] Epoch[107] Batch [10]#011Speed: 150.42 samples/sec#011loss=2.363269\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:26 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11080.379962921143, \"sum\": 11080.379962921143, \"min\": 11080.379962921143}}, \"EndTime\": 1538408906.229981, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408895.149342}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:26 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.150191848 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:26 INFO 139663579178816] #progress_metric: host=algo-2, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:26 INFO 140196492334912] Epoch[105] Batch[5] avg_epoch_loss=1.966016\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:26 INFO 140196492334912] Epoch[105] Batch [5]#011Speed: 150.37 samples/sec#011loss=1.966016\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:27 INFO 140624803325760] Epoch[108] Batch[5] avg_epoch_loss=2.007800\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:27 INFO 140624803325760] Epoch[108] Batch [5]#011Speed: 146.13 samples/sec#011loss=2.007800\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:28 INFO 139663579178816] Epoch[108] Batch[0] avg_epoch_loss=2.060436\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:29 INFO 140252856112960] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10524.768114089966, \"sum\": 10524.768114089966, \"min\": 10524.768114089966}}, \"EndTime\": 1538408909.504583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408898.979593}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:29 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.04652249 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:29 INFO 140252856112960] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:30 INFO 140196492334912] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10604.11810874939, \"sum\": 10604.11810874939, \"min\": 10604.11810874939}}, \"EndTime\": 1538408910.226211, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408899.621849}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.480397526 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 42 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:31 INFO 140624803325760] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10552.814960479736, \"sum\": 10552.814960479736, \"min\": 10552.814960479736}}, \"EndTime\": 1538408911.379498, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408900.826468}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.208632633 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:31 INFO 140252856112960] Epoch[107] Batch[0] avg_epoch_loss=2.463719\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:32 INFO 140196492334912] Epoch[106] Batch[0] avg_epoch_loss=2.347578\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:32 INFO 139663579178816] Epoch[108] Batch[5] avg_epoch_loss=2.053898\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:32 INFO 139663579178816] Epoch[108] Batch [5]#011Speed: 154.03 samples/sec#011loss=2.053898\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:33 INFO 140624803325760] Epoch[109] Batch[0] avg_epoch_loss=2.135104\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:36 INFO 140252856112960] Epoch[107] Batch[5] avg_epoch_loss=2.183994\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:36 INFO 140252856112960] Epoch[107] Batch [5]#011Speed: 150.79 samples/sec#011loss=2.183994\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:36 INFO 140196492334912] Epoch[106] Batch[5] avg_epoch_loss=2.281746\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:36 INFO 140196492334912] Epoch[106] Batch [5]#011Speed: 149.28 samples/sec#011loss=2.281746\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:37 INFO 140624803325760] Epoch[109] Batch[5] avg_epoch_loss=2.097486\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:37 INFO 140624803325760] Epoch[109] Batch [5]#011Speed: 155.66 samples/sec#011loss=2.097486\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:37 INFO 139663579178816] Epoch[108] Batch[10] avg_epoch_loss=2.130230\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:37 INFO 139663579178816] Epoch[108] Batch [10]#011Speed: 121.86 samples/sec#011loss=2.221829\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:37 INFO 139663579178816] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11758.224964141846, \"sum\": 11758.224964141846, \"min\": 11758.224964141846}}, \"EndTime\": 1538408917.988496, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408906.230047}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:37 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.964605463 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:37 INFO 139663579178816] #progress_metric: host=algo-2, completed 43 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:39 INFO 139663579178816] Epoch[109] Batch[0] avg_epoch_loss=2.359445\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:40 INFO 140252856112960] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10530.924081802368, \"sum\": 10530.924081802368, \"min\": 10530.924081802368}}, \"EndTime\": 1538408920.035819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408909.504658}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.405984147 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:40 INFO 140196492334912] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10622.524976730347, \"sum\": 10622.524976730347, \"min\": 10622.524976730347}}, \"EndTime\": 1538408920.849069, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408910.226298}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:40 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.672996562 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:40 INFO 140196492334912] #progress_metric: host=algo-4, completed 42 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:41 INFO 140624803325760] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10205.497026443481, \"sum\": 10205.497026443481, \"min\": 10205.497026443481}}, \"EndTime\": 1538408921.585298, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408911.379574}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:41 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.853460357 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:41 INFO 140624803325760] #progress_metric: host=algo-3, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:42 INFO 140252856112960] Epoch[108] Batch[0] avg_epoch_loss=2.007228\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:43 INFO 140196492334912] Epoch[107] Batch[0] avg_epoch_loss=2.048529\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:44 INFO 140624803325760] Epoch[110] Batch[0] avg_epoch_loss=1.872340\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:44 INFO 139663579178816] Epoch[109] Batch[5] avg_epoch_loss=2.066230\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:44 INFO 139663579178816] Epoch[109] Batch [5]#011Speed: 122.53 samples/sec#011loss=2.066230\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:46 INFO 140252856112960] Epoch[108] Batch[5] avg_epoch_loss=2.056742\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:46 INFO 140252856112960] Epoch[108] Batch [5]#011Speed: 151.68 samples/sec#011loss=2.056742\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:47 INFO 140196492334912] Epoch[107] Batch[5] avg_epoch_loss=2.009880\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:47 INFO 140196492334912] Epoch[107] Batch [5]#011Speed: 150.69 samples/sec#011loss=2.009880\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:48 INFO 139663579178816] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10597.097873687744, \"sum\": 10597.097873687744, \"min\": 10597.097873687744}}, \"EndTime\": 1538408928.585901, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408917.988565}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:48 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.011986551 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:48 INFO 139663579178816] #progress_metric: host=algo-2, completed 44 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:48 INFO 140624803325760] Epoch[110] Batch[5] avg_epoch_loss=1.938532\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:48 INFO 140624803325760] Epoch[110] Batch [5]#011Speed: 151.61 samples/sec#011loss=1.938532\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:50 INFO 139663579178816] Epoch[110] Batch[0] avg_epoch_loss=1.839236\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:48:50 INFO 140252856112960] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10504.595041275024, \"sum\": 10504.595041275024, \"min\": 10504.595041275024}}, \"EndTime\": 1538408930.540737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408920.035898}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:50 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.802954575 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:50 INFO 140252856112960] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:52 INFO 140252856112960] Epoch[109] Batch[0] avg_epoch_loss=1.873394\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:52 INFO 140196492334912] Epoch[107] Batch[10] avg_epoch_loss=1.999584\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:52 INFO 140196492334912] Epoch[107] Batch [10]#011Speed: 117.77 samples/sec#011loss=1.987229\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:52 INFO 140196492334912] processed a total of 1346 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12123.126983642578, \"sum\": 12123.126983642578, \"min\": 12123.126983642578}}, \"EndTime\": 1538408932.972542, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408920.849158}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.026270307 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 43 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:53 INFO 140624803325760] Epoch[110] Batch[10] avg_epoch_loss=2.161498\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:53 INFO 140624803325760] Epoch[110] Batch [10]#011Speed: 119.53 samples/sec#011loss=2.429057\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:53 INFO 140624803325760] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12035.824060440063, \"sum\": 12035.824060440063, \"min\": 12035.824060440063}}, \"EndTime\": 1538408933.621426, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408921.585375}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.920896735 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:48:54 INFO 140196492334912] Epoch[108] Batch[0] avg_epoch_loss=2.026919\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:55 INFO 139663579178816] Epoch[110] Batch[5] avg_epoch_loss=1.998102\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:55 INFO 139663579178816] Epoch[110] Batch [5]#011Speed: 120.81 samples/sec#011loss=1.998102\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:48:55 INFO 140624803325760] Epoch[111] Batch[0] avg_epoch_loss=1.833108\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:57 INFO 140252856112960] Epoch[109] Batch[5] avg_epoch_loss=1.912714\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:48:57 INFO 140252856112960] Epoch[109] Batch [5]#011Speed: 147.89 samples/sec#011loss=1.912714\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:59 INFO 139663579178816] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10793.950080871582, \"sum\": 10793.950080871582, \"min\": 10793.950080871582}}, \"EndTime\": 1538408939.380153, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408928.585973}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:59 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.507466611 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:48:59 INFO 139663579178816] #progress_metric: host=algo-2, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:00 INFO 140196492334912] Epoch[108] Batch[5] avg_epoch_loss=1.898355\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:00 INFO 140196492334912] Epoch[108] Batch [5]#011Speed: 117.58 samples/sec#011loss=1.898355\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:04 INFO 140196492334912] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11036.712169647217, \"sum\": 11036.712169647217, \"min\": 11036.712169647217}}, \"EndTime\": 1538408944.009622, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408932.972628}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:04 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.884399253 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:04 INFO 140196492334912] #progress_metric: host=algo-4, completed 43 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:00 INFO 140624803325760] Epoch[111] Batch[5] avg_epoch_loss=1.973435\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:00 INFO 140624803325760] Epoch[111] Batch [5]#011Speed: 120.57 samples/sec#011loss=1.973435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:00 INFO 139663579178816] Epoch[111] Batch[0] avg_epoch_loss=2.182017\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:02 INFO 140252856112960] Epoch[109] Batch[10] avg_epoch_loss=2.153478\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:02 INFO 140252856112960] Epoch[109] Batch [10]#011Speed: 117.01 samples/sec#011loss=2.442395\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:02 INFO 140252856112960] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12189.38398361206, \"sum\": 12189.38398361206, \"min\": 12189.38398361206}}, \"EndTime\": 1538408942.730425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408930.540815}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=105.746740167 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:04 INFO 140624803325760] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10846.13013267517, \"sum\": 10846.13013267517, \"min\": 10846.13013267517}}, \"EndTime\": 1538408944.467847, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408933.621496}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:04 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.708330813 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:04 INFO 140624803325760] #progress_metric: host=algo-3, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:04 INFO 140252856112960] Epoch[110] Batch[0] avg_epoch_loss=2.088552\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:05 INFO 140196492334912] Epoch[109] Batch[0] avg_epoch_loss=2.021814\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:06 INFO 139663579178816] Epoch[111] Batch[5] avg_epoch_loss=2.074943\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:06 INFO 139663579178816] Epoch[111] Batch [5]#011Speed: 120.64 samples/sec#011loss=2.074943\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:06 INFO 140624803325760] Epoch[112] Batch[0] avg_epoch_loss=1.822546\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:09 INFO 140252856112960] Epoch[110] Batch[5] avg_epoch_loss=1.971226\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:09 INFO 140252856112960] Epoch[110] Batch [5]#011Speed: 118.16 samples/sec#011loss=1.971226\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:10 INFO 139663579178816] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10767.607927322388, \"sum\": 10767.607927322388, \"min\": 10767.607927322388}}, \"EndTime\": 1538408950.148061, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408939.380225}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.316663798 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:13 INFO 140252856112960] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11033.123016357422, \"sum\": 11033.123016357422, \"min\": 11033.123016357422}}, \"EndTime\": 1538408953.763923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408942.730508}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:13 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.565997977 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:13 INFO 140252856112960] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:11 INFO 140196492334912] Epoch[109] Batch[5] avg_epoch_loss=2.084715\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:11 INFO 140196492334912] Epoch[109] Batch [5]#011Speed: 116.05 samples/sec#011loss=2.084715\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:11 INFO 140624803325760] Epoch[112] Batch[5] avg_epoch_loss=1.925292\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:11 INFO 140624803325760] Epoch[112] Batch [5]#011Speed: 124.72 samples/sec#011loss=1.925292\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:11 INFO 139663579178816] Epoch[112] Batch[0] avg_epoch_loss=1.676546\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:14 INFO 140624803325760] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10458.926916122437, \"sum\": 10458.926916122437, \"min\": 10458.926916122437}}, \"EndTime\": 1538408954.927081, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408944.46792}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:14 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.079728147 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:14 INFO 140624803325760] #progress_metric: host=algo-3, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:15 INFO 140196492334912] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11119.877099990845, \"sum\": 11119.877099990845, \"min\": 11119.877099990845}}, \"EndTime\": 1538408955.129855, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408944.009725}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:15 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.859640799 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:15 INFO 140196492334912] #progress_metric: host=algo-4, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:15 INFO 140252856112960] Epoch[111] Batch[0] avg_epoch_loss=2.112828\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:16 INFO 140196492334912] Epoch[110] Batch[0] avg_epoch_loss=1.654936\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:16 INFO 140624803325760] Epoch[113] Batch[0] avg_epoch_loss=2.041772\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:17 INFO 139663579178816] Epoch[112] Batch[5] avg_epoch_loss=1.968444\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:17 INFO 139663579178816] Epoch[112] Batch [5]#011Speed: 119.90 samples/sec#011loss=1.968444\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:20 INFO 139663579178816] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10847.917079925537, \"sum\": 10847.917079925537, \"min\": 10847.917079925537}}, \"EndTime\": 1538408960.99629, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408950.148131}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:20 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.15017646 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:20 INFO 139663579178816] #progress_metric: host=algo-2, completed 45 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:49:20 INFO 140252856112960] Epoch[111] Batch[5] avg_epoch_loss=2.095072\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:20 INFO 140252856112960] Epoch[111] Batch [5]#011Speed: 118.50 samples/sec#011loss=2.095072\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:21 INFO 140624803325760] Epoch[113] Batch[5] avg_epoch_loss=1.925288\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:21 INFO 140624803325760] Epoch[113] Batch [5]#011Speed: 125.39 samples/sec#011loss=1.925288\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:22 INFO 139663579178816] Epoch[113] Batch[0] avg_epoch_loss=2.090133\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:22 INFO 140196492334912] Epoch[110] Batch[5] avg_epoch_loss=1.863022\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:22 INFO 140196492334912] Epoch[110] Batch [5]#011Speed: 114.17 samples/sec#011loss=1.863022\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:24 INFO 140252856112960] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10968.120098114014, \"sum\": 10968.120098114014, \"min\": 10968.120098114014}}, \"EndTime\": 1538408964.732358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408953.763997}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:24 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.965506766 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:24 INFO 140252856112960] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:26 INFO 140196492334912] Epoch[110] Batch[10] avg_epoch_loss=2.185268\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:26 INFO 140196492334912] Epoch[110] Batch [10]#011Speed: 141.88 samples/sec#011loss=2.571964\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:26 INFO 140196492334912] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11777.612924575806, \"sum\": 11777.612924575806, \"min\": 11777.612924575806}}, \"EndTime\": 1538408966.907818, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408955.129943}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:26 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.226807869 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:26 INFO 140196492334912] #progress_metric: host=algo-4, completed 44 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:26 INFO 140624803325760] Epoch[113] Batch[10] avg_epoch_loss=2.172693\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:26 INFO 140624803325760] Epoch[113] Batch [10]#011Speed: 144.18 samples/sec#011loss=2.469578\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:26 INFO 140624803325760] processed a total of 1340 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11229.344844818115, \"sum\": 11229.344844818115, \"min\": 11229.344844818115}}, \"EndTime\": 1538408966.156727, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408954.927155}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.329095842 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:26 INFO 140252856112960] Epoch[112] Batch[0] avg_epoch_loss=1.923124\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:27 INFO 139663579178816] Epoch[113] Batch[5] avg_epoch_loss=2.052532\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:27 INFO 139663579178816] Epoch[113] Batch [5]#011Speed: 121.59 samples/sec#011loss=2.052532\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:28 INFO 140624803325760] Epoch[114] Batch[0] avg_epoch_loss=2.214462\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:29 INFO 140196492334912] Epoch[111] Batch[0] avg_epoch_loss=2.143704\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:32 INFO 139663579178816] Epoch[113] Batch[10] avg_epoch_loss=1.801307\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:32 INFO 139663579178816] Epoch[113] Batch [10]#011Speed: 148.66 samples/sec#011loss=1.499836\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:32 INFO 139663579178816] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11203.2790184021, \"sum\": 11203.2790184021, \"min\": 11203.2790184021}}, \"EndTime\": 1538408972.199883, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408960.996363}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:32 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.232960174 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:32 INFO 139663579178816] #progress_metric: host=algo-2, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:33 INFO 140196492334912] Epoch[111] Batch[5] avg_epoch_loss=2.144969\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:33 INFO 140196492334912] Epoch[111] Batch [5]#011Speed: 148.65 samples/sec#011loss=2.144969\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:31 INFO 140252856112960] Epoch[112] Batch[5] avg_epoch_loss=2.136289\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:31 INFO 140252856112960] Epoch[112] Batch [5]#011Speed: 117.48 samples/sec#011loss=2.136289\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:32 INFO 140624803325760] Epoch[114] Batch[5] avg_epoch_loss=2.292483\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:32 INFO 140624803325760] Epoch[114] Batch [5]#011Speed: 150.45 samples/sec#011loss=2.292483\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:34 INFO 139663579178816] Epoch[114] Batch[0] avg_epoch_loss=2.206040\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:35 INFO 140252856112960] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10975.253105163574, \"sum\": 10975.253105163574, \"min\": 10975.253105163574}}, \"EndTime\": 1538408975.707915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408964.732434}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:35 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.893956072 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:35 INFO 140252856112960] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:36 INFO 140624803325760] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10543.352127075195, \"sum\": 10543.352127075195, \"min\": 10543.352127075195}}, \"EndTime\": 1538408976.700379, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408966.156796}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:36 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.074343677 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:36 INFO 140624803325760] #progress_metric: host=algo-3, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:37 INFO 140196492334912] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10707.684993743896, \"sum\": 10707.684993743896, \"min\": 10707.684993743896}}, \"EndTime\": 1538408977.61583, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408966.907898}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:37 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.044559459 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:37 INFO 140196492334912] #progress_metric: host=algo-4, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:37 INFO 140252856112960] Epoch[113] Batch[0] avg_epoch_loss=1.992866\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:39 INFO 140624803325760] Epoch[115] Batch[0] avg_epoch_loss=2.020015\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:38 INFO 139663579178816] Epoch[114] Batch[5] avg_epoch_loss=2.065161\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:38 INFO 139663579178816] Epoch[114] Batch [5]#011Speed: 153.46 samples/sec#011loss=2.065161\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:39 INFO 140196492334912] Epoch[112] Batch[0] avg_epoch_loss=1.929586\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:42 INFO 140252856112960] Epoch[113] Batch[5] avg_epoch_loss=2.036237\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:42 INFO 140252856112960] Epoch[113] Batch [5]#011Speed: 118.67 samples/sec#011loss=2.036237\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:43 INFO 140624803325760] Epoch[115] Batch[5] avg_epoch_loss=1.951048\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:43 INFO 140624803325760] Epoch[115] Batch [5]#011Speed: 150.90 samples/sec#011loss=1.951048\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:44 INFO 139663579178816] Epoch[114] Batch[10] avg_epoch_loss=1.803441\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:44 INFO 139663579178816] Epoch[114] Batch [10]#011Speed: 120.54 samples/sec#011loss=1.489377\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:44 INFO 139663579178816] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11877.946138381958, \"sum\": 11877.946138381958, \"min\": 11877.946138381958}}, \"EndTime\": 1538408984.078123, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408972.199951}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.708415585 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:44 INFO 140196492334912] Epoch[112] Batch[5] avg_epoch_loss=1.948365\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:44 INFO 140196492334912] Epoch[112] Batch [5]#011Speed: 147.14 samples/sec#011loss=1.948365\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:45 INFO 139663579178816] Epoch[115] Batch[0] avg_epoch_loss=2.049717\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:46 INFO 140252856112960] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10889.824867248535, \"sum\": 10889.824867248535, \"min\": 10889.824867248535}}, \"EndTime\": 1538408986.598007, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408975.70797}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.43788994 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:47 INFO 140624803325760] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10585.95895767212, \"sum\": 10585.95895767212, \"min\": 10585.95895767212}}, \"EndTime\": 1538408987.286657, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408976.700457}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:47 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.630019983 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:47 INFO 140624803325760] #progress_metric: host=algo-3, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:48 INFO 140196492334912] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10690.890789031982, \"sum\": 10690.890789031982, \"min\": 10690.890789031982}}, \"EndTime\": 1538408988.307072, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408977.615922}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:48 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.668792076 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:48 INFO 140196492334912] #progress_metric: host=algo-4, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:48 INFO 140252856112960] Epoch[114] Batch[0] avg_epoch_loss=1.758698\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:49 INFO 140624803325760] Epoch[116] Batch[0] avg_epoch_loss=1.997868\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:50 INFO 140196492334912] Epoch[113] Batch[0] avg_epoch_loss=2.386645\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:51 INFO 139663579178816] Epoch[115] Batch[5] avg_epoch_loss=2.066563\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:51 INFO 139663579178816] Epoch[115] Batch [5]#011Speed: 121.10 samples/sec#011loss=2.066563\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:53 INFO 140624803325760] Epoch[116] Batch[5] avg_epoch_loss=2.101472\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:53 INFO 140624803325760] Epoch[116] Batch [5]#011Speed: 151.29 samples/sec#011loss=2.101472\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:55 INFO 139663579178816] Epoch[115] Batch[10] avg_epoch_loss=2.079888\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:55 INFO 139663579178816] Epoch[115] Batch [10]#011Speed: 148.54 samples/sec#011loss=2.095877\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:55 INFO 139663579178816] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11237.169027328491, \"sum\": 11237.169027328491, \"min\": 11237.169027328491}}, \"EndTime\": 1538408995.315604, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408984.07819}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:55 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.351647782 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:55 INFO 139663579178816] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:49:53 INFO 140252856112960] Epoch[114] Batch[5] avg_epoch_loss=2.027953\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:53 INFO 140252856112960] Epoch[114] Batch [5]#011Speed: 119.48 samples/sec#011loss=2.027953\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:54 INFO 140196492334912] Epoch[113] Batch[5] avg_epoch_loss=2.118130\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:49:55 INFO 140196492334912] Epoch[113] Batch [5]#011Speed: 149.13 samples/sec#011loss=2.118130\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:57 INFO 140624803325760] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10444.566011428833, \"sum\": 10444.566011428833, \"min\": 10444.566011428833}}, \"EndTime\": 1538408997.731558, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408987.286742}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:57 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.07145612 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:49:57 INFO 140624803325760] #progress_metric: host=algo-3, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:57 INFO 140252856112960] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10783.957958221436, \"sum\": 10783.957958221436, \"min\": 10783.957958221436}}, \"EndTime\": 1538408997.382275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408986.598078}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.819014272 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:49:57 INFO 139663579178816] Epoch[116] Batch[0] avg_epoch_loss=1.982565\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:49:58 INFO 140252856112960] Epoch[115] Batch[0] avg_epoch_loss=1.854839\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:00 INFO 140196492334912] Epoch[113] Batch[10] avg_epoch_loss=2.099012\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:00 INFO 140196492334912] Epoch[113] Batch [10]#011Speed: 112.93 samples/sec#011loss=2.076071\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:00 INFO 140196492334912] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12365.181922912598, \"sum\": 12365.181922912598, \"min\": 12365.181922912598}}, \"EndTime\": 1538409000.672612, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408988.307162}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:00 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=103.596111125 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:00 INFO 140196492334912] #progress_metric: host=algo-4, completed 45 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:00 INFO 140624803325760] Epoch[117] Batch[0] avg_epoch_loss=1.992070\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:01 INFO 139663579178816] Epoch[116] Batch[5] avg_epoch_loss=2.108054\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:02 INFO 139663579178816] Epoch[116] Batch [5]#011Speed: 149.44 samples/sec#011loss=2.108054\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:02 INFO 140196492334912] Epoch[114] Batch[0] avg_epoch_loss=2.278521\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:04 INFO 140624803325760] Epoch[117] Batch[5] avg_epoch_loss=2.082034\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:04 INFO 140624803325760] Epoch[117] Batch [5]#011Speed: 149.94 samples/sec#011loss=2.082034\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:04 INFO 140252856112960] Epoch[115] Batch[5] avg_epoch_loss=2.070242\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:04 INFO 140252856112960] Epoch[115] Batch [5]#011Speed: 118.73 samples/sec#011loss=2.070242\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:05 INFO 139663579178816] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10587.393045425415, \"sum\": 10587.393045425415, \"min\": 10587.393045425415}}, \"EndTime\": 1538409005.903287, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408995.315669}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.952751808 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:07 INFO 140196492334912] Epoch[114] Batch[5] avg_epoch_loss=2.055744\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:07 INFO 140196492334912] Epoch[114] Batch [5]#011Speed: 116.43 samples/sec#011loss=2.055744\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:08 INFO 139663579178816] Epoch[117] Batch[0] avg_epoch_loss=2.009986\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:08 INFO 140252856112960] processed a total of 1212 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10930.65595626831, \"sum\": 10930.65595626831, \"min\": 10930.65595626831}}, \"EndTime\": 1538409008.313246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408997.382349}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:08 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.879918829 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:08 INFO 140252856112960] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:09 INFO 140624803325760] Epoch[117] Batch[10] avg_epoch_loss=2.078810\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:09 INFO 140624803325760] Epoch[117] Batch [10]#011Speed: 117.24 samples/sec#011loss=2.074943\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:09 INFO 140624803325760] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12197.643041610718, \"sum\": 12197.643041610718, \"min\": 12197.643041610718}}, \"EndTime\": 1538409009.929551, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538408997.731649}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:09 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=106.986778308 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:09 INFO 140624803325760] #progress_metric: host=algo-3, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:09 INFO 140252856112960] Epoch[116] Batch[0] avg_epoch_loss=2.270108\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:11 INFO 140196492334912] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11110.634088516235, \"sum\": 11110.634088516235, \"min\": 11110.634088516235}}, \"EndTime\": 1538409011.783591, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409000.672702}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:11 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.22354898 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:11 INFO 140196492334912] #progress_metric: host=algo-4, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:11 INFO 140624803325760] Epoch[118] Batch[0] avg_epoch_loss=2.327547\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:12 INFO 139663579178816] Epoch[117] Batch[5] avg_epoch_loss=2.160830\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:12 INFO 139663579178816] Epoch[117] Batch [5]#011Speed: 152.70 samples/sec#011loss=2.160830\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:13 INFO 140196492334912] Epoch[115] Batch[0] avg_epoch_loss=2.095865\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:15 INFO 140252856112960] Epoch[116] Batch[5] avg_epoch_loss=2.189331\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:15 INFO 140252856112960] Epoch[116] Batch [5]#011Speed: 118.83 samples/sec#011loss=2.189331\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:16 INFO 140624803325760] Epoch[118] Batch[5] avg_epoch_loss=1.952143\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:16 INFO 140624803325760] Epoch[118] Batch [5]#011Speed: 121.08 samples/sec#011loss=1.952143\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:17 INFO 139663579178816] Epoch[117] Batch[10] avg_epoch_loss=2.313275\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:17 INFO 139663579178816] Epoch[117] Batch [10]#011Speed: 117.06 samples/sec#011loss=2.496209\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:17 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12082.839965820312, \"sum\": 12082.839965820312, \"min\": 12082.839965820312}}, \"EndTime\": 1538409017.986426, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409005.903362}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=106.099971073 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:18 INFO 140196492334912] Epoch[115] Batch[5] avg_epoch_loss=2.066491\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:18 INFO 140196492334912] Epoch[115] Batch [5]#011Speed: 116.71 samples/sec#011loss=2.066491\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:19 INFO 139663579178816] Epoch[118] Batch[0] avg_epoch_loss=2.421974\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:19 INFO 140252856112960] Epoch[116] Batch[10] avg_epoch_loss=2.180932\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:19 INFO 140252856112960] Epoch[116] Batch [10]#011Speed: 144.40 samples/sec#011loss=2.170853\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:19 INFO 140252856112960] processed a total of 1376 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11479.157209396362, \"sum\": 11479.157209396362, \"min\": 11479.157209396362}}, \"EndTime\": 1538409019.792656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409008.313302}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:19 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.868341772 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:19 INFO 140252856112960] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:20 INFO 140624803325760] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10710.346937179565, \"sum\": 10710.346937179565, \"min\": 10710.346937179565}}, \"EndTime\": 1538409020.640233, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409009.929635}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.295130925 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:22 INFO 140196492334912] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11015.645980834961, \"sum\": 11015.645980834961, \"min\": 11015.645980834961}}, \"EndTime\": 1538409022.799595, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409011.783673}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:22 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.384305077 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:22 INFO 140196492334912] #progress_metric: host=algo-4, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:22 INFO 140624803325760] Epoch[119] Batch[0] avg_epoch_loss=2.242280\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:22 INFO 140252856112960] Epoch[117] Batch[0] avg_epoch_loss=2.237376\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:26 INFO 140252856112960] Epoch[117] Batch[5] avg_epoch_loss=1.999877\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:26 INFO 140252856112960] Epoch[117] Batch [5]#011Speed: 151.28 samples/sec#011loss=1.999877\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:50:24 INFO 140196492334912] Epoch[116] Batch[0] avg_epoch_loss=1.819335\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:24 INFO 139663579178816] Epoch[118] Batch[5] avg_epoch_loss=2.103609\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:24 INFO 139663579178816] Epoch[118] Batch [5]#011Speed: 123.51 samples/sec#011loss=2.103609\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:27 INFO 140624803325760] Epoch[119] Batch[5] avg_epoch_loss=1.968817\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:27 INFO 140624803325760] Epoch[119] Batch [5]#011Speed: 119.30 samples/sec#011loss=1.968817\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:28 INFO 139663579178816] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10522.192001342773, \"sum\": 10522.192001342773, \"min\": 10522.192001342773}}, \"EndTime\": 1538409028.508925, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409017.986497}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:28 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.506008468 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:28 INFO 139663579178816] #progress_metric: host=algo-2, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:29 INFO 140196492334912] Epoch[116] Batch[5] avg_epoch_loss=2.027917\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:29 INFO 140196492334912] Epoch[116] Batch [5]#011Speed: 116.24 samples/sec#011loss=2.027917\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:30 INFO 139663579178816] Epoch[119] Batch[0] avg_epoch_loss=1.987432\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:30 INFO 140252856112960] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10456.8350315094, \"sum\": 10456.8350315094, \"min\": 10456.8350315094}}, \"EndTime\": 1538409030.249784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409019.792725}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.546005174 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:32 INFO 140624803325760] Epoch[119] Batch[10] avg_epoch_loss=2.188634\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:32 INFO 140624803325760] Epoch[119] Batch [10]#011Speed: 145.94 samples/sec#011loss=2.452416\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:32 INFO 140624803325760] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11419.540882110596, \"sum\": 11419.540882110596, \"min\": 11419.540882110596}}, \"EndTime\": 1538409032.060132, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409020.640343}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:32 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.465733045 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:32 INFO 140624803325760] #progress_metric: host=algo-3, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:32 INFO 140252856112960] Epoch[118] Batch[0] avg_epoch_loss=2.146755\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:33 INFO 140196492334912] processed a total of 1208 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11075.281143188477, \"sum\": 11075.281143188477, \"min\": 11075.281143188477}}, \"EndTime\": 1538409033.875222, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409022.799681}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:33 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.070455368 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:33 INFO 140196492334912] #progress_metric: host=algo-4, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:34 INFO 140624803325760] Epoch[120] Batch[0] avg_epoch_loss=1.905183\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:35 INFO 139663579178816] Epoch[119] Batch[5] avg_epoch_loss=1.923524\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:35 INFO 139663579178816] Epoch[119] Batch [5]#011Speed: 123.91 samples/sec#011loss=1.923524\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:35 INFO 140196492334912] Epoch[117] Batch[0] avg_epoch_loss=2.503171\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:36 INFO 140252856112960] Epoch[118] Batch[5] avg_epoch_loss=2.076645\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:36 INFO 140252856112960] Epoch[118] Batch [5]#011Speed: 151.32 samples/sec#011loss=2.076645\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:38 INFO 140624803325760] Epoch[120] Batch[5] avg_epoch_loss=2.178187\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:38 INFO 140624803325760] Epoch[120] Batch [5]#011Speed: 149.69 samples/sec#011loss=2.178187\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:39 INFO 139663579178816] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10514.90306854248, \"sum\": 10514.90306854248, \"min\": 10514.90306854248}}, \"EndTime\": 1538409039.02414, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409028.508997}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:39 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.260942796 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:39 INFO 139663579178816] #progress_metric: host=algo-2, completed 48 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:40 INFO 139663579178816] Epoch[120] Batch[0] avg_epoch_loss=2.484567\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:40 INFO 140252856112960] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10439.334154129028, \"sum\": 10439.334154129028, \"min\": 10439.334154129028}}, \"EndTime\": 1538409040.689436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409030.24986}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.941337056 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:41 INFO 140196492334912] Epoch[117] Batch[5] avg_epoch_loss=2.223279\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:41 INFO 140196492334912] Epoch[117] Batch [5]#011Speed: 115.41 samples/sec#011loss=2.223279\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:43 INFO 140252856112960] Epoch[119] Batch[0] avg_epoch_loss=2.101281\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:45 INFO 140196492334912] Epoch[117] Batch[10] avg_epoch_loss=2.244502\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:45 INFO 140196492334912] Epoch[117] Batch [10]#011Speed: 142.80 samples/sec#011loss=2.269969\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:45 INFO 140196492334912] processed a total of 1359 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11738.95001411438, \"sum\": 11738.95001411438, \"min\": 11738.95001411438}}, \"EndTime\": 1538409045.614513, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409033.875307}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:45 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.767235044 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:45 INFO 140196492334912] #progress_metric: host=algo-4, completed 47 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:43 INFO 140624803325760] Epoch[120] Batch[10] avg_epoch_loss=2.339641\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:43 INFO 140624803325760] Epoch[120] Batch [10]#011Speed: 121.68 samples/sec#011loss=2.533387\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:43 INFO 140624803325760] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11910.607099533081, \"sum\": 11910.607099533081, \"min\": 11910.607099533081}}, \"EndTime\": 1538409043.971118, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409032.060217}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.984964723 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:45 INFO 140624803325760] Epoch[121] Batch[0] avg_epoch_loss=2.046863\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:45 INFO 139663579178816] Epoch[120] Batch[5] avg_epoch_loss=2.151766\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:45 INFO 139663579178816] Epoch[120] Batch [5]#011Speed: 123.28 samples/sec#011loss=2.151766\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:47 INFO 140252856112960] Epoch[119] Batch[5] avg_epoch_loss=2.017114\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:47 INFO 140252856112960] Epoch[119] Batch [5]#011Speed: 153.49 samples/sec#011loss=2.017114\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:48 INFO 140196492334912] Epoch[118] Batch[0] avg_epoch_loss=2.171248\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:50 INFO 139663579178816] Epoch[120] Batch[10] avg_epoch_loss=2.160358\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:50 INFO 139663579178816] Epoch[120] Batch [10]#011Speed: 141.89 samples/sec#011loss=2.170668\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:50 INFO 139663579178816] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11306.944131851196, \"sum\": 11306.944131851196, \"min\": 11306.944131851196}}, \"EndTime\": 1538409050.331387, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409039.024213}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:50 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.645961606 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:50 INFO 139663579178816] #progress_metric: host=algo-2, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:50 INFO 140624803325760] Epoch[121] Batch[5] avg_epoch_loss=2.093713\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:50 INFO 140624803325760] Epoch[121] Batch [5]#011Speed: 122.06 samples/sec#011loss=2.093713\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:51 INFO 140252856112960] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10345.8251953125, \"sum\": 10345.8251953125, \"min\": 10345.8251953125}}, \"EndTime\": 1538409051.035576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409040.689513}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.113988684 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:52 INFO 140196492334912] Epoch[118] Batch[5] avg_epoch_loss=2.115358\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:52 INFO 140196492334912] Epoch[118] Batch [5]#011Speed: 153.90 samples/sec#011loss=2.115358\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:52 INFO 139663579178816] Epoch[121] Batch[0] avg_epoch_loss=2.197173\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:55 INFO 140624803325760] Epoch[121] Batch[10] avg_epoch_loss=2.234541\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:55 INFO 140624803325760] Epoch[121] Batch [10]#011Speed: 149.72 samples/sec#011loss=2.403534\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:55 INFO 140624803325760] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11133.316993713379, \"sum\": 11133.316993713379, \"min\": 11133.316993713379}}, \"EndTime\": 1538409055.104746, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409043.971188}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:55 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.370318131 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:55 INFO 140624803325760] #progress_metric: host=algo-3, completed 48 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:56 INFO 139663579178816] Epoch[121] Batch[5] avg_epoch_loss=2.009404\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:50:56 INFO 139663579178816] Epoch[121] Batch [5]#011Speed: 151.37 samples/sec#011loss=2.009404\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:50:53 INFO 140252856112960] Epoch[120] Batch[0] avg_epoch_loss=2.157022\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:56 INFO 140196492334912] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10438.966035842896, \"sum\": 10438.966035842896, \"min\": 10438.966035842896}}, \"EndTime\": 1538409056.053811, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409045.614594}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:56 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.604266338 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:56 INFO 140196492334912] #progress_metric: host=algo-4, completed 47 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:50:57 INFO 140624803325760] Epoch[122] Batch[0] avg_epoch_loss=2.190769\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:57 INFO 140252856112960] Epoch[120] Batch[5] avg_epoch_loss=2.121427\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:50:57 INFO 140252856112960] Epoch[120] Batch [5]#011Speed: 154.10 samples/sec#011loss=2.121427\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:50:58 INFO 140196492334912] Epoch[119] Batch[0] avg_epoch_loss=2.115081\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:01 INFO 140624803325760] Epoch[122] Batch[5] avg_epoch_loss=2.212165\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:01 INFO 140624803325760] Epoch[122] Batch [5]#011Speed: 152.94 samples/sec#011loss=2.212165\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:02 INFO 139663579178816] Epoch[121] Batch[10] avg_epoch_loss=2.010709\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:02 INFO 139663579178816] Epoch[121] Batch [10]#011Speed: 120.83 samples/sec#011loss=2.012276\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:02 INFO 139663579178816] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11879.11605834961, \"sum\": 11879.11605834961, \"min\": 11879.11605834961}}, \"EndTime\": 1538409062.210801, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409050.331455}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:02 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.444737911 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:02 INFO 139663579178816] #progress_metric: host=algo-2, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:02 INFO 140196492334912] Epoch[119] Batch[5] avg_epoch_loss=2.071768\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:02 INFO 140196492334912] Epoch[119] Batch [5]#011Speed: 153.64 samples/sec#011loss=2.071768\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:02 INFO 140252856112960] Epoch[120] Batch[10] avg_epoch_loss=1.842632\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:02 INFO 140252856112960] Epoch[120] Batch [10]#011Speed: 118.52 samples/sec#011loss=1.508079\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:02 INFO 140252856112960] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11914.454936981201, \"sum\": 11914.454936981201, \"min\": 11914.454936981201}}, \"EndTime\": 1538409062.95034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409051.035651}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.270643182 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:03 INFO 139663579178816] Epoch[122] Batch[0] avg_epoch_loss=2.379886\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:04 INFO 140252856112960] Epoch[121] Batch[0] avg_epoch_loss=2.181400\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:06 INFO 140196492334912] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10338.523864746094, \"sum\": 10338.523864746094, \"min\": 10338.523864746094}}, \"EndTime\": 1538409066.392689, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409056.053901}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.80872622 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:05 INFO 140624803325760] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10516.15595817566, \"sum\": 10516.15595817566, \"min\": 10516.15595817566}}, \"EndTime\": 1538409065.621201, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409055.104816}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:05 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.768382454 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:05 INFO 140624803325760] #progress_metric: host=algo-3, completed 49 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:07 INFO 140624803325760] Epoch[123] Batch[0] avg_epoch_loss=2.561877\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:08 INFO 140196492334912] Epoch[120] Batch[0] avg_epoch_loss=1.958588\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:09 INFO 139663579178816] Epoch[122] Batch[5] avg_epoch_loss=2.164511\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:09 INFO 139663579178816] Epoch[122] Batch [5]#011Speed: 120.38 samples/sec#011loss=2.164511\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:09 INFO 140252856112960] Epoch[121] Batch[5] avg_epoch_loss=2.191167\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:09 INFO 140252856112960] Epoch[121] Batch [5]#011Speed: 122.06 samples/sec#011loss=2.191167\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:11 INFO 140624803325760] Epoch[123] Batch[5] avg_epoch_loss=2.196155\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:11 INFO 140624803325760] Epoch[123] Batch [5]#011Speed: 159.21 samples/sec#011loss=2.196155\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:12 INFO 140196492334912] Epoch[120] Batch[5] avg_epoch_loss=2.016060\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:12 INFO 140196492334912] Epoch[120] Batch [5]#011Speed: 153.14 samples/sec#011loss=2.016060\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:13 INFO 139663579178816] Epoch[122] Batch[10] avg_epoch_loss=1.943929\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:13 INFO 139663579178816] Epoch[122] Batch [10]#011Speed: 147.13 samples/sec#011loss=1.679230\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:13 INFO 139663579178816] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11382.61604309082, \"sum\": 11382.61604309082, \"min\": 11382.61604309082}}, \"EndTime\": 1538409073.593767, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409062.21089}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.843397959 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:14 INFO 140252856112960] Epoch[121] Batch[10] avg_epoch_loss=2.056152\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:14 INFO 140252856112960] Epoch[121] Batch [10]#011Speed: 147.92 samples/sec#011loss=1.894133\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:14 INFO 140252856112960] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11266.450881958008, \"sum\": 11266.450881958008, \"min\": 11266.450881958008}}, \"EndTime\": 1538409074.217127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409062.950431}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.142908118 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:16 INFO 140252856112960] Epoch[122] Batch[0] avg_epoch_loss=2.084641\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:15 INFO 139663579178816] Epoch[123] Batch[0] avg_epoch_loss=2.062294\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:16 INFO 140196492334912] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10406.715869903564, \"sum\": 10406.715869903564, \"min\": 10406.715869903564}}, \"EndTime\": 1538409076.799753, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409066.392777}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:16 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.095327046 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:16 INFO 140196492334912] #progress_metric: host=algo-4, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:17 INFO 140624803325760] Epoch[123] Batch[10] avg_epoch_loss=2.094140\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:17 INFO 140624803325760] Epoch[123] Batch [10]#011Speed: 125.01 samples/sec#011loss=1.971721\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:17 INFO 140624803325760] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11430.954933166504, \"sum\": 11430.954933166504, \"min\": 11430.954933166504}}, \"EndTime\": 1538409077.052463, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409065.621276}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:17 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.524465586 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:17 INFO 140624803325760] #progress_metric: host=algo-3, completed 49 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:18 INFO 140624803325760] Epoch[124] Batch[0] avg_epoch_loss=1.920980\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:19 INFO 140196492334912] Epoch[121] Batch[0] avg_epoch_loss=2.338665\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:20 INFO 139663579178816] Epoch[123] Batch[5] avg_epoch_loss=2.077963\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:20 INFO 139663579178816] Epoch[123] Batch [5]#011Speed: 151.25 samples/sec#011loss=2.077963\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:21 INFO 140252856112960] Epoch[122] Batch[5] avg_epoch_loss=1.991387\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:21 INFO 140252856112960] Epoch[122] Batch [5]#011Speed: 147.77 samples/sec#011loss=1.991387\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:23 INFO 140196492334912] Epoch[121] Batch[5] avg_epoch_loss=2.065933\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:23 INFO 140196492334912] Epoch[121] Batch [5]#011Speed: 153.21 samples/sec#011loss=2.065933\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:23 INFO 140624803325760] Epoch[124] Batch[5] avg_epoch_loss=2.013027\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:23 INFO 140624803325760] Epoch[124] Batch [5]#011Speed: 126.36 samples/sec#011loss=2.013027\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:24 INFO 139663579178816] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10472.090005874634, \"sum\": 10472.090005874634, \"min\": 10472.090005874634}}, \"EndTime\": 1538409084.066212, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409073.593867}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:24 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.037092368 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:24 INFO 139663579178816] #progress_metric: host=algo-2, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:25 INFO 140252856112960] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10840.364933013916, \"sum\": 10840.364933013916, \"min\": 10840.364933013916}}, \"EndTime\": 1538409085.057833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409074.217212}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.954039513 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:26 INFO 139663579178816] Epoch[124] Batch[0] avg_epoch_loss=2.070134\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:27 INFO 140196492334912] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10392.22002029419, \"sum\": 10392.22002029419, \"min\": 10392.22002029419}}, \"EndTime\": 1538409087.192313, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409076.799838}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:27 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.628126838 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:27 INFO 140196492334912] #progress_metric: host=algo-4, completed 48 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:27 INFO 140624803325760] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10285.70818901062, \"sum\": 10285.70818901062, \"min\": 10285.70818901062}}, \"EndTime\": 1538409087.338481, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409077.052549}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:27 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.373754782 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:27 INFO 140624803325760] #progress_metric: host=algo-3, completed 50 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:51:27 INFO 140252856112960] Epoch[123] Batch[0] avg_epoch_loss=2.312400\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:28 INFO 140624803325760] Epoch[125] Batch[0] avg_epoch_loss=2.222854\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:29 INFO 140196492334912] Epoch[122] Batch[0] avg_epoch_loss=1.995797\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:30 INFO 139663579178816] Epoch[124] Batch[5] avg_epoch_loss=2.005959\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:30 INFO 139663579178816] Epoch[124] Batch [5]#011Speed: 154.57 samples/sec#011loss=2.005959\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:31 INFO 140252856112960] Epoch[123] Batch[5] avg_epoch_loss=2.057609\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:31 INFO 140252856112960] Epoch[123] Batch [5]#011Speed: 150.01 samples/sec#011loss=2.057609\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:33 INFO 140196492334912] Epoch[122] Batch[5] avg_epoch_loss=1.832019\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:33 INFO 140196492334912] Epoch[122] Batch [5]#011Speed: 153.23 samples/sec#011loss=1.832019\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:33 INFO 140624803325760] Epoch[125] Batch[5] avg_epoch_loss=1.965361\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:33 INFO 140624803325760] Epoch[125] Batch [5]#011Speed: 126.08 samples/sec#011loss=1.965361\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:37 INFO 140624803325760] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10324.418067932129, \"sum\": 10324.418067932129, \"min\": 10324.418067932129}}, \"EndTime\": 1538409097.663195, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409087.338555}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.039784686 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 50 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:36 INFO 139663579178816] Epoch[124] Batch[10] avg_epoch_loss=2.060623\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:36 INFO 139663579178816] Epoch[124] Batch [10]#011Speed: 118.19 samples/sec#011loss=2.126220\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:36 INFO 139663579178816] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11937.863111495972, \"sum\": 11937.863111495972, \"min\": 11937.863111495972}}, \"EndTime\": 1538409096.004429, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409084.066304}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:36 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=107.30466573 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:36 INFO 139663579178816] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:37 INFO 140252856112960] Epoch[123] Batch[10] avg_epoch_loss=2.067249\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:37 INFO 140252856112960] Epoch[123] Batch [10]#011Speed: 117.35 samples/sec#011loss=2.078818\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:37 INFO 140252856112960] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12107.54108428955, \"sum\": 12107.54108428955, \"min\": 12107.54108428955}}, \"EndTime\": 1538409097.16572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409085.057924}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:37 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.939256184 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:37 INFO 140252856112960] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:37 INFO 139663579178816] Epoch[125] Batch[0] avg_epoch_loss=2.161065\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:37 INFO 140196492334912] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10294.849157333374, \"sum\": 10294.849157333374, \"min\": 10294.849157333374}}, \"EndTime\": 1538409097.48747, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409087.19239}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:37 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=124.138390095 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:37 INFO 140196492334912] #progress_metric: host=algo-4, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:38 INFO 140252856112960] Epoch[124] Batch[0] avg_epoch_loss=2.004706\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:39 INFO 140196492334912] Epoch[123] Batch[0] avg_epoch_loss=1.884115\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:39 INFO 140624803325760] Epoch[126] Batch[0] avg_epoch_loss=2.341261\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:42 INFO 139663579178816] Epoch[125] Batch[5] avg_epoch_loss=2.121116\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:42 INFO 139663579178816] Epoch[125] Batch [5]#011Speed: 123.11 samples/sec#011loss=2.121116\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:43 INFO 140196492334912] Epoch[123] Batch[5] avg_epoch_loss=1.981044\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:43 INFO 140196492334912] Epoch[123] Batch [5]#011Speed: 154.69 samples/sec#011loss=1.981044\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:44 INFO 140252856112960] Epoch[124] Batch[5] avg_epoch_loss=2.104686\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:44 INFO 140252856112960] Epoch[124] Batch [5]#011Speed: 119.83 samples/sec#011loss=2.104686\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:44 INFO 140624803325760] Epoch[126] Batch[5] avg_epoch_loss=2.026413\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:44 INFO 140624803325760] Epoch[126] Batch [5]#011Speed: 125.79 samples/sec#011loss=2.026413\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:47 INFO 139663579178816] Epoch[125] Batch[10] avg_epoch_loss=2.231441\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:47 INFO 139663579178816] Epoch[125] Batch [10]#011Speed: 146.84 samples/sec#011loss=2.363831\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:47 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11124.387979507446, \"sum\": 11124.387979507446, \"min\": 11124.387979507446}}, \"EndTime\": 1538409107.129146, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409096.004495}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:47 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.241000687 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:47 INFO 139663579178816] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:47 INFO 140196492334912] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10250.669956207275, \"sum\": 10250.669956207275, \"min\": 10250.669956207275}}, \"EndTime\": 1538409107.738448, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409097.487546}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.820189213 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:48 INFO 140252856112960] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10849.776029586792, \"sum\": 10849.776029586792, \"min\": 10849.776029586792}}, \"EndTime\": 1538409108.01582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409097.165803}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:48 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.655285887 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:48 INFO 140252856112960] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:48 INFO 140624803325760] Epoch[126] Batch[10] avg_epoch_loss=2.165133\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:48 INFO 140624803325760] Epoch[126] Batch [10]#011Speed: 153.41 samples/sec#011loss=2.331596\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:48 INFO 140624803325760] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10825.97303390503, \"sum\": 10825.97303390503, \"min\": 10825.97303390503}}, \"EndTime\": 1538409108.489451, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409097.66325}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.064398166 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 50 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:49 INFO 139663579178816] Epoch[126] Batch[0] avg_epoch_loss=2.180277\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:49 INFO 140252856112960] Epoch[125] Batch[0] avg_epoch_loss=1.921974\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:50 INFO 140196492334912] Epoch[124] Batch[0] avg_epoch_loss=1.914397\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:50 INFO 140624803325760] Epoch[127] Batch[0] avg_epoch_loss=2.111288\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:53 INFO 139663579178816] Epoch[126] Batch[5] avg_epoch_loss=1.988301\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:53 INFO 139663579178816] Epoch[126] Batch [5]#011Speed: 153.34 samples/sec#011loss=1.988301\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:54 INFO 140196492334912] Epoch[124] Batch[5] avg_epoch_loss=1.935857\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:54 INFO 140196492334912] Epoch[124] Batch [5]#011Speed: 153.52 samples/sec#011loss=1.935857\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:54 INFO 140624803325760] Epoch[127] Batch[5] avg_epoch_loss=1.969078\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:54 INFO 140624803325760] Epoch[127] Batch [5]#011Speed: 157.74 samples/sec#011loss=1.969078\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:55 INFO 140252856112960] Epoch[125] Batch[5] avg_epoch_loss=1.880088\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:55 INFO 140252856112960] Epoch[125] Batch [5]#011Speed: 117.79 samples/sec#011loss=1.880088\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:57 INFO 139663579178816] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10392.581939697266, \"sum\": 10392.581939697266, \"min\": 10392.581939697266}}, \"EndTime\": 1538409117.522064, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409107.12923}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.68206523 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:58 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10914.26706314087, \"sum\": 10914.26706314087, \"min\": 10914.26706314087}}, \"EndTime\": 1538409118.930454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409108.015915}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:58 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.893938251 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:51:58 INFO 140252856112960] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:51:58 INFO 140196492334912] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10363.569974899292, \"sum\": 10363.569974899292, \"min\": 10363.569974899292}}, \"EndTime\": 1538409118.102331, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409107.738525}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:58 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=122.06086564 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:51:58 INFO 140196492334912] #progress_metric: host=algo-4, completed 50 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:58 INFO 140624803325760] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10006.988048553467, \"sum\": 10006.988048553467, \"min\": 10006.988048553467}}, \"EndTime\": 1538409118.496725, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409108.489519}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.813548853 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:51:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 51 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:51:59 INFO 139663579178816] Epoch[127] Batch[0] avg_epoch_loss=1.700512\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:00 INFO 140196492334912] Epoch[125] Batch[0] avg_epoch_loss=1.746424\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:00 INFO 140624803325760] Epoch[128] Batch[0] avg_epoch_loss=1.992210\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:00 INFO 140252856112960] Epoch[126] Batch[0] avg_epoch_loss=2.286607\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:04 INFO 139663579178816] Epoch[127] Batch[5] avg_epoch_loss=1.820628\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:04 INFO 139663579178816] Epoch[127] Batch [5]#011Speed: 153.04 samples/sec#011loss=1.820628\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:04 INFO 140196492334912] Epoch[125] Batch[5] avg_epoch_loss=2.060444\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:04 INFO 140196492334912] Epoch[125] Batch [5]#011Speed: 154.90 samples/sec#011loss=2.060444\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:04 INFO 140624803325760] Epoch[128] Batch[5] avg_epoch_loss=2.065913\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:04 INFO 140624803325760] Epoch[128] Batch [5]#011Speed: 156.28 samples/sec#011loss=2.065913\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:05 INFO 140252856112960] Epoch[126] Batch[5] avg_epoch_loss=2.153667\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:05 INFO 140252856112960] Epoch[126] Batch [5]#011Speed: 120.28 samples/sec#011loss=2.153667\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:08 INFO 140196492334912] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10257.04288482666, \"sum\": 10257.04288482666, \"min\": 10257.04288482666}}, \"EndTime\": 1538409128.359681, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409118.102407}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:08 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.671177158 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:08 INFO 140196492334912] #progress_metric: host=algo-4, completed 50 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:08 INFO 140624803325760] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10095.321893692017, \"sum\": 10095.321893692017, \"min\": 10095.321893692017}}, \"EndTime\": 1538409128.592404, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409118.496799}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.393750401 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 51 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:07 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10357.712030410767, \"sum\": 10357.712030410767, \"min\": 10357.712030410767}}, \"EndTime\": 1538409127.880144, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409117.522154}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:07 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.28822688 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:07 INFO 139663579178816] #progress_metric: host=algo-2, completed 51 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:10 INFO 139663579178816] Epoch[128] Batch[0] avg_epoch_loss=2.298203\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:10 INFO 140196492334912] Epoch[126] Batch[0] avg_epoch_loss=2.350219\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:10 INFO 140624803325760] Epoch[129] Batch[0] avg_epoch_loss=2.081915\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:10 INFO 140252856112960] Epoch[126] Batch[10] avg_epoch_loss=2.164377\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:10 INFO 140252856112960] Epoch[126] Batch [10]#011Speed: 146.01 samples/sec#011loss=2.177230\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:10 INFO 140252856112960] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11374.698877334595, \"sum\": 11374.698877334595, \"min\": 11374.698877334595}}, \"EndTime\": 1538409130.305551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409118.930556}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.67204749 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:12 INFO 140252856112960] Epoch[127] Batch[0] avg_epoch_loss=1.640735\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:14 INFO 139663579178816] Epoch[128] Batch[5] avg_epoch_loss=2.192822\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:14 INFO 139663579178816] Epoch[128] Batch [5]#011Speed: 151.61 samples/sec#011loss=2.192822\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:14 INFO 140196492334912] Epoch[126] Batch[5] avg_epoch_loss=2.132739\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:14 INFO 140196492334912] Epoch[126] Batch [5]#011Speed: 154.57 samples/sec#011loss=2.132739\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:14 INFO 140624803325760] Epoch[129] Batch[5] avg_epoch_loss=1.995274\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:14 INFO 140624803325760] Epoch[129] Batch [5]#011Speed: 157.55 samples/sec#011loss=1.995274\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:18 INFO 139663579178816] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10472.157001495361, \"sum\": 10472.157001495361, \"min\": 10472.157001495361}}, \"EndTime\": 1538409138.352645, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409127.880231}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:18 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.598701465 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:18 INFO 139663579178816] #progress_metric: host=algo-2, completed 51 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:16 INFO 140252856112960] Epoch[127] Batch[5] avg_epoch_loss=2.062993\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:16 INFO 140252856112960] Epoch[127] Batch [5]#011Speed: 151.60 samples/sec#011loss=2.062993\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:18 INFO 140196492334912] processed a total of 1215 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10336.212873458862, \"sum\": 10336.212873458862, \"min\": 10336.212873458862}}, \"EndTime\": 1538409138.696208, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409128.359758}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:18 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.546603469 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:18 INFO 140196492334912] #progress_metric: host=algo-4, completed 50 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:18 INFO 140624803325760] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10088.238000869751, \"sum\": 10088.238000869751, \"min\": 10088.238000869751}}, \"EndTime\": 1538409138.680952, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409128.592481}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.779937814 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:20 INFO 140624803325760] Epoch[130] Batch[0] avg_epoch_loss=1.964476\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:20 INFO 140252856112960] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10435.452938079834, \"sum\": 10435.452938079834, \"min\": 10435.452938079834}}, \"EndTime\": 1538409140.741336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409130.305637}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.645142907 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:20 INFO 139663579178816] Epoch[129] Batch[0] avg_epoch_loss=1.757904\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:21 INFO 140196492334912] Epoch[127] Batch[0] avg_epoch_loss=2.141688\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:23 INFO 140252856112960] Epoch[128] Batch[0] avg_epoch_loss=1.979170\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:25 INFO 140624803325760] Epoch[130] Batch[5] avg_epoch_loss=1.934863\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:25 INFO 140624803325760] Epoch[130] Batch [5]#011Speed: 157.27 samples/sec#011loss=1.934863\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:24 INFO 139663579178816] Epoch[129] Batch[5] avg_epoch_loss=1.940269\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:24 INFO 139663579178816] Epoch[129] Batch [5]#011Speed: 153.76 samples/sec#011loss=1.940269\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:25 INFO 140196492334912] Epoch[127] Batch[5] avg_epoch_loss=2.141446\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:25 INFO 140196492334912] Epoch[127] Batch [5]#011Speed: 155.00 samples/sec#011loss=2.141446\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:28 INFO 140196492334912] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10292.442083358765, \"sum\": 10292.442083358765, \"min\": 10292.442083358765}}, \"EndTime\": 1538409148.988962, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409138.696285}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:28 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.390183798 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:28 INFO 140196492334912] #progress_metric: host=algo-4, completed 51 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:28 INFO 140624803325760] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10084.341049194336, \"sum\": 10084.341049194336, \"min\": 10084.341049194336}}, \"EndTime\": 1538409148.765605, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409138.681027}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:28 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.349830028 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:28 INFO 140624803325760] #progress_metric: host=algo-3, completed 52 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:52:27 INFO 140252856112960] Epoch[128] Batch[5] avg_epoch_loss=2.007078\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:27 INFO 140252856112960] Epoch[128] Batch [5]#011Speed: 152.07 samples/sec#011loss=2.007078\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:30 INFO 139663579178816] Epoch[129] Batch[10] avg_epoch_loss=1.956168\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:30 INFO 139663579178816] Epoch[129] Batch [10]#011Speed: 120.17 samples/sec#011loss=1.975247\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:30 INFO 139663579178816] processed a total of 1320 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11873.55899810791, \"sum\": 11873.55899810791, \"min\": 11873.55899810791}}, \"EndTime\": 1538409150.22655, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409138.352734}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:30 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.170188611 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:30 INFO 139663579178816] #progress_metric: host=algo-2, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:31 INFO 140624803325760] Epoch[131] Batch[0] avg_epoch_loss=1.952603\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:31 INFO 140252856112960] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10423.59709739685, \"sum\": 10423.59709739685, \"min\": 10423.59709739685}}, \"EndTime\": 1538409151.165243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409140.741409}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:31 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.110809374 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:31 INFO 140252856112960] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:31 INFO 140196492334912] Epoch[128] Batch[0] avg_epoch_loss=2.001137\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:31 INFO 139663579178816] Epoch[130] Batch[0] avg_epoch_loss=2.012644\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:33 INFO 140252856112960] Epoch[129] Batch[0] avg_epoch_loss=1.870836\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:35 INFO 140624803325760] Epoch[131] Batch[5] avg_epoch_loss=2.047873\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:35 INFO 140624803325760] Epoch[131] Batch [5]#011Speed: 157.34 samples/sec#011loss=2.047873\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:35 INFO 140196492334912] Epoch[128] Batch[5] avg_epoch_loss=2.085676\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:35 INFO 140196492334912] Epoch[128] Batch [5]#011Speed: 156.10 samples/sec#011loss=2.085676\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:37 INFO 139663579178816] Epoch[130] Batch[5] avg_epoch_loss=1.891474\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:37 INFO 139663579178816] Epoch[130] Batch [5]#011Speed: 121.75 samples/sec#011loss=1.891474\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:37 INFO 140252856112960] Epoch[129] Batch[5] avg_epoch_loss=1.998252\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:37 INFO 140252856112960] Epoch[129] Batch [5]#011Speed: 144.94 samples/sec#011loss=1.998252\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:41 INFO 139663579178816] Epoch[130] Batch[10] avg_epoch_loss=2.114417\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:41 INFO 139663579178816] Epoch[130] Batch [10]#011Speed: 147.90 samples/sec#011loss=2.381948\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:41 INFO 139663579178816] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11223.178148269653, \"sum\": 11223.178148269653, \"min\": 11223.178148269653}}, \"EndTime\": 1538409161.450058, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409150.226636}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.632331027 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:38 INFO 140624803325760] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10062.604904174805, \"sum\": 10062.604904174805, \"min\": 10062.604904174805}}, \"EndTime\": 1538409158.828525, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409148.765681}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:38 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.823440629 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:38 INFO 140624803325760] #progress_metric: host=algo-3, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:40 INFO 140196492334912] Epoch[128] Batch[10] avg_epoch_loss=2.115330\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:40 INFO 140196492334912] Epoch[128] Batch [10]#011Speed: 118.29 samples/sec#011loss=2.150914\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:40 INFO 140196492334912] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11852.835893630981, \"sum\": 11852.835893630981, \"min\": 11852.835893630981}}, \"EndTime\": 1538409160.842096, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409148.989038}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:40 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.424182448 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:40 INFO 140196492334912] #progress_metric: host=algo-4, completed 51 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:41 INFO 140624803325760] Epoch[132] Batch[0] avg_epoch_loss=1.971702\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:42 INFO 140196492334912] Epoch[129] Batch[0] avg_epoch_loss=2.242391\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:43 INFO 140252856112960] Epoch[129] Batch[10] avg_epoch_loss=2.131336\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:43 INFO 140252856112960] Epoch[129] Batch [10]#011Speed: 115.88 samples/sec#011loss=2.291036\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:43 INFO 140252856112960] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12351.957082748413, \"sum\": 12351.957082748413, \"min\": 12351.957082748413}}, \"EndTime\": 1538409163.517526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409151.165319}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:43 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.431298092 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:43 INFO 140252856112960] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:43 INFO 139663579178816] Epoch[131] Batch[0] avg_epoch_loss=2.253781\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:45 INFO 140624803325760] Epoch[132] Batch[5] avg_epoch_loss=2.131494\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:45 INFO 140624803325760] Epoch[132] Batch [5]#011Speed: 158.37 samples/sec#011loss=2.131494\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:45 INFO 140252856112960] Epoch[130] Batch[0] avg_epoch_loss=2.214721\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:47 INFO 140196492334912] Epoch[129] Batch[5] avg_epoch_loss=2.021272\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:47 INFO 140196492334912] Epoch[129] Batch [5]#011Speed: 117.11 samples/sec#011loss=2.021272\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:47 INFO 139663579178816] Epoch[131] Batch[5] avg_epoch_loss=2.253960\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:47 INFO 139663579178816] Epoch[131] Batch [5]#011Speed: 153.83 samples/sec#011loss=2.253960\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:48 INFO 140624803325760] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10090.351104736328, \"sum\": 10090.351104736328, \"min\": 10090.351104736328}}, \"EndTime\": 1538409168.91919, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409158.828599}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.98741985 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:51 INFO 140196492334912] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11096.720933914185, \"sum\": 11096.720933914185, \"min\": 11096.720933914185}}, \"EndTime\": 1538409171.939128, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409160.842182}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:51 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.176742685 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:51 INFO 140196492334912] #progress_metric: host=algo-4, completed 52 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:52:50 INFO 140252856112960] Epoch[130] Batch[5] avg_epoch_loss=2.187723\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:50 INFO 140252856112960] Epoch[130] Batch [5]#011Speed: 116.89 samples/sec#011loss=2.187723\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:51 INFO 139663579178816] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10253.864049911499, \"sum\": 10253.864049911499, \"min\": 10253.864049911499}}, \"EndTime\": 1538409171.704262, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409161.450141}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:51 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.683842727 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:51 INFO 139663579178816] #progress_metric: host=algo-2, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:51 INFO 140624803325760] Epoch[133] Batch[0] avg_epoch_loss=1.780468\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:53 INFO 140196492334912] Epoch[130] Batch[0] avg_epoch_loss=2.092439\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:54 INFO 139663579178816] Epoch[132] Batch[0] avg_epoch_loss=1.835977\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:54 INFO 140252856112960] Epoch[130] Batch[10] avg_epoch_loss=2.053882\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:54 INFO 140252856112960] Epoch[130] Batch [10]#011Speed: 150.19 samples/sec#011loss=1.893274\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:54 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11446.08998298645, \"sum\": 11446.08998298645, \"min\": 11446.08998298645}}, \"EndTime\": 1538409174.963914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409163.517598}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:54 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.66206824 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:54 INFO 140252856112960] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:55 INFO 140624803325760] Epoch[133] Batch[5] avg_epoch_loss=2.042371\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:52:55 INFO 140624803325760] Epoch[133] Batch [5]#011Speed: 159.56 samples/sec#011loss=2.042371\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:52:57 INFO 140252856112960] Epoch[131] Batch[0] avg_epoch_loss=2.032923\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:01 INFO 140252856112960] Epoch[131] Batch[5] avg_epoch_loss=2.130558\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:01 INFO 140252856112960] Epoch[131] Batch [5]#011Speed: 152.08 samples/sec#011loss=2.130558\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:58 INFO 139663579178816] Epoch[132] Batch[5] avg_epoch_loss=1.919747\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:52:58 INFO 139663579178816] Epoch[132] Batch [5]#011Speed: 152.56 samples/sec#011loss=1.919747\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:59 INFO 140196492334912] Epoch[130] Batch[5] avg_epoch_loss=2.035071\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:52:59 INFO 140196492334912] Epoch[130] Batch [5]#011Speed: 116.52 samples/sec#011loss=2.035071\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:00 INFO 140624803325760] Epoch[133] Batch[10] avg_epoch_loss=2.101279\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:00 INFO 140624803325760] Epoch[133] Batch [10]#011Speed: 124.93 samples/sec#011loss=2.171967\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:00 INFO 140624803325760] processed a total of 1353 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11516.62302017212, \"sum\": 11516.62302017212, \"min\": 11516.62302017212}}, \"EndTime\": 1538409180.43613, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409168.919266}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:00 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.481397263 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:00 INFO 140624803325760] #progress_metric: host=algo-3, completed 53 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:02 INFO 140624803325760] Epoch[134] Batch[0] avg_epoch_loss=1.782435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:02 INFO 139663579178816] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10372.47610092163, \"sum\": 10372.47610092163, \"min\": 10372.47610092163}}, \"EndTime\": 1538409182.077088, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409171.704351}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:02 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.016303063 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:02 INFO 139663579178816] #progress_metric: host=algo-2, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:02 INFO 140196492334912] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11037.543058395386, \"sum\": 11037.543058395386, \"min\": 11037.543058395386}}, \"EndTime\": 1538409182.977007, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409171.939204}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:02 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.429757188 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:02 INFO 140196492334912] #progress_metric: host=algo-4, completed 52 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:04 INFO 139663579178816] Epoch[133] Batch[0] avg_epoch_loss=1.919591\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:04 INFO 140196492334912] Epoch[131] Batch[0] avg_epoch_loss=1.704414\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:05 INFO 140252856112960] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10460.665941238403, \"sum\": 10460.665941238403, \"min\": 10460.665941238403}}, \"EndTime\": 1538409185.4249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409174.963995}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:05 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.832150394 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:05 INFO 140252856112960] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:07 INFO 140624803325760] Epoch[134] Batch[5] avg_epoch_loss=2.010425\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:07 INFO 140624803325760] Epoch[134] Batch [5]#011Speed: 126.96 samples/sec#011loss=2.010425\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:07 INFO 140252856112960] Epoch[132] Batch[0] avg_epoch_loss=2.122881\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:08 INFO 139663579178816] Epoch[133] Batch[5] avg_epoch_loss=2.050099\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:08 INFO 139663579178816] Epoch[133] Batch [5]#011Speed: 154.64 samples/sec#011loss=2.050099\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:10 INFO 140624803325760] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10265.496969223022, \"sum\": 10265.496969223022, \"min\": 10265.496969223022}}, \"EndTime\": 1538409190.701973, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409180.436193}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.278772598 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:10 INFO 140196492334912] Epoch[131] Batch[5] avg_epoch_loss=1.988559\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:10 INFO 140196492334912] Epoch[131] Batch [5]#011Speed: 117.25 samples/sec#011loss=1.988559\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:12 INFO 140252856112960] Epoch[132] Batch[5] avg_epoch_loss=2.099946\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:12 INFO 140252856112960] Epoch[132] Batch [5]#011Speed: 153.46 samples/sec#011loss=2.099946\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:12 INFO 140624803325760] Epoch[135] Batch[0] avg_epoch_loss=2.052819\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:14 INFO 140196492334912] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11043.279886245728, \"sum\": 11043.279886245728, \"min\": 11043.279886245728}}, \"EndTime\": 1538409194.020632, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409182.977091}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.465188608 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 52 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:13 INFO 139663579178816] Epoch[133] Batch[10] avg_epoch_loss=2.021367\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:13 INFO 139663579178816] Epoch[133] Batch [10]#011Speed: 119.38 samples/sec#011loss=1.986887\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:13 INFO 139663579178816] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11871.803045272827, \"sum\": 11871.803045272827, \"min\": 11871.803045272827}}, \"EndTime\": 1538409193.949234, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409182.077176}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=108.154158113 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 53 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:15 INFO 139663579178816] Epoch[134] Batch[0] avg_epoch_loss=1.981747\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:15 INFO 140196492334912] Epoch[132] Batch[0] avg_epoch_loss=2.283707\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:15 INFO 140252856112960] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10558.918952941895, \"sum\": 10558.918952941895, \"min\": 10558.918952941895}}, \"EndTime\": 1538409195.984155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409185.424982}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:15 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.48769606 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:15 INFO 140252856112960] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:17 INFO 140624803325760] Epoch[135] Batch[5] avg_epoch_loss=1.988733\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:17 INFO 140624803325760] Epoch[135] Batch [5]#011Speed: 127.24 samples/sec#011loss=1.988733\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:21 INFO 140624803325760] Epoch[135] Batch[10] avg_epoch_loss=2.060794\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:21 INFO 140624803325760] Epoch[135] Batch [10]#011Speed: 154.72 samples/sec#011loss=2.147266\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:21 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10768.392086029053, \"sum\": 10768.392086029053, \"min\": 10768.392086029053}}, \"EndTime\": 1538409201.470666, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409190.702046}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:21 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.001158009 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:21 INFO 140624803325760] #progress_metric: host=algo-3, completed 54 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:53:18 INFO 140252856112960] Epoch[133] Batch[0] avg_epoch_loss=2.015993\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:20 INFO 139663579178816] Epoch[134] Batch[5] avg_epoch_loss=2.101577\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:20 INFO 139663579178816] Epoch[134] Batch [5]#011Speed: 121.36 samples/sec#011loss=2.101577\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:21 INFO 140196492334912] Epoch[132] Batch[5] avg_epoch_loss=2.139454\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:21 INFO 140196492334912] Epoch[132] Batch [5]#011Speed: 117.24 samples/sec#011loss=2.139454\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:22 INFO 140252856112960] Epoch[133] Batch[5] avg_epoch_loss=2.155599\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:22 INFO 140252856112960] Epoch[133] Batch [5]#011Speed: 154.25 samples/sec#011loss=2.155599\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:23 INFO 140624803325760] Epoch[136] Batch[0] avg_epoch_loss=2.115446\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:25 INFO 139663579178816] Epoch[134] Batch[10] avg_epoch_loss=2.264657\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:25 INFO 139663579178816] Epoch[134] Batch [10]#011Speed: 147.60 samples/sec#011loss=2.460354\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:25 INFO 139663579178816] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11247.07293510437, \"sum\": 11247.07293510437, \"min\": 11247.07293510437}}, \"EndTime\": 1538409205.196654, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409193.949332}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:25 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.051872966 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:25 INFO 139663579178816] #progress_metric: host=algo-2, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:25 INFO 140196492334912] Epoch[132] Batch[10] avg_epoch_loss=2.043545\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:25 INFO 140196492334912] Epoch[132] Batch [10]#011Speed: 143.06 samples/sec#011loss=1.928455\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:25 INFO 140196492334912] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11606.950998306274, \"sum\": 11606.950998306274, \"min\": 11606.950998306274}}, \"EndTime\": 1538409205.627948, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409194.020727}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:25 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.292960947 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:25 INFO 140196492334912] #progress_metric: host=algo-4, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:26 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10318.95899772644, \"sum\": 10318.95899772644, \"min\": 10318.95899772644}}, \"EndTime\": 1538409206.303458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409195.984245}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:26 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=122.685272108 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:26 INFO 140252856112960] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:27 INFO 139663579178816] Epoch[135] Batch[0] avg_epoch_loss=1.950570\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:28 INFO 140196492334912] Epoch[133] Batch[0] avg_epoch_loss=2.012457\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:27 INFO 140624803325760] Epoch[136] Batch[5] avg_epoch_loss=2.000428\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:27 INFO 140624803325760] Epoch[136] Batch [5]#011Speed: 158.71 samples/sec#011loss=2.000428\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:28 INFO 140252856112960] Epoch[134] Batch[0] avg_epoch_loss=2.086341\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:31 INFO 139663579178816] Epoch[135] Batch[5] avg_epoch_loss=1.952081\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:31 INFO 139663579178816] Epoch[135] Batch [5]#011Speed: 153.62 samples/sec#011loss=1.952081\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:31 INFO 140624803325760] processed a total of 1199 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10016.220092773438, \"sum\": 10016.220092773438, \"min\": 10016.220092773438}}, \"EndTime\": 1538409211.487172, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409201.470733}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.70452247 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:32 INFO 140196492334912] Epoch[133] Batch[5] avg_epoch_loss=1.863522\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:32 INFO 140196492334912] Epoch[133] Batch [5]#011Speed: 148.62 samples/sec#011loss=1.863522\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:32 INFO 140252856112960] Epoch[134] Batch[5] avg_epoch_loss=2.160030\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:32 INFO 140252856112960] Epoch[134] Batch [5]#011Speed: 154.82 samples/sec#011loss=2.160030\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:33 INFO 140624803325760] Epoch[137] Batch[0] avg_epoch_loss=1.969149\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:36 INFO 139663579178816] Epoch[135] Batch[10] avg_epoch_loss=2.047897\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:36 INFO 139663579178816] Epoch[135] Batch [10]#011Speed: 122.69 samples/sec#011loss=2.162876\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:36 INFO 139663579178816] processed a total of 1379 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11755.522966384888, \"sum\": 11755.522966384888, \"min\": 11755.522966384888}}, \"EndTime\": 1538409216.952509, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409205.196737}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:36 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.305295317 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:36 INFO 139663579178816] #progress_metric: host=algo-2, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:37 INFO 140196492334912] Epoch[133] Batch[10] avg_epoch_loss=1.827770\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:37 INFO 140196492334912] Epoch[133] Batch [10]#011Speed: 117.28 samples/sec#011loss=1.784867\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:37 INFO 140196492334912] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12161.171913146973, \"sum\": 12161.171913146973, \"min\": 12161.171913146973}}, \"EndTime\": 1538409217.789451, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409205.628031}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:37 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=105.415822829 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:37 INFO 140196492334912] #progress_metric: host=algo-4, completed 53 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:37 INFO 140624803325760] Epoch[137] Batch[5] avg_epoch_loss=1.951285\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:37 INFO 140624803325760] Epoch[137] Batch [5]#011Speed: 158.32 samples/sec#011loss=1.951285\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:38 INFO 140252856112960] Epoch[134] Batch[10] avg_epoch_loss=2.182836\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:38 INFO 140252856112960] Epoch[134] Batch [10]#011Speed: 119.90 samples/sec#011loss=2.210203\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:38 INFO 140252856112960] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11852.07200050354, \"sum\": 11852.07200050354, \"min\": 11852.07200050354}}, \"EndTime\": 1538409218.155844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409206.303539}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:38 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.287599654 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:38 INFO 140252856112960] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:38 INFO 139663579178816] Epoch[136] Batch[0] avg_epoch_loss=2.055158\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:39 INFO 140196492334912] Epoch[134] Batch[0] avg_epoch_loss=1.819075\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:39 INFO 140252856112960] Epoch[135] Batch[0] avg_epoch_loss=2.062011\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:43 INFO 140624803325760] Epoch[137] Batch[10] avg_epoch_loss=1.942377\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:43 INFO 140624803325760] Epoch[137] Batch [10]#011Speed: 122.95 samples/sec#011loss=1.931688\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:43 INFO 140624803325760] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11567.509889602661, \"sum\": 11567.509889602661, \"min\": 11567.509889602661}}, \"EndTime\": 1538409223.054994, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409211.487248}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.691082145 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 55 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:43 INFO 139663579178816] Epoch[136] Batch[5] avg_epoch_loss=2.012830\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:43 INFO 139663579178816] Epoch[136] Batch [5]#011Speed: 122.77 samples/sec#011loss=2.012830\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:44 INFO 140196492334912] Epoch[134] Batch[5] avg_epoch_loss=1.976792\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:44 INFO 140196492334912] Epoch[134] Batch [5]#011Speed: 118.33 samples/sec#011loss=1.976792\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:53:44 INFO 140624803325760] Epoch[138] Batch[0] avg_epoch_loss=2.093494\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:44 INFO 140252856112960] Epoch[135] Batch[5] avg_epoch_loss=2.116182\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:44 INFO 140252856112960] Epoch[135] Batch [5]#011Speed: 121.36 samples/sec#011loss=2.116182\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:47 INFO 139663579178816] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10591.821908950806, \"sum\": 10591.821908950806, \"min\": 10591.821908950806}}, \"EndTime\": 1538409227.544655, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409216.952594}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:47 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.675299316 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:47 INFO 139663579178816] #progress_metric: host=algo-2, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:48 INFO 140252856112960] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10671.62299156189, \"sum\": 10671.62299156189, \"min\": 10671.62299156189}}, \"EndTime\": 1538409228.827762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409218.155911}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:48 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.445178524 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:48 INFO 140252856112960] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:49 INFO 139663579178816] Epoch[137] Batch[0] avg_epoch_loss=2.064054\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:49 INFO 140196492334912] Epoch[134] Batch[10] avg_epoch_loss=1.934040\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:49 INFO 140196492334912] Epoch[134] Batch [10]#011Speed: 140.61 samples/sec#011loss=1.882738\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:49 INFO 140196492334912] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11662.00304031372, \"sum\": 11662.00304031372, \"min\": 11662.00304031372}}, \"EndTime\": 1538409229.451849, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409217.789599}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:49 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.70020867 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:49 INFO 140196492334912] #progress_metric: host=algo-4, completed 54 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:49 INFO 140624803325760] Epoch[138] Batch[5] avg_epoch_loss=2.048221\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:49 INFO 140624803325760] Epoch[138] Batch [5]#011Speed: 124.30 samples/sec#011loss=2.048221\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:50 INFO 140252856112960] Epoch[136] Batch[0] avg_epoch_loss=2.450268\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:51 INFO 140196492334912] Epoch[135] Batch[0] avg_epoch_loss=1.959960\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:53 INFO 140624803325760] Epoch[138] Batch[10] avg_epoch_loss=1.971978\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:53 INFO 140624803325760] Epoch[138] Batch [10]#011Speed: 154.81 samples/sec#011loss=1.880486\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:53 INFO 140624803325760] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10896.803140640259, \"sum\": 10896.803140640259, \"min\": 10896.803140640259}}, \"EndTime\": 1538409233.952107, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409223.05507}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.685883601 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 55 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:54 INFO 139663579178816] Epoch[137] Batch[5] avg_epoch_loss=1.896073\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:54 INFO 139663579178816] Epoch[137] Batch [5]#011Speed: 121.73 samples/sec#011loss=1.896073\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:55 INFO 140252856112960] Epoch[136] Batch[5] avg_epoch_loss=2.093118\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:55 INFO 140252856112960] Epoch[136] Batch [5]#011Speed: 121.42 samples/sec#011loss=2.093118\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:56 INFO 140196492334912] Epoch[135] Batch[5] avg_epoch_loss=1.983751\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:53:56 INFO 140196492334912] Epoch[135] Batch [5]#011Speed: 152.12 samples/sec#011loss=1.983751\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:53:56 INFO 140624803325760] Epoch[139] Batch[0] avg_epoch_loss=2.138303\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:58 INFO 139663579178816] Epoch[137] Batch[10] avg_epoch_loss=1.888162\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:58 INFO 139663579178816] Epoch[137] Batch [10]#011Speed: 148.71 samples/sec#011loss=1.878669\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:58 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11136.384010314941, \"sum\": 11136.384010314941, \"min\": 11136.384010314941}}, \"EndTime\": 1538409238.681308, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409227.544722}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:58 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.116862858 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:53:58 INFO 139663579178816] #progress_metric: host=algo-2, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:59 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10668.872833251953, \"sum\": 10668.872833251953, \"min\": 10668.872833251953}}, \"EndTime\": 1538409239.496959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409228.827837}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:59 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.661689629 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:53:59 INFO 140252856112960] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:00 INFO 140624803325760] Epoch[139] Batch[5] avg_epoch_loss=1.968976\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:00 INFO 140624803325760] Epoch[139] Batch [5]#011Speed: 158.91 samples/sec#011loss=1.968976\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:01 INFO 140252856112960] Epoch[137] Batch[0] avg_epoch_loss=1.807958\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:01 INFO 139663579178816] Epoch[138] Batch[0] avg_epoch_loss=2.081700\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:01 INFO 140196492334912] Epoch[135] Batch[10] avg_epoch_loss=2.106732\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:01 INFO 140196492334912] Epoch[135] Batch [10]#011Speed: 121.62 samples/sec#011loss=2.254310\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:01 INFO 140196492334912] processed a total of 1356 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11910.583019256592, \"sum\": 11910.583019256592, \"min\": 11910.583019256592}}, \"EndTime\": 1538409241.362752, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409229.451931}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:01 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.847337559 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:01 INFO 140196492334912] #progress_metric: host=algo-4, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:02 INFO 140196492334912] Epoch[136] Batch[0] avg_epoch_loss=2.039052\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:04 INFO 140624803325760] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10090.617895126343, \"sum\": 10090.617895126343, \"min\": 10090.617895126343}}, \"EndTime\": 1538409244.043098, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409233.952177}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:04 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.966158807 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:04 INFO 140624803325760] #progress_metric: host=algo-3, completed 56 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:05 INFO 139663579178816] Epoch[138] Batch[5] avg_epoch_loss=2.070940\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:05 INFO 139663579178816] Epoch[138] Batch [5]#011Speed: 153.13 samples/sec#011loss=2.070940\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:06 INFO 140624803325760] Epoch[140] Batch[0] avg_epoch_loss=2.013459\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:06 INFO 140252856112960] Epoch[137] Batch[5] avg_epoch_loss=2.024835\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:06 INFO 140252856112960] Epoch[137] Batch [5]#011Speed: 122.25 samples/sec#011loss=2.024835\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:08 INFO 140196492334912] Epoch[136] Batch[5] avg_epoch_loss=2.101036\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:08 INFO 140196492334912] Epoch[136] Batch [5]#011Speed: 121.71 samples/sec#011loss=2.101036\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:10 INFO 139663579178816] Epoch[138] Batch[10] avg_epoch_loss=2.185622\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:10 INFO 139663579178816] Epoch[138] Batch [10]#011Speed: 119.17 samples/sec#011loss=2.323240\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:10 INFO 139663579178816] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11921.064853668213, \"sum\": 11921.064853668213, \"min\": 11921.064853668213}}, \"EndTime\": 1538409250.602725, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409238.681392}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.888219789 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:10 INFO 140252856112960] Epoch[137] Batch[10] avg_epoch_loss=1.925215\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:10 INFO 140252856112960] Epoch[137] Batch [10]#011Speed: 146.84 samples/sec#011loss=1.805671\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:10 INFO 140252856112960] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11235.9619140625, \"sum\": 11235.9619140625, \"min\": 11235.9619140625}}, \"EndTime\": 1538409250.733227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409239.497037}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.944866548 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:12 INFO 140196492334912] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10653.755903244019, \"sum\": 10653.755903244019, \"min\": 10653.755903244019}}, \"EndTime\": 1538409252.016811, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409241.362823}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:12 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.956443913 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:12 INFO 140196492334912] #progress_metric: host=algo-4, completed 54 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:12 INFO 139663579178816] Epoch[139] Batch[0] avg_epoch_loss=1.895829\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:54:10 INFO 140624803325760] Epoch[140] Batch[5] avg_epoch_loss=1.994712\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:10 INFO 140624803325760] Epoch[140] Batch [5]#011Speed: 159.65 samples/sec#011loss=1.994712\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:13 INFO 140252856112960] Epoch[138] Batch[0] avg_epoch_loss=1.916878\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:13 INFO 140196492334912] Epoch[137] Batch[0] avg_epoch_loss=2.262101\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:15 INFO 140624803325760] Epoch[140] Batch[10] avg_epoch_loss=1.817014\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:15 INFO 140624803325760] Epoch[140] Batch [10]#011Speed: 125.28 samples/sec#011loss=1.603775\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:15 INFO 140624803325760] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11470.890045166016, \"sum\": 11470.890045166016, \"min\": 11470.890045166016}}, \"EndTime\": 1538409255.514299, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409244.043176}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:15 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.160028895 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:15 INFO 140624803325760] #progress_metric: host=algo-3, completed 56 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:17 INFO 140624803325760] Epoch[141] Batch[0] avg_epoch_loss=1.888144\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:17 INFO 139663579178816] Epoch[139] Batch[5] avg_epoch_loss=2.001583\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:17 INFO 139663579178816] Epoch[139] Batch [5]#011Speed: 122.23 samples/sec#011loss=2.001583\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:17 INFO 140252856112960] Epoch[138] Batch[5] avg_epoch_loss=1.962971\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:17 INFO 140252856112960] Epoch[138] Batch [5]#011Speed: 148.63 samples/sec#011loss=1.962971\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:18 INFO 140196492334912] Epoch[137] Batch[5] avg_epoch_loss=2.104406\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:18 INFO 140196492334912] Epoch[137] Batch [5]#011Speed: 122.96 samples/sec#011loss=2.104406\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:21 INFO 139663579178816] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10627.331972122192, \"sum\": 10627.331972122192, \"min\": 10627.331972122192}}, \"EndTime\": 1538409261.230397, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409250.602822}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:21 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.501866886 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:21 INFO 139663579178816] #progress_metric: host=algo-2, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:21 INFO 140252856112960] processed a total of 1196 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10706.433057785034, \"sum\": 10706.433057785034, \"min\": 10706.433057785034}}, \"EndTime\": 1538409261.439953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409250.733296}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.707370324 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:22 INFO 140624803325760] Epoch[141] Batch[5] avg_epoch_loss=1.939307\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:22 INFO 140624803325760] Epoch[141] Batch [5]#011Speed: 125.05 samples/sec#011loss=1.939307\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:22 INFO 139663579178816] Epoch[140] Batch[0] avg_epoch_loss=1.902623\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:23 INFO 140196492334912] Epoch[137] Batch[10] avg_epoch_loss=2.068849\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:23 INFO 140196492334912] Epoch[137] Batch [10]#011Speed: 150.78 samples/sec#011loss=2.026181\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:23 INFO 140196492334912] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11050.191879272461, \"sum\": 11050.191879272461, \"min\": 11050.191879272461}}, \"EndTime\": 1538409263.067315, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409252.016887}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:23 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.729840239 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:23 INFO 140196492334912] #progress_metric: host=algo-4, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:23 INFO 140252856112960] Epoch[139] Batch[0] avg_epoch_loss=2.218076\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:25 INFO 140196492334912] Epoch[138] Batch[0] avg_epoch_loss=1.841684\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:26 INFO 140624803325760] Epoch[141] Batch[10] avg_epoch_loss=1.995912\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:26 INFO 140624803325760] Epoch[141] Batch [10]#011Speed: 153.31 samples/sec#011loss=2.063839\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:26 INFO 140624803325760] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10887.460947036743, \"sum\": 10887.460947036743, \"min\": 10887.460947036743}}, \"EndTime\": 1538409266.402068, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409255.514369}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.024541888 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 56 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:28 INFO 139663579178816] Epoch[140] Batch[5] avg_epoch_loss=2.005581\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:28 INFO 139663579178816] Epoch[140] Batch [5]#011Speed: 122.03 samples/sec#011loss=2.005581\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:28 INFO 140624803325760] Epoch[142] Batch[0] avg_epoch_loss=2.191149\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:28 INFO 140252856112960] Epoch[139] Batch[5] avg_epoch_loss=2.001602\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:28 INFO 140252856112960] Epoch[139] Batch [5]#011Speed: 146.64 samples/sec#011loss=2.001602\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:29 INFO 140196492334912] Epoch[138] Batch[5] avg_epoch_loss=2.113120\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:29 INFO 140196492334912] Epoch[138] Batch [5]#011Speed: 152.25 samples/sec#011loss=2.113120\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:32 INFO 139663579178816] Epoch[140] Batch[10] avg_epoch_loss=2.123568\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:32 INFO 139663579178816] Epoch[140] Batch [10]#011Speed: 148.91 samples/sec#011loss=2.265151\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:32 INFO 139663579178816] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11141.099214553833, \"sum\": 11141.099214553833, \"min\": 11141.099214553833}}, \"EndTime\": 1538409272.371778, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409261.230471}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:32 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.209631433 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:32 INFO 139663579178816] #progress_metric: host=algo-2, completed 56 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:32 INFO 140624803325760] Epoch[142] Batch[5] avg_epoch_loss=2.054488\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:32 INFO 140624803325760] Epoch[142] Batch [5]#011Speed: 159.34 samples/sec#011loss=2.054488\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:33 INFO 140196492334912] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10309.516191482544, \"sum\": 10309.516191482544, \"min\": 10309.516191482544}}, \"EndTime\": 1538409273.377131, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409263.067385}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:33 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.954908767 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:33 INFO 140196492334912] #progress_metric: host=algo-4, completed 55 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:33 INFO 140252856112960] Epoch[139] Batch[10] avg_epoch_loss=2.122401\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:33 INFO 140252856112960] Epoch[139] Batch [10]#011Speed: 116.26 samples/sec#011loss=2.267359\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:33 INFO 140252856112960] processed a total of 1347 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12345.494031906128, \"sum\": 12345.494031906128, \"min\": 12345.494031906128}}, \"EndTime\": 1538409273.785758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409261.44003}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:33 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.107661336 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:33 INFO 140252856112960] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:34 INFO 139663579178816] Epoch[141] Batch[0] avg_epoch_loss=1.856056\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:35 INFO 140196492334912] Epoch[139] Batch[0] avg_epoch_loss=2.323016\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:35 INFO 140252856112960] Epoch[140] Batch[0] avg_epoch_loss=2.316952\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:36 INFO 140624803325760] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9998.52705001831, \"sum\": 9998.52705001831, \"min\": 9998.52705001831}}, \"EndTime\": 1538409276.400883, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409266.402138}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:36 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=127.117290214 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:36 INFO 140624803325760] #progress_metric: host=algo-3, completed 57 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:38 INFO 140624803325760] Epoch[143] Batch[0] avg_epoch_loss=1.899191\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:38 INFO 139663579178816] Epoch[141] Batch[5] avg_epoch_loss=2.043414\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:38 INFO 139663579178816] Epoch[141] Batch [5]#011Speed: 154.76 samples/sec#011loss=2.043414\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:39 INFO 140196492334912] Epoch[139] Batch[5] avg_epoch_loss=2.103199\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:39 INFO 140196492334912] Epoch[139] Batch [5]#011Speed: 154.81 samples/sec#011loss=2.103199\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:42 INFO 139663579178816] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10276.623010635376, \"sum\": 10276.623010635376, \"min\": 10276.623010635376}}, \"EndTime\": 1538409282.648745, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409272.371863}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:42 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.412157821 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:42 INFO 139663579178816] #progress_metric: host=algo-2, completed 56 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:54:40 INFO 140252856112960] Epoch[140] Batch[5] avg_epoch_loss=1.919604\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:40 INFO 140252856112960] Epoch[140] Batch [5]#011Speed: 117.05 samples/sec#011loss=1.919604\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:42 INFO 140624803325760] Epoch[143] Batch[5] avg_epoch_loss=1.930510\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:42 INFO 140624803325760] Epoch[143] Batch [5]#011Speed: 159.10 samples/sec#011loss=1.930510\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:44 INFO 140252856112960] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10995.306968688965, \"sum\": 10995.306968688965, \"min\": 10995.306968688965}}, \"EndTime\": 1538409284.781338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409273.785834}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.411752377 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:45 INFO 139663579178816] Epoch[142] Batch[0] avg_epoch_loss=2.177758\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:45 INFO 140196492334912] Epoch[139] Batch[10] avg_epoch_loss=2.105602\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:45 INFO 140196492334912] Epoch[139] Batch [10]#011Speed: 121.77 samples/sec#011loss=2.108486\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:45 INFO 140196492334912] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11723.071098327637, \"sum\": 11723.071098327637, \"min\": 11723.071098327637}}, \"EndTime\": 1538409285.100509, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409273.377206}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:45 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.40322848 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:45 INFO 140196492334912] #progress_metric: host=algo-4, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:46 INFO 140196492334912] Epoch[140] Batch[0] avg_epoch_loss=2.267671\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:46 INFO 140624803325760] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9932.491064071655, \"sum\": 9932.491064071655, \"min\": 9932.491064071655}}, \"EndTime\": 1538409286.333681, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409276.400959}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.250879487 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 57 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:46 INFO 140252856112960] Epoch[141] Batch[0] avg_epoch_loss=2.235458\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:48 INFO 140624803325760] Epoch[144] Batch[0] avg_epoch_loss=2.031690\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:49 INFO 139663579178816] Epoch[142] Batch[5] avg_epoch_loss=1.908833\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:49 INFO 139663579178816] Epoch[142] Batch [5]#011Speed: 153.96 samples/sec#011loss=1.908833\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:52 INFO 140624803325760] Epoch[144] Batch[5] avg_epoch_loss=2.316821\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:52 INFO 140624803325760] Epoch[144] Batch [5]#011Speed: 160.37 samples/sec#011loss=2.316821\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:51 INFO 140196492334912] Epoch[140] Batch[5] avg_epoch_loss=2.088786\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:51 INFO 140196492334912] Epoch[140] Batch [5]#011Speed: 122.92 samples/sec#011loss=2.088786\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:51 INFO 140252856112960] Epoch[141] Batch[5] avg_epoch_loss=1.999272\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:51 INFO 140252856112960] Epoch[141] Batch [5]#011Speed: 119.50 samples/sec#011loss=1.999272\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:54 INFO 139663579178816] Epoch[142] Batch[10] avg_epoch_loss=2.058162\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:54 INFO 139663579178816] Epoch[142] Batch [10]#011Speed: 119.59 samples/sec#011loss=2.237357\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:54 INFO 139663579178816] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11904.267072677612, \"sum\": 11904.267072677612, \"min\": 11904.267072677612}}, \"EndTime\": 1538409294.55337, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409282.648833}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:54 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=108.363230303 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:54 INFO 139663579178816] #progress_metric: host=algo-2, completed 57 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:55 INFO 140252856112960] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10834.963083267212, \"sum\": 10834.963083267212, \"min\": 10834.963083267212}}, \"EndTime\": 1538409295.616611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409284.781407}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:55 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.458318784 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:55 INFO 140252856112960] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:54:56 INFO 139663579178816] Epoch[143] Batch[0] avg_epoch_loss=2.568666\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:56 INFO 140196492334912] Epoch[140] Batch[10] avg_epoch_loss=1.894234\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:56 INFO 140196492334912] Epoch[140] Batch [10]#011Speed: 151.54 samples/sec#011loss=1.660772\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:56 INFO 140196492334912] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11019.3190574646, \"sum\": 11019.3190574646, \"min\": 11019.3190574646}}, \"EndTime\": 1538409296.12013, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409285.100583}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:56 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.699479333 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:56 INFO 140196492334912] #progress_metric: host=algo-4, completed 56 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:56 INFO 140624803325760] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9891.340017318726, \"sum\": 9891.340017318726, \"min\": 9891.340017318726}}, \"EndTime\": 1538409296.225324, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409286.333758}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:56 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.169571605 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:56 INFO 140624803325760] #progress_metric: host=algo-3, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:54:57 INFO 140252856112960] Epoch[142] Batch[0] avg_epoch_loss=1.894425\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:54:58 INFO 140196492334912] Epoch[141] Batch[0] avg_epoch_loss=2.002129\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:54:58 INFO 140624803325760] Epoch[145] Batch[0] avg_epoch_loss=1.902560\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:01 INFO 139663579178816] Epoch[143] Batch[5] avg_epoch_loss=2.195146\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:01 INFO 139663579178816] Epoch[143] Batch [5]#011Speed: 121.48 samples/sec#011loss=2.195146\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:02 INFO 140196492334912] Epoch[141] Batch[5] avg_epoch_loss=2.068229\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:02 INFO 140196492334912] Epoch[141] Batch [5]#011Speed: 154.14 samples/sec#011loss=2.068229\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:02 INFO 140624803325760] Epoch[145] Batch[5] avg_epoch_loss=2.055486\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:02 INFO 140624803325760] Epoch[145] Batch [5]#011Speed: 158.85 samples/sec#011loss=2.055486\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:02 INFO 140252856112960] Epoch[142] Batch[5] avg_epoch_loss=1.953382\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:02 INFO 140252856112960] Epoch[142] Batch [5]#011Speed: 118.42 samples/sec#011loss=1.953382\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:05 INFO 139663579178816] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10678.859949111938, \"sum\": 10678.859949111938, \"min\": 10678.859949111938}}, \"EndTime\": 1538409305.232559, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409294.553455}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.58422419 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:06 INFO 140196492334912] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10237.149953842163, \"sum\": 10237.149953842163, \"min\": 10237.149953842163}}, \"EndTime\": 1538409306.357577, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409296.1202}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.223760003 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:06 INFO 140252856112960] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10877.441883087158, \"sum\": 10877.441883087158, \"min\": 10877.441883087158}}, \"EndTime\": 1538409306.494423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409295.616689}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:06 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.904052443 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:06 INFO 140252856112960] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:06 INFO 139663579178816] Epoch[144] Batch[0] avg_epoch_loss=2.009828\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:07 INFO 140624803325760] Epoch[145] Batch[10] avg_epoch_loss=2.094602\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:07 INFO 140624803325760] Epoch[145] Batch [10]#011Speed: 125.57 samples/sec#011loss=2.141541\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:07 INFO 140624803325760] processed a total of 1383 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11471.308946609497, \"sum\": 11471.308946609497, \"min\": 11471.308946609497}}, \"EndTime\": 1538409307.696928, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409296.225398}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:07 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.560540686 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:07 INFO 140624803325760] #progress_metric: host=algo-3, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:08 INFO 140252856112960] Epoch[143] Batch[0] avg_epoch_loss=2.239476\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:08 INFO 140196492334912] Epoch[142] Batch[0] avg_epoch_loss=1.914411\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:09 INFO 140624803325760] Epoch[146] Batch[0] avg_epoch_loss=1.954374\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:12 INFO 140196492334912] Epoch[142] Batch[5] avg_epoch_loss=2.083394\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:12 INFO 140196492334912] Epoch[142] Batch [5]#011Speed: 144.64 samples/sec#011loss=2.083394\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:55:12 INFO 139663579178816] Epoch[144] Batch[5] avg_epoch_loss=2.027405\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:12 INFO 139663579178816] Epoch[144] Batch [5]#011Speed: 122.23 samples/sec#011loss=2.027405\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:13 INFO 140252856112960] Epoch[143] Batch[5] avg_epoch_loss=2.083889\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:13 INFO 140252856112960] Epoch[143] Batch [5]#011Speed: 118.10 samples/sec#011loss=2.083889\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:14 INFO 140624803325760] Epoch[146] Batch[5] avg_epoch_loss=2.032775\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:14 INFO 140624803325760] Epoch[146] Batch [5]#011Speed: 126.36 samples/sec#011loss=2.032775\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:15 INFO 139663579178816] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10599.912166595459, \"sum\": 10599.912166595459, \"min\": 10599.912166595459}}, \"EndTime\": 1538409315.832825, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409305.232634}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:15 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.037264493 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:15 INFO 139663579178816] #progress_metric: host=algo-2, completed 58 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:17 INFO 139663579178816] Epoch[145] Batch[0] avg_epoch_loss=1.666295\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:17 INFO 140624803325760] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10236.42897605896, \"sum\": 10236.42897605896, \"min\": 10236.42897605896}}, \"EndTime\": 1538409317.933645, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409307.697003}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:17 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.749196393 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:17 INFO 140624803325760] #progress_metric: host=algo-3, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:17 INFO 140252856112960] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10926.01203918457, \"sum\": 10926.01203918457, \"min\": 10926.01203918457}}, \"EndTime\": 1538409317.42085, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409306.494513}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.933514165 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:18 INFO 140196492334912] Epoch[142] Batch[10] avg_epoch_loss=2.024247\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:18 INFO 140196492334912] Epoch[142] Batch [10]#011Speed: 116.14 samples/sec#011loss=1.953272\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:18 INFO 140196492334912] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12124.042987823486, \"sum\": 12124.042987823486, \"min\": 12124.042987823486}}, \"EndTime\": 1538409318.481946, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409306.357657}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:18 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.213522375 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:18 INFO 140196492334912] #progress_metric: host=algo-4, completed 57 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:19 INFO 140252856112960] Epoch[144] Batch[0] avg_epoch_loss=2.048341\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:19 INFO 140624803325760] Epoch[147] Batch[0] avg_epoch_loss=1.994590\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:20 INFO 140196492334912] Epoch[143] Batch[0] avg_epoch_loss=1.766413\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:22 INFO 139663579178816] Epoch[145] Batch[5] avg_epoch_loss=1.870978\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:22 INFO 139663579178816] Epoch[145] Batch [5]#011Speed: 120.55 samples/sec#011loss=1.870978\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:24 INFO 140624803325760] Epoch[147] Batch[5] avg_epoch_loss=2.054513\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:24 INFO 140624803325760] Epoch[147] Batch [5]#011Speed: 125.63 samples/sec#011loss=2.054513\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:24 INFO 140252856112960] Epoch[144] Batch[5] avg_epoch_loss=1.938428\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:24 INFO 140252856112960] Epoch[144] Batch [5]#011Speed: 118.28 samples/sec#011loss=1.938428\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:25 INFO 140196492334912] Epoch[143] Batch[5] avg_epoch_loss=2.034564\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:25 INFO 140196492334912] Epoch[143] Batch [5]#011Speed: 121.77 samples/sec#011loss=2.034564\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:27 INFO 139663579178816] Epoch[145] Batch[10] avg_epoch_loss=1.583505\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:27 INFO 139663579178816] Epoch[145] Batch [10]#011Speed: 147.02 samples/sec#011loss=1.238537\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:27 INFO 139663579178816] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11325.959920883179, \"sum\": 11325.959920883179, \"min\": 11325.959920883179}}, \"EndTime\": 1538409327.15913, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409315.832914}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:27 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.454282625 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:27 INFO 139663579178816] #progress_metric: host=algo-2, completed 58 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:28 INFO 140624803325760] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10260.426998138428, \"sum\": 10260.426998138428, \"min\": 10260.426998138428}}, \"EndTime\": 1538409328.194402, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409317.933725}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:28 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.287880959 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:28 INFO 140624803325760] #progress_metric: host=algo-3, completed 59 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:28 INFO 140252856112960] Epoch[144] Batch[10] avg_epoch_loss=2.218038\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:28 INFO 140252856112960] Epoch[144] Batch [10]#011Speed: 146.18 samples/sec#011loss=2.553571\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:28 INFO 140252856112960] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11397.304058074951, \"sum\": 11397.304058074951, \"min\": 11397.304058074951}}, \"EndTime\": 1538409328.818493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409317.420923}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:28 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.534255901 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:28 INFO 140252856112960] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:29 INFO 139663579178816] Epoch[146] Batch[0] avg_epoch_loss=1.876617\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:29 INFO 140196492334912] Epoch[143] Batch[10] avg_epoch_loss=2.053431\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:29 INFO 140196492334912] Epoch[143] Batch [10]#011Speed: 151.18 samples/sec#011loss=2.076071\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:29 INFO 140196492334912] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11122.623920440674, \"sum\": 11122.623920440674, \"min\": 11122.623920440674}}, \"EndTime\": 1538409329.604926, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409318.482036}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.754587433 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 57 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:29 INFO 140624803325760] Epoch[148] Batch[0] avg_epoch_loss=1.659655\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:31 INFO 140252856112960] Epoch[145] Batch[0] avg_epoch_loss=1.969423\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:32 INFO 140196492334912] Epoch[144] Batch[0] avg_epoch_loss=1.995026\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:33 INFO 139663579178816] Epoch[146] Batch[5] avg_epoch_loss=1.870359\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:33 INFO 139663579178816] Epoch[146] Batch [5]#011Speed: 151.93 samples/sec#011loss=1.870359\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:55:34 INFO 140624803325760] Epoch[148] Batch[5] avg_epoch_loss=1.860890\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:34 INFO 140624803325760] Epoch[148] Batch [5]#011Speed: 125.18 samples/sec#011loss=1.860890\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:35 INFO 140252856112960] Epoch[145] Batch[5] avg_epoch_loss=1.836525\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:35 INFO 140252856112960] Epoch[145] Batch [5]#011Speed: 149.23 samples/sec#011loss=1.836525\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:36 INFO 140196492334912] Epoch[144] Batch[5] avg_epoch_loss=1.843163\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:36 INFO 140196492334912] Epoch[144] Batch [5]#011Speed: 150.64 samples/sec#011loss=1.843163\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:38 INFO 140624803325760] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10307.610988616943, \"sum\": 10307.610988616943, \"min\": 10307.610988616943}}, \"EndTime\": 1538409338.502334, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409328.194478}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:38 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.178639391 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:38 INFO 140624803325760] #progress_metric: host=algo-3, completed 59 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:39 INFO 139663579178816] Epoch[146] Batch[10] avg_epoch_loss=1.653575\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:39 INFO 139663579178816] Epoch[146] Batch [10]#011Speed: 120.62 samples/sec#011loss=1.393435\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:39 INFO 139663579178816] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11904.87813949585, \"sum\": 11904.87813949585, \"min\": 11904.87813949585}}, \"EndTime\": 1538409339.06441, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409327.159277}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:39 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.113780838 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:39 INFO 139663579178816] #progress_metric: host=algo-2, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:40 INFO 140196492334912] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10468.77408027649, \"sum\": 10468.77408027649, \"min\": 10468.77408027649}}, \"EndTime\": 1538409340.07403, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409329.605007}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:40 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.789311234 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:40 INFO 140196492334912] #progress_metric: host=algo-4, completed 58 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:40 INFO 140624803325760] Epoch[149] Batch[0] avg_epoch_loss=2.276507\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:39 INFO 140252856112960] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10573.714971542358, \"sum\": 10573.714971542358, \"min\": 10573.714971542358}}, \"EndTime\": 1538409339.392591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409328.818579}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:39 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.040931263 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:39 INFO 140252856112960] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:40 INFO 139663579178816] Epoch[147] Batch[0] avg_epoch_loss=1.875861\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:41 INFO 140252856112960] Epoch[146] Batch[0] avg_epoch_loss=1.863594\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:42 INFO 140196492334912] Epoch[145] Batch[0] avg_epoch_loss=1.990076\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:46 INFO 140252856112960] Epoch[146] Batch[5] avg_epoch_loss=2.071039\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:46 INFO 140252856112960] Epoch[146] Batch [5]#011Speed: 150.27 samples/sec#011loss=2.071039\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:46 INFO 140196492334912] Epoch[145] Batch[5] avg_epoch_loss=2.027216\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:46 INFO 140196492334912] Epoch[145] Batch [5]#011Speed: 155.12 samples/sec#011loss=2.027216\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:45 INFO 140624803325760] Epoch[149] Batch[5] avg_epoch_loss=2.079000\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:45 INFO 140624803325760] Epoch[149] Batch [5]#011Speed: 126.90 samples/sec#011loss=2.079000\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:45 INFO 139663579178816] Epoch[147] Batch[5] avg_epoch_loss=1.984378\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:45 INFO 139663579178816] Epoch[147] Batch [5]#011Speed: 121.22 samples/sec#011loss=1.984378\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:48 INFO 140624803325760] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10293.408155441284, \"sum\": 10293.408155441284, \"min\": 10293.408155441284}}, \"EndTime\": 1538409348.796099, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409338.502414}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.241324948 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 60 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:49 INFO 139663579178816] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10712.070941925049, \"sum\": 10712.070941925049, \"min\": 10712.070941925049}}, \"EndTime\": 1538409349.776807, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409339.064494}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.369710117 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 59 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:49 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10595.295906066895, \"sum\": 10595.295906066895, \"min\": 10595.295906066895}}, \"EndTime\": 1538409349.988242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409339.392686}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:49 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.485464 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:49 INFO 140252856112960] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:50 INFO 140196492334912] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10252.279043197632, \"sum\": 10252.279043197632, \"min\": 10252.279043197632}}, \"EndTime\": 1538409350.326639, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409340.074112}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:50 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.580938876 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:50 INFO 140196492334912] #progress_metric: host=algo-4, completed 58 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:50 INFO 140624803325760] Epoch[150] Batch[0] avg_epoch_loss=2.147120\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:51 INFO 139663579178816] Epoch[148] Batch[0] avg_epoch_loss=1.910505\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:52 INFO 140196492334912] Epoch[146] Batch[0] avg_epoch_loss=1.978267\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:52 INFO 140252856112960] Epoch[147] Batch[0] avg_epoch_loss=1.788091\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:55 INFO 140624803325760] Epoch[150] Batch[5] avg_epoch_loss=1.938581\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:55 INFO 140624803325760] Epoch[150] Batch [5]#011Speed: 127.66 samples/sec#011loss=1.938581\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:56 INFO 139663579178816] Epoch[148] Batch[5] avg_epoch_loss=1.990073\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:55:56 INFO 139663579178816] Epoch[148] Batch [5]#011Speed: 121.35 samples/sec#011loss=1.990073\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:56 INFO 140196492334912] Epoch[146] Batch[5] avg_epoch_loss=1.979428\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:55:56 INFO 140196492334912] Epoch[146] Batch [5]#011Speed: 154.33 samples/sec#011loss=1.979428\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:56 INFO 140252856112960] Epoch[147] Batch[5] avg_epoch_loss=1.857687\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:55:56 INFO 140252856112960] Epoch[147] Batch [5]#011Speed: 150.14 samples/sec#011loss=1.857687\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:59 INFO 140624803325760] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10241.27197265625, \"sum\": 10241.27197265625, \"min\": 10241.27197265625}}, \"EndTime\": 1538409359.037721, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409348.796176}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.127877927 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:55:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 60 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:00 INFO 140624803325760] Epoch[151] Batch[0] avg_epoch_loss=1.996375\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:01 INFO 139663579178816] Epoch[148] Batch[10] avg_epoch_loss=1.928280\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:01 INFO 139663579178816] Epoch[148] Batch [10]#011Speed: 143.01 samples/sec#011loss=1.854129\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:01 INFO 139663579178816] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11393.203020095825, \"sum\": 11393.203020095825, \"min\": 11393.203020095825}}, \"EndTime\": 1538409361.170345, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409349.776894}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.750735167 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 59 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:02 INFO 140252856112960] Epoch[147] Batch[10] avg_epoch_loss=1.853907\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:02 INFO 140252856112960] Epoch[147] Batch [10]#011Speed: 116.18 samples/sec#011loss=1.849372\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:02 INFO 140252856112960] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12154.921054840088, \"sum\": 12154.921054840088, \"min\": 12154.921054840088}}, \"EndTime\": 1538409362.143512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409349.988331}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.938728275 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:02 INFO 140196492334912] Epoch[146] Batch[10] avg_epoch_loss=2.168239\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:02 INFO 140196492334912] Epoch[146] Batch [10]#011Speed: 120.94 samples/sec#011loss=2.394812\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:02 INFO 140196492334912] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11806.75983428955, \"sum\": 11806.75983428955, \"min\": 11806.75983428955}}, \"EndTime\": 1538409362.133713, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409350.326713}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:02 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.528914848 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:02 INFO 140196492334912] #progress_metric: host=algo-4, completed 58 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:03 INFO 139663579178816] Epoch[149] Batch[0] avg_epoch_loss=1.943295\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:03 INFO 140196492334912] Epoch[147] Batch[0] avg_epoch_loss=2.051741\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:03 INFO 140252856112960] Epoch[148] Batch[0] avg_epoch_loss=2.154002\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:56:06 INFO 140624803325760] Epoch[151] Batch[5] avg_epoch_loss=1.885436\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:06 INFO 140624803325760] Epoch[151] Batch [5]#011Speed: 118.34 samples/sec#011loss=1.885436\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:07 INFO 139663579178816] Epoch[149] Batch[5] avg_epoch_loss=2.034813\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:07 INFO 139663579178816] Epoch[149] Batch [5]#011Speed: 155.93 samples/sec#011loss=2.034813\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:08 INFO 140196492334912] Epoch[147] Batch[5] avg_epoch_loss=1.972990\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:08 INFO 140196492334912] Epoch[147] Batch [5]#011Speed: 122.99 samples/sec#011loss=1.972990\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:09 INFO 140252856112960] Epoch[148] Batch[5] avg_epoch_loss=2.019587\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:09 INFO 140252856112960] Epoch[148] Batch [5]#011Speed: 120.29 samples/sec#011loss=2.019587\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:10 INFO 140624803325760] Epoch[151] Batch[10] avg_epoch_loss=1.554623\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:10 INFO 140624803325760] Epoch[151] Batch [10]#011Speed: 147.51 samples/sec#011loss=1.157648\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:10 INFO 140624803325760] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11338.834047317505, \"sum\": 11338.834047317505, \"min\": 11338.834047317505}}, \"EndTime\": 1538409370.376866, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409359.037799}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.149954958 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 60 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:11 INFO 139663579178816] processed a total of 1207 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10176.756858825684, \"sum\": 10176.756858825684, \"min\": 10176.756858825684}}, \"EndTime\": 1538409371.347442, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409361.170431}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:11 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.602013453 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:11 INFO 139663579178816] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:12 INFO 140624803325760] Epoch[152] Batch[0] avg_epoch_loss=2.053801\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:12 INFO 140252856112960] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10745.028972625732, \"sum\": 10745.028972625732, \"min\": 10745.028972625732}}, \"EndTime\": 1538409372.888876, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409362.143589}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:12 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.844182972 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:12 INFO 140252856112960] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:13 INFO 139663579178816] Epoch[150] Batch[0] avg_epoch_loss=1.675708\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:13 INFO 140196492334912] Epoch[147] Batch[10] avg_epoch_loss=2.153224\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:13 INFO 140196492334912] Epoch[147] Batch [10]#011Speed: 151.82 samples/sec#011loss=2.369505\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:13 INFO 140196492334912] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10992.659091949463, \"sum\": 10992.659091949463, \"min\": 10992.659091949463}}, \"EndTime\": 1538409373.126673, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409362.133783}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:13 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.351303444 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:13 INFO 140196492334912] #progress_metric: host=algo-4, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:15 INFO 140196492334912] Epoch[148] Batch[0] avg_epoch_loss=1.862716\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:14 INFO 140252856112960] Epoch[149] Batch[0] avg_epoch_loss=1.919386\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:16 INFO 140624803325760] Epoch[152] Batch[5] avg_epoch_loss=1.973721\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:16 INFO 140624803325760] Epoch[152] Batch [5]#011Speed: 152.08 samples/sec#011loss=1.973721\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:18 INFO 139663579178816] Epoch[150] Batch[5] avg_epoch_loss=2.080056\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:18 INFO 139663579178816] Epoch[150] Batch [5]#011Speed: 148.75 samples/sec#011loss=2.080056\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:19 INFO 140196492334912] Epoch[148] Batch[5] avg_epoch_loss=1.959058\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:19 INFO 140196492334912] Epoch[148] Batch [5]#011Speed: 155.93 samples/sec#011loss=1.959058\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:19 INFO 140252856112960] Epoch[149] Batch[5] avg_epoch_loss=2.013397\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:19 INFO 140252856112960] Epoch[149] Batch [5]#011Speed: 119.06 samples/sec#011loss=2.013397\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:20 INFO 140624803325760] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10364.748001098633, \"sum\": 10364.748001098633, \"min\": 10364.748001098633}}, \"EndTime\": 1538409380.741907, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409370.376934}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.249080213 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 61 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:23 INFO 140624803325760] Epoch[153] Batch[0] avg_epoch_loss=2.079429\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:23 INFO 139663579178816] Epoch[150] Batch[10] avg_epoch_loss=2.027694\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:23 INFO 139663579178816] Epoch[150] Batch [10]#011Speed: 119.34 samples/sec#011loss=1.964859\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:23 INFO 139663579178816] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12055.05895614624, \"sum\": 12055.05895614624, \"min\": 12055.05895614624}}, \"EndTime\": 1538409383.40285, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409371.347531}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:23 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.160004145 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:23 INFO 139663579178816] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:23 INFO 140196492334912] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10152.44197845459, \"sum\": 10152.44197845459, \"min\": 10152.44197845459}}, \"EndTime\": 1538409383.279407, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409373.12674}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:23 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.462321651 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:23 INFO 140196492334912] #progress_metric: host=algo-4, completed 59 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:23 INFO 140252856112960] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10876.559019088745, \"sum\": 10876.559019088745, \"min\": 10876.559019088745}}, \"EndTime\": 1538409383.765783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409372.888964}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:23 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.407181967 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:23 INFO 140252856112960] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:25 INFO 139663579178816] Epoch[151] Batch[0] avg_epoch_loss=2.215179\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:25 INFO 140196492334912] Epoch[149] Batch[0] avg_epoch_loss=1.827516\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:25 INFO 140252856112960] Epoch[150] Batch[0] avg_epoch_loss=2.027197\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:27 INFO 140624803325760] Epoch[153] Batch[5] avg_epoch_loss=2.052932\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:27 INFO 140624803325760] Epoch[153] Batch [5]#011Speed: 151.54 samples/sec#011loss=2.052932\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:30 INFO 140196492334912] Epoch[149] Batch[5] avg_epoch_loss=2.007968\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:30 INFO 140196492334912] Epoch[149] Batch [5]#011Speed: 145.60 samples/sec#011loss=2.007968\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:30 INFO 139663579178816] Epoch[151] Batch[5] avg_epoch_loss=1.950758\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:30 INFO 139663579178816] Epoch[151] Batch [5]#011Speed: 125.55 samples/sec#011loss=1.950758\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:31 INFO 140624803325760] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10379.59599494934, \"sum\": 10379.59599494934, \"min\": 10379.59599494934}}, \"EndTime\": 1538409391.121812, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409380.741984}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.487053253 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 61 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:30 INFO 140252856112960] Epoch[150] Batch[5] avg_epoch_loss=1.982424\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:30 INFO 140252856112960] Epoch[150] Batch [5]#011Speed: 120.52 samples/sec#011loss=1.982424\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:33 INFO 140624803325760] Epoch[154] Batch[0] avg_epoch_loss=1.993594\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:34 INFO 139663579178816] Epoch[151] Batch[10] avg_epoch_loss=1.842963\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:34 INFO 139663579178816] Epoch[151] Batch [10]#011Speed: 152.74 samples/sec#011loss=1.713609\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:34 INFO 139663579178816] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10999.359130859375, \"sum\": 10999.359130859375, \"min\": 10999.359130859375}}, \"EndTime\": 1538409394.402528, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409383.402935}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:34 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.823955882 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:34 INFO 139663579178816] #progress_metric: host=algo-2, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:34 INFO 140252856112960] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10824.924945831299, \"sum\": 10824.924945831299, \"min\": 10824.924945831299}}, \"EndTime\": 1538409394.591042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409383.765864}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:34 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.179526973 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:34 INFO 140252856112960] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:37 INFO 140624803325760] Epoch[154] Batch[5] avg_epoch_loss=2.005350\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:37 INFO 140624803325760] Epoch[154] Batch [5]#011Speed: 160.35 samples/sec#011loss=2.005350\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:56:35 INFO 140196492334912] Epoch[149] Batch[10] avg_epoch_loss=1.997064\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:35 INFO 140196492334912] Epoch[149] Batch [10]#011Speed: 116.29 samples/sec#011loss=1.983979\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:35 INFO 140196492334912] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12231.93883895874, \"sum\": 12231.93883895874, \"min\": 12231.93883895874}}, \"EndTime\": 1538409395.511668, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409383.279482}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.403635381 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:36 INFO 140252856112960] Epoch[151] Batch[0] avg_epoch_loss=2.041317\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:36 INFO 139663579178816] Epoch[152] Batch[0] avg_epoch_loss=1.972630\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:37 INFO 140196492334912] Epoch[150] Batch[0] avg_epoch_loss=1.698493\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:40 INFO 139663579178816] Epoch[152] Batch[5] avg_epoch_loss=1.930708\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:40 INFO 139663579178816] Epoch[152] Batch [5]#011Speed: 157.04 samples/sec#011loss=1.930708\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:41 INFO 140252856112960] Epoch[151] Batch[5] avg_epoch_loss=2.010657\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:41 INFO 140252856112960] Epoch[151] Batch [5]#011Speed: 119.46 samples/sec#011loss=2.010657\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:42 INFO 140196492334912] Epoch[150] Batch[5] avg_epoch_loss=1.936305\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:42 INFO 140196492334912] Epoch[150] Batch [5]#011Speed: 122.95 samples/sec#011loss=1.936305\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:42 INFO 140624803325760] Epoch[154] Batch[10] avg_epoch_loss=1.773830\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:42 INFO 140624803325760] Epoch[154] Batch [10]#011Speed: 125.90 samples/sec#011loss=1.496006\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:42 INFO 140624803325760] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11322.153091430664, \"sum\": 11322.153091430664, \"min\": 11322.153091430664}}, \"EndTime\": 1538409402.444295, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409391.121888}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:42 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.142675976 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:42 INFO 140624803325760] #progress_metric: host=algo-3, completed 62 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:44 INFO 140624803325760] Epoch[155] Batch[0] avg_epoch_loss=1.700309\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:46 INFO 140252856112960] Epoch[151] Batch[10] avg_epoch_loss=1.944726\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:46 INFO 140252856112960] Epoch[151] Batch [10]#011Speed: 145.34 samples/sec#011loss=1.865609\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:46 INFO 140252856112960] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11417.874097824097, \"sum\": 11417.874097824097, \"min\": 11417.874097824097}}, \"EndTime\": 1538409406.009275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409394.591131}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.468383427 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:46 INFO 140196492334912] Epoch[150] Batch[10] avg_epoch_loss=1.641951\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:46 INFO 140196492334912] Epoch[150] Batch [10]#011Speed: 150.50 samples/sec#011loss=1.288727\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:46 INFO 140196492334912] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11120.415925979614, \"sum\": 11120.415925979614, \"min\": 11120.415925979614}}, \"EndTime\": 1538409406.632402, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409395.511744}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:46 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.069817111 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:46 INFO 140196492334912] #progress_metric: host=algo-4, completed 60 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:46 INFO 139663579178816] Epoch[152] Batch[10] avg_epoch_loss=2.154932\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:46 INFO 139663579178816] Epoch[152] Batch [10]#011Speed: 122.27 samples/sec#011loss=2.424001\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:46 INFO 139663579178816] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11621.915817260742, \"sum\": 11621.915817260742, \"min\": 11621.915817260742}}, \"EndTime\": 1538409406.02474, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409394.402595}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:46 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.16833011 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:46 INFO 139663579178816] #progress_metric: host=algo-2, completed 61 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:47 INFO 139663579178816] Epoch[153] Batch[0] avg_epoch_loss=1.890767\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:49 INFO 140196492334912] Epoch[151] Batch[0] avg_epoch_loss=1.970948\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:48 INFO 140252856112960] Epoch[152] Batch[0] avg_epoch_loss=2.167075\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:49 INFO 140624803325760] Epoch[155] Batch[5] avg_epoch_loss=2.204702\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:49 INFO 140624803325760] Epoch[155] Batch [5]#011Speed: 121.81 samples/sec#011loss=2.204702\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:52 INFO 139663579178816] Epoch[153] Batch[5] avg_epoch_loss=2.211262\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:52 INFO 139663579178816] Epoch[153] Batch [5]#011Speed: 125.56 samples/sec#011loss=2.211262\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:53 INFO 140624803325760] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10701.306104660034, \"sum\": 10701.306104660034, \"min\": 10701.306104660034}}, \"EndTime\": 1538409413.145906, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409402.444366}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.049676386 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:52 INFO 140252856112960] Epoch[152] Batch[5] avg_epoch_loss=2.397521\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:52 INFO 140252856112960] Epoch[152] Batch [5]#011Speed: 153.73 samples/sec#011loss=2.397521\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:53 INFO 140196492334912] Epoch[151] Batch[5] avg_epoch_loss=2.382419\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:53 INFO 140196492334912] Epoch[151] Batch [5]#011Speed: 154.52 samples/sec#011loss=2.382419\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:54 INFO 140624803325760] Epoch[156] Batch[0] avg_epoch_loss=2.421478\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:56 INFO 139663579178816] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10400.929927825928, \"sum\": 10400.929927825928, \"min\": 10400.929927825928}}, \"EndTime\": 1538409416.425921, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409406.024804}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:56 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.757342386 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:56 INFO 139663579178816] #progress_metric: host=algo-2, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:56 INFO 140196492334912] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10310.60814857483, \"sum\": 10310.60814857483, \"min\": 10310.60814857483}}, \"EndTime\": 1538409416.943337, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409406.632484}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:56 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.463758067 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:56 INFO 140196492334912] #progress_metric: host=algo-4, completed 60 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:56:56 INFO 140252856112960] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10360.713958740234, \"sum\": 10360.713958740234, \"min\": 10360.713958740234}}, \"EndTime\": 1538409416.370321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409406.009359}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:56 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=122.962931286 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:56 INFO 140252856112960] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:56:57 INFO 139663579178816] Epoch[154] Batch[0] avg_epoch_loss=2.019684\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:56:58 INFO 140252856112960] Epoch[153] Batch[0] avg_epoch_loss=2.392788\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:56:59 INFO 140196492334912] Epoch[152] Batch[0] avg_epoch_loss=2.308093\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:59 INFO 140624803325760] Epoch[156] Batch[5] avg_epoch_loss=2.372242\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:56:59 INFO 140624803325760] Epoch[156] Batch [5]#011Speed: 123.37 samples/sec#011loss=2.372242\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:02 INFO 140252856112960] Epoch[153] Batch[5] avg_epoch_loss=2.491751\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:02 INFO 140252856112960] Epoch[153] Batch [5]#011Speed: 151.54 samples/sec#011loss=2.491751\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:03 INFO 139663579178816] Epoch[154] Batch[5] avg_epoch_loss=2.479251\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:03 INFO 139663579178816] Epoch[154] Batch [5]#011Speed: 123.33 samples/sec#011loss=2.479251\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:03 INFO 140196492334912] Epoch[152] Batch[5] avg_epoch_loss=2.636581\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:03 INFO 140196492334912] Epoch[152] Batch [5]#011Speed: 154.44 samples/sec#011loss=2.636581\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:03 INFO 140624803325760] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10608.720064163208, \"sum\": 10608.720064163208, \"min\": 10608.720064163208}}, \"EndTime\": 1538409423.754937, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409413.145981}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:03 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.166230661 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:03 INFO 140624803325760] #progress_metric: host=algo-3, completed 62 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:05 INFO 140624803325760] Epoch[157] Batch[0] avg_epoch_loss=2.708414\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:06 INFO 139663579178816] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10507.653951644897, \"sum\": 10507.653951644897, \"min\": 10507.653951644897}}, \"EndTime\": 1538409426.933894, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409416.425989}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.053356662 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:07 INFO 140196492334912] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10239.407062530518, \"sum\": 10239.407062530518, \"min\": 10239.407062530518}}, \"EndTime\": 1538409427.183056, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409416.943411}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:07 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.708830145 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:07 INFO 140196492334912] #progress_metric: host=algo-4, completed 61 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:08 INFO 139663579178816] Epoch[155] Batch[0] avg_epoch_loss=2.314267\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:08 INFO 140252856112960] Epoch[153] Batch[10] avg_epoch_loss=2.507358\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:08 INFO 140252856112960] Epoch[153] Batch [10]#011Speed: 118.53 samples/sec#011loss=2.526086\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:08 INFO 140252856112960] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12001.092910766602, \"sum\": 12001.092910766602, \"min\": 12001.092910766602}}, \"EndTime\": 1538409428.371757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409416.37041}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:08 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.572396915 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:08 INFO 140252856112960] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:09 INFO 140196492334912] Epoch[153] Batch[0] avg_epoch_loss=2.533538\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:09 INFO 140252856112960] Epoch[154] Batch[0] avg_epoch_loss=2.144162\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:10 INFO 140624803325760] Epoch[157] Batch[5] avg_epoch_loss=2.443313\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:10 INFO 140624803325760] Epoch[157] Batch [5]#011Speed: 121.21 samples/sec#011loss=2.443313\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:13 INFO 139663579178816] Epoch[155] Batch[5] avg_epoch_loss=2.158193\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:13 INFO 139663579178816] Epoch[155] Batch [5]#011Speed: 124.15 samples/sec#011loss=2.158193\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:13 INFO 140196492334912] Epoch[153] Batch[5] avg_epoch_loss=2.270111\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:13 INFO 140196492334912] Epoch[153] Batch [5]#011Speed: 153.35 samples/sec#011loss=2.270111\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:14 INFO 140624803325760] Epoch[157] Batch[10] avg_epoch_loss=2.419352\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:14 INFO 140624803325760] Epoch[157] Batch [10]#011Speed: 148.87 samples/sec#011loss=2.390599\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:14 INFO 140624803325760] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11194.310903549194, \"sum\": 11194.310903549194, \"min\": 11194.310903549194}}, \"EndTime\": 1538409434.949598, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409423.755035}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:14 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.039789007 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:14 INFO 140624803325760] #progress_metric: host=algo-3, completed 63 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:15 INFO 140252856112960] Epoch[154] Batch[5] avg_epoch_loss=2.043804\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:15 INFO 140252856112960] Epoch[154] Batch [5]#011Speed: 120.72 samples/sec#011loss=2.043804\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:17 INFO 140196492334912] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10274.009943008423, \"sum\": 10274.009943008423, \"min\": 10274.009943008423}}, \"EndTime\": 1538409437.457385, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409427.183128}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:17 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.982939579 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:17 INFO 140196492334912] #progress_metric: host=algo-4, completed 61 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:19 INFO 140252856112960] processed a total of 1217 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10762.771844863892, \"sum\": 10762.771844863892, \"min\": 10762.771844863892}}, \"EndTime\": 1538409439.13486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409428.371841}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:19 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.073362312 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:19 INFO 140252856112960] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:17 INFO 140624803325760] Epoch[158] Batch[0] avg_epoch_loss=1.949334\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:17 INFO 139663579178816] Epoch[155] Batch[10] avg_epoch_loss=2.289263\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:17 INFO 139663579178816] Epoch[155] Batch [10]#011Speed: 151.58 samples/sec#011loss=2.446546\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:17 INFO 139663579178816] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11012.830972671509, \"sum\": 11012.830972671509, \"min\": 11012.830972671509}}, \"EndTime\": 1538409437.947041, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409426.933967}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.406123013 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:19 INFO 140196492334912] Epoch[154] Batch[0] avg_epoch_loss=2.011167\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:20 INFO 139663579178816] Epoch[156] Batch[0] avg_epoch_loss=2.122639\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:20 INFO 140252856112960] Epoch[155] Batch[0] avg_epoch_loss=2.055523\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:21 INFO 140624803325760] Epoch[158] Batch[5] avg_epoch_loss=1.929254\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:21 INFO 140624803325760] Epoch[158] Batch [5]#011Speed: 150.98 samples/sec#011loss=1.929254\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:23 INFO 140196492334912] Epoch[154] Batch[5] avg_epoch_loss=2.006965\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:23 INFO 140196492334912] Epoch[154] Batch [5]#011Speed: 154.98 samples/sec#011loss=2.006965\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:24 INFO 139663579178816] Epoch[156] Batch[5] avg_epoch_loss=1.910465\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:24 INFO 139663579178816] Epoch[156] Batch [5]#011Speed: 155.81 samples/sec#011loss=1.910465\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:25 INFO 140624803325760] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10479.098081588745, \"sum\": 10479.098081588745, \"min\": 10479.098081588745}}, \"EndTime\": 1538409445.429031, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409434.949682}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:25 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.619514501 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:25 INFO 140624803325760] #progress_metric: host=algo-3, completed 63 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:26 INFO 140252856112960] Epoch[155] Batch[5] avg_epoch_loss=2.004352\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:26 INFO 140252856112960] Epoch[155] Batch [5]#011Speed: 119.89 samples/sec#011loss=2.004352\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:27 INFO 140624803325760] Epoch[159] Batch[0] avg_epoch_loss=2.433982\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:29 INFO 140252856112960] processed a total of 1217 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10826.814889907837, \"sum\": 10826.814889907837, \"min\": 10826.814889907837}}, \"EndTime\": 1538409449.962033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409439.134969}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:29 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.404702983 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:29 INFO 140252856112960] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:57:27 INFO 140196492334912] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10257.946014404297, \"sum\": 10257.946014404297, \"min\": 10257.946014404297}}, \"EndTime\": 1538409447.715695, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409437.457463}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:27 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.610117572 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:27 INFO 140196492334912] #progress_metric: host=algo-4, completed 62 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:29 INFO 139663579178816] Epoch[156] Batch[10] avg_epoch_loss=2.026623\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:29 INFO 139663579178816] Epoch[156] Batch [10]#011Speed: 122.77 samples/sec#011loss=2.166013\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:29 INFO 139663579178816] processed a total of 1398 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11723.646879196167, \"sum\": 11723.646879196167, \"min\": 11723.646879196167}}, \"EndTime\": 1538409449.671048, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409437.947111}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.245317344 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:30 INFO 140196492334912] Epoch[155] Batch[0] avg_epoch_loss=2.146374\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:31 INFO 139663579178816] Epoch[157] Batch[0] avg_epoch_loss=1.852858\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:32 INFO 140624803325760] Epoch[159] Batch[5] avg_epoch_loss=2.473595\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:32 INFO 140624803325760] Epoch[159] Batch [5]#011Speed: 152.59 samples/sec#011loss=2.473595\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:31 INFO 140252856112960] Epoch[156] Batch[0] avg_epoch_loss=2.621320\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:34 INFO 140196492334912] Epoch[155] Batch[5] avg_epoch_loss=2.442253\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:34 INFO 140196492334912] Epoch[155] Batch [5]#011Speed: 144.71 samples/sec#011loss=2.442253\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:36 INFO 139663579178816] Epoch[157] Batch[5] avg_epoch_loss=2.118199\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:36 INFO 139663579178816] Epoch[157] Batch [5]#011Speed: 120.50 samples/sec#011loss=2.118199\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:37 INFO 140252856112960] Epoch[156] Batch[5] avg_epoch_loss=2.339841\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:37 INFO 140252856112960] Epoch[156] Batch [5]#011Speed: 120.05 samples/sec#011loss=2.339841\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:38 INFO 140196492334912] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10763.89217376709, \"sum\": 10763.89217376709, \"min\": 10763.89217376709}}, \"EndTime\": 1538409458.479893, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409447.715773}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:38 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.592342932 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:38 INFO 140196492334912] #progress_metric: host=algo-4, completed 62 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:37 INFO 140624803325760] Epoch[159] Batch[10] avg_epoch_loss=2.530801\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:37 INFO 140624803325760] Epoch[159] Batch [10]#011Speed: 118.19 samples/sec#011loss=2.599449\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:37 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12051.378011703491, \"sum\": 12051.378011703491, \"min\": 12051.378011703491}}, \"EndTime\": 1538409457.480756, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409445.429121}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.036281306 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 64 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:39 INFO 140624803325760] Epoch[160] Batch[0] avg_epoch_loss=2.218463\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:40 INFO 140196492334912] Epoch[156] Batch[0] avg_epoch_loss=2.156774\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:40 INFO 139663579178816] Epoch[157] Batch[10] avg_epoch_loss=1.903576\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:40 INFO 139663579178816] Epoch[157] Batch [10]#011Speed: 150.92 samples/sec#011loss=1.646029\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:40 INFO 139663579178816] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11173.017978668213, \"sum\": 11173.017978668213, \"min\": 11173.017978668213}}, \"EndTime\": 1538409460.844398, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409449.671104}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:40 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.409171699 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:40 INFO 139663579178816] #progress_metric: host=algo-2, completed 63 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:41 INFO 140252856112960] Epoch[156] Batch[10] avg_epoch_loss=2.400407\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:41 INFO 140252856112960] Epoch[156] Batch [10]#011Speed: 144.20 samples/sec#011loss=2.473085\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:41 INFO 140252856112960] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11485.047101974487, \"sum\": 11485.047101974487, \"min\": 11485.047101974487}}, \"EndTime\": 1538409461.447424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409449.962123}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:41 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.189630163 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:41 INFO 140252856112960] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:43 INFO 139663579178816] Epoch[158] Batch[0] avg_epoch_loss=2.023734\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:43 INFO 140252856112960] Epoch[157] Batch[0] avg_epoch_loss=2.074717\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:44 INFO 140624803325760] Epoch[160] Batch[5] avg_epoch_loss=1.976779\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:44 INFO 140624803325760] Epoch[160] Batch [5]#011Speed: 121.02 samples/sec#011loss=1.976779\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:45 INFO 140196492334912] Epoch[156] Batch[5] avg_epoch_loss=1.964761\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:45 INFO 140196492334912] Epoch[156] Batch [5]#011Speed: 146.63 samples/sec#011loss=1.964761\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:47 INFO 139663579178816] Epoch[158] Batch[5] avg_epoch_loss=1.997995\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:47 INFO 139663579178816] Epoch[158] Batch [5]#011Speed: 155.57 samples/sec#011loss=1.997995\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:48 INFO 140624803325760] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10707.605838775635, \"sum\": 10707.605838775635, \"min\": 10707.605838775635}}, \"EndTime\": 1538409468.188703, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409457.480842}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.072890569 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:47 INFO 140252856112960] Epoch[157] Batch[5] avg_epoch_loss=1.942094\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:47 INFO 140252856112960] Epoch[157] Batch [5]#011Speed: 153.16 samples/sec#011loss=1.942094\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:49 INFO 140196492334912] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10696.258068084717, \"sum\": 10696.258068084717, \"min\": 10696.258068084717}}, \"EndTime\": 1538409469.176458, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409458.479967}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:49 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.357922017 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:49 INFO 140196492334912] #progress_metric: host=algo-4, completed 62 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:51 INFO 139663579178816] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10166.620969772339, \"sum\": 10166.620969772339, \"min\": 10166.620969772339}}, \"EndTime\": 1538409471.01132, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409460.844469}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:51 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.802471429 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:51 INFO 139663579178816] #progress_metric: host=algo-2, completed 63 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:51 INFO 140252856112960] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10405.8518409729, \"sum\": 10405.8518409729, \"min\": 10405.8518409729}}, \"EndTime\": 1538409471.853574, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409461.447493}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.835137479 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:57:49 INFO 140624803325760] Epoch[161] Batch[0] avg_epoch_loss=2.095472\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:51 INFO 140196492334912] Epoch[157] Batch[0] avg_epoch_loss=1.969302\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:53 INFO 139663579178816] Epoch[159] Batch[0] avg_epoch_loss=2.037702\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:54 INFO 140252856112960] Epoch[158] Batch[0] avg_epoch_loss=1.863320\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:55 INFO 140624803325760] Epoch[161] Batch[5] avg_epoch_loss=1.817302\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:55 INFO 140624803325760] Epoch[161] Batch [5]#011Speed: 120.89 samples/sec#011loss=1.817302\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:55 INFO 140196492334912] Epoch[157] Batch[5] avg_epoch_loss=1.914466\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:55 INFO 140196492334912] Epoch[157] Batch [5]#011Speed: 146.77 samples/sec#011loss=1.914466\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:57 INFO 139663579178816] Epoch[159] Batch[5] avg_epoch_loss=1.966952\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:57:57 INFO 139663579178816] Epoch[159] Batch [5]#011Speed: 149.28 samples/sec#011loss=1.966952\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:58 INFO 140624803325760] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10692.454099655151, \"sum\": 10692.454099655151, \"min\": 10692.454099655151}}, \"EndTime\": 1538409478.881519, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409468.188777}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.622885132 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:57:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:58 INFO 140252856112960] Epoch[158] Batch[5] avg_epoch_loss=1.894354\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:57:58 INFO 140252856112960] Epoch[158] Batch [5]#011Speed: 152.96 samples/sec#011loss=1.894354\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:59 INFO 140196492334912] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10714.095830917358, \"sum\": 10714.095830917358, \"min\": 10714.095830917358}}, \"EndTime\": 1538409479.890863, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409469.176534}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.374165913 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:57:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 63 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:00 INFO 140624803325760] Epoch[162] Batch[0] avg_epoch_loss=2.284898\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:02 INFO 140252856112960] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10380.5091381073, \"sum\": 10380.5091381073, \"min\": 10380.5091381073}}, \"EndTime\": 1538409482.234388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409471.85365}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.283699785 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:02 INFO 140196492334912] Epoch[158] Batch[0] avg_epoch_loss=2.013170\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:03 INFO 139663579178816] Epoch[159] Batch[10] avg_epoch_loss=1.941905\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:03 INFO 139663579178816] Epoch[159] Batch [10]#011Speed: 116.96 samples/sec#011loss=1.911849\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:03 INFO 139663579178816] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12120.991945266724, \"sum\": 12120.991945266724, \"min\": 12120.991945266724}}, \"EndTime\": 1538409483.13262, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409471.011397}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:03 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=106.838532838 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:03 INFO 139663579178816] #progress_metric: host=algo-2, completed 64 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:04 INFO 139663579178816] Epoch[160] Batch[0] avg_epoch_loss=2.537394\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:04 INFO 140252856112960] Epoch[159] Batch[0] avg_epoch_loss=2.286786\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:05 INFO 140624803325760] Epoch[162] Batch[5] avg_epoch_loss=2.284485\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:05 INFO 140624803325760] Epoch[162] Batch [5]#011Speed: 121.23 samples/sec#011loss=2.284485\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:06 INFO 140196492334912] Epoch[158] Batch[5] avg_epoch_loss=2.277514\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:06 INFO 140196492334912] Epoch[158] Batch [5]#011Speed: 149.63 samples/sec#011loss=2.277514\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:08 INFO 140252856112960] Epoch[159] Batch[5] avg_epoch_loss=2.217907\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:08 INFO 140252856112960] Epoch[159] Batch [5]#011Speed: 152.67 samples/sec#011loss=2.217907\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:10 INFO 140624803325760] Epoch[162] Batch[10] avg_epoch_loss=2.181087\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:10 INFO 140624803325760] Epoch[162] Batch [10]#011Speed: 146.57 samples/sec#011loss=2.057010\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:10 INFO 140624803325760] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11308.631896972656, \"sum\": 11308.631896972656, \"min\": 11308.631896972656}}, \"EndTime\": 1538409490.190501, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409478.881606}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.751005374 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 65 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:09 INFO 139663579178816] Epoch[160] Batch[5] avg_epoch_loss=2.274930\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:09 INFO 139663579178816] Epoch[160] Batch [5]#011Speed: 122.94 samples/sec#011loss=2.274930\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:12 INFO 140196492334912] Epoch[158] Batch[10] avg_epoch_loss=2.284303\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:12 INFO 140196492334912] Epoch[158] Batch [10]#011Speed: 115.36 samples/sec#011loss=2.292449\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:12 INFO 140196492334912] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12239.202976226807, \"sum\": 12239.202976226807, \"min\": 12239.202976226807}}, \"EndTime\": 1538409492.130384, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409479.89094}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:12 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.82582748 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:12 INFO 140196492334912] #progress_metric: host=algo-4, completed 63 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:12 INFO 140624803325760] Epoch[163] Batch[0] avg_epoch_loss=1.993424\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:13 INFO 139663579178816] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10655.734062194824, \"sum\": 10655.734062194824, \"min\": 10655.734062194824}}, \"EndTime\": 1538409493.788643, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409483.132689}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.64959546 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:13 INFO 140196492334912] Epoch[159] Batch[0] avg_epoch_loss=2.080143\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:14 INFO 140252856112960] Epoch[159] Batch[10] avg_epoch_loss=1.807137\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:14 INFO 140252856112960] Epoch[159] Batch [10]#011Speed: 113.71 samples/sec#011loss=1.314213\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:14 INFO 140252856112960] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12191.01595878601, \"sum\": 12191.01595878601, \"min\": 12191.01595878601}}, \"EndTime\": 1538409494.425758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409482.234464}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.881074889 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:15 INFO 139663579178816] Epoch[161] Batch[0] avg_epoch_loss=1.930369\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:16 INFO 140252856112960] Epoch[160] Batch[0] avg_epoch_loss=2.091983\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:16 INFO 140624803325760] Epoch[163] Batch[5] avg_epoch_loss=2.083265\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:16 INFO 140624803325760] Epoch[163] Batch [5]#011Speed: 152.55 samples/sec#011loss=2.083265\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 15:58:19 INFO 140196492334912] Epoch[159] Batch[5] avg_epoch_loss=2.032394\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:19 INFO 140196492334912] Epoch[159] Batch [5]#011Speed: 116.50 samples/sec#011loss=2.032394\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:20 INFO 139663579178816] Epoch[161] Batch[5] avg_epoch_loss=1.833056\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:20 INFO 139663579178816] Epoch[161] Batch [5]#011Speed: 124.15 samples/sec#011loss=1.833056\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:22 INFO 140624803325760] Epoch[163] Batch[10] avg_epoch_loss=2.013788\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:22 INFO 140624803325760] Epoch[163] Batch [10]#011Speed: 118.49 samples/sec#011loss=1.930416\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:22 INFO 140624803325760] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11995.367050170898, \"sum\": 11995.367050170898, \"min\": 11995.367050170898}}, \"EndTime\": 1538409502.186198, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409490.190583}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:22 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=107.206896159 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:22 INFO 140624803325760] #progress_metric: host=algo-3, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:21 INFO 140252856112960] Epoch[160] Batch[5] avg_epoch_loss=1.940091\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:21 INFO 140252856112960] Epoch[160] Batch [5]#011Speed: 117.64 samples/sec#011loss=1.940091\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:23 INFO 140196492334912] Epoch[159] Batch[10] avg_epoch_loss=1.960427\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:23 INFO 140196492334912] Epoch[159] Batch [10]#011Speed: 144.50 samples/sec#011loss=1.874066\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:23 INFO 140196492334912] processed a total of 1348 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11616.065979003906, \"sum\": 11616.065979003906, \"min\": 11616.065979003906}}, \"EndTime\": 1538409503.746801, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409492.130487}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:23 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.044872345 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:23 INFO 140196492334912] #progress_metric: host=algo-4, completed 64 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:23 INFO 140624803325760] Epoch[164] Batch[0] avg_epoch_loss=2.076928\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:24 INFO 139663579178816] Epoch[161] Batch[10] avg_epoch_loss=2.002310\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:24 INFO 139663579178816] Epoch[161] Batch [10]#011Speed: 153.50 samples/sec#011loss=2.205414\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:24 INFO 139663579178816] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10941.606044769287, \"sum\": 10941.606044769287, \"min\": 10941.606044769287}}, \"EndTime\": 1538409504.730552, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409493.788718}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:24 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.547885163 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:24 INFO 139663579178816] #progress_metric: host=algo-2, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:26 INFO 140196492334912] Epoch[160] Batch[0] avg_epoch_loss=1.891943\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:25 INFO 140252856112960] Epoch[160] Batch[10] avg_epoch_loss=1.956920\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:25 INFO 140252856112960] Epoch[160] Batch [10]#011Speed: 144.81 samples/sec#011loss=1.977116\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:25 INFO 140252856112960] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11541.912078857422, \"sum\": 11541.912078857422, \"min\": 11541.912078857422}}, \"EndTime\": 1538409505.967975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409494.425829}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.010938404 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:27 INFO 139663579178816] Epoch[162] Batch[0] avg_epoch_loss=1.870225\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:29 INFO 140624803325760] Epoch[164] Batch[5] avg_epoch_loss=2.018153\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:29 INFO 140624803325760] Epoch[164] Batch [5]#011Speed: 121.97 samples/sec#011loss=2.018153\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:28 INFO 140252856112960] Epoch[161] Batch[0] avg_epoch_loss=1.640895\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:31 INFO 139663579178816] Epoch[162] Batch[5] avg_epoch_loss=1.917433\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:31 INFO 139663579178816] Epoch[162] Batch [5]#011Speed: 157.02 samples/sec#011loss=1.917433\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:32 INFO 140252856112960] Epoch[161] Batch[5] avg_epoch_loss=1.873567\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:32 INFO 140252856112960] Epoch[161] Batch [5]#011Speed: 152.65 samples/sec#011loss=1.873567\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:30 INFO 140196492334912] Epoch[160] Batch[5] avg_epoch_loss=2.034906\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:30 INFO 140196492334912] Epoch[160] Batch [5]#011Speed: 151.10 samples/sec#011loss=2.034906\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:33 INFO 140624803325760] Epoch[164] Batch[10] avg_epoch_loss=1.778336\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:33 INFO 140624803325760] Epoch[164] Batch [10]#011Speed: 146.65 samples/sec#011loss=1.490555\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:33 INFO 140624803325760] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11243.023872375488, \"sum\": 11243.023872375488, \"min\": 11243.023872375488}}, \"EndTime\": 1538409513.429557, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409502.186286}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:33 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.558642416 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:33 INFO 140624803325760] #progress_metric: host=algo-3, completed 66 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:34 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10103.166103363037, \"sum\": 10103.166103363037, \"min\": 10103.166103363037}}, \"EndTime\": 1538409514.834013, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409504.730622}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:34 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=126.394646468 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:34 INFO 139663579178816] #progress_metric: host=algo-2, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:35 INFO 140196492334912] Epoch[160] Batch[10] avg_epoch_loss=2.011958\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:35 INFO 140196492334912] Epoch[160] Batch [10]#011Speed: 116.60 samples/sec#011loss=1.984421\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:35 INFO 140196492334912] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12128.062009811401, \"sum\": 12128.062009811401, \"min\": 12128.062009811401}}, \"EndTime\": 1538409515.875201, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409503.746888}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.858509092 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 64 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:35 INFO 140624803325760] Epoch[165] Batch[0] avg_epoch_loss=2.082023\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:36 INFO 140252856112960] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10443.38607788086, \"sum\": 10443.38607788086, \"min\": 10443.38607788086}}, \"EndTime\": 1538409516.411658, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409505.968045}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:36 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.510998304 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:36 INFO 140252856112960] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:37 INFO 139663579178816] Epoch[163] Batch[0] avg_epoch_loss=1.856128\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:37 INFO 140196492334912] Epoch[161] Batch[0] avg_epoch_loss=2.031331\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:38 INFO 140252856112960] Epoch[162] Batch[0] avg_epoch_loss=2.049548\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 15:58:39 INFO 140624803325760] Epoch[165] Batch[5] avg_epoch_loss=2.012758\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:39 INFO 140624803325760] Epoch[165] Batch [5]#011Speed: 152.94 samples/sec#011loss=2.012758\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:41 INFO 139663579178816] Epoch[163] Batch[5] avg_epoch_loss=2.056857\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:41 INFO 139663579178816] Epoch[163] Batch [5]#011Speed: 155.85 samples/sec#011loss=2.056857\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:43 INFO 140196492334912] Epoch[161] Batch[5] avg_epoch_loss=1.896962\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:43 INFO 140196492334912] Epoch[161] Batch [5]#011Speed: 115.82 samples/sec#011loss=1.896962\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:43 INFO 140252856112960] Epoch[162] Batch[5] avg_epoch_loss=1.981585\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:43 INFO 140252856112960] Epoch[162] Batch [5]#011Speed: 150.64 samples/sec#011loss=1.981585\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:43 INFO 140624803325760] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10367.729902267456, \"sum\": 10367.729902267456, \"min\": 10367.729902267456}}, \"EndTime\": 1538409523.79762, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409513.429642}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.81878764 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 66 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:46 INFO 140624803325760] Epoch[166] Batch[0] avg_epoch_loss=2.078534\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:46 INFO 139663579178816] Epoch[163] Batch[10] avg_epoch_loss=2.236011\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:46 INFO 139663579178816] Epoch[163] Batch [10]#011Speed: 120.90 samples/sec#011loss=2.450996\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:46 INFO 139663579178816] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11709.280014038086, \"sum\": 11709.280014038086, \"min\": 11709.280014038086}}, \"EndTime\": 1538409526.543608, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409514.83409}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:46 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.826302926 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:46 INFO 139663579178816] #progress_metric: host=algo-2, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:47 INFO 140196492334912] Epoch[161] Batch[10] avg_epoch_loss=2.242716\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:47 INFO 140196492334912] Epoch[161] Batch [10]#011Speed: 144.52 samples/sec#011loss=2.657620\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:47 INFO 140196492334912] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11623.215913772583, \"sum\": 11623.215913772583, \"min\": 11623.215913772583}}, \"EndTime\": 1538409527.498742, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409515.875285}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.671870445 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 64 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:48 INFO 139663579178816] Epoch[164] Batch[0] avg_epoch_loss=2.147679\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:48 INFO 140252856112960] Epoch[162] Batch[10] avg_epoch_loss=1.958042\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:48 INFO 140252856112960] Epoch[162] Batch [10]#011Speed: 117.95 samples/sec#011loss=1.929790\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:48 INFO 140252856112960] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12037.461996078491, \"sum\": 12037.461996078491, \"min\": 12037.461996078491}}, \"EndTime\": 1538409528.449439, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409516.411735}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:48 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.995389425 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:48 INFO 140252856112960] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:49 INFO 140196492334912] Epoch[162] Batch[0] avg_epoch_loss=2.088515\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:50 INFO 140252856112960] Epoch[163] Batch[0] avg_epoch_loss=1.736175\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:50 INFO 140624803325760] Epoch[166] Batch[5] avg_epoch_loss=2.024822\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:50 INFO 140624803325760] Epoch[166] Batch [5]#011Speed: 153.67 samples/sec#011loss=2.024822\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:53 INFO 139663579178816] Epoch[164] Batch[5] avg_epoch_loss=2.210026\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:53 INFO 139663579178816] Epoch[164] Batch [5]#011Speed: 123.52 samples/sec#011loss=2.210026\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:54 INFO 140196492334912] Epoch[162] Batch[5] avg_epoch_loss=2.035173\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:54 INFO 140196492334912] Epoch[162] Batch [5]#011Speed: 150.77 samples/sec#011loss=2.035173\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:55 INFO 140624803325760] Epoch[166] Batch[10] avg_epoch_loss=2.037430\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:55 INFO 140624803325760] Epoch[166] Batch [10]#011Speed: 119.35 samples/sec#011loss=2.052560\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:55 INFO 140624803325760] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11895.543098449707, \"sum\": 11895.543098449707, \"min\": 11895.543098449707}}, \"EndTime\": 1538409535.6935, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409523.797704}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:55 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.526847748 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:55 INFO 140624803325760] #progress_metric: host=algo-3, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:55 INFO 140252856112960] Epoch[163] Batch[5] avg_epoch_loss=1.921691\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:55 INFO 140252856112960] Epoch[163] Batch [5]#011Speed: 120.39 samples/sec#011loss=1.921691\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:57 INFO 139663579178816] Epoch[164] Batch[10] avg_epoch_loss=1.889011\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:57 INFO 139663579178816] Epoch[164] Batch [10]#011Speed: 150.83 samples/sec#011loss=1.503793\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:57 INFO 139663579178816] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11048.7380027771, \"sum\": 11048.7380027771, \"min\": 11048.7380027771}}, \"EndTime\": 1538409537.592652, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409526.543694}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.297360708 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 66 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:58:57 INFO 140624803325760] Epoch[167] Batch[0] avg_epoch_loss=2.004562\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:59 INFO 140252856112960] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10782.509803771973, \"sum\": 10782.509803771973, \"min\": 10782.509803771973}}, \"EndTime\": 1538409539.232214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409528.449494}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:59 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.64923135 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:58:59 INFO 140252856112960] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:58:59 INFO 139663579178816] Epoch[165] Batch[0] avg_epoch_loss=2.456645\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:59 INFO 140196492334912] Epoch[162] Batch[10] avg_epoch_loss=2.103315\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:59 INFO 140196492334912] Epoch[162] Batch [10]#011Speed: 114.40 samples/sec#011loss=2.185085\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:59 INFO 140196492334912] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12259.324073791504, \"sum\": 12259.324073791504, \"min\": 12259.324073791504}}, \"EndTime\": 1538409539.7584, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409527.498824}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.285301994 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:58:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 65 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:00 INFO 140252856112960] Epoch[164] Batch[0] avg_epoch_loss=1.869419\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:01 INFO 140196492334912] Epoch[163] Batch[0] avg_epoch_loss=2.091031\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:02 INFO 140624803325760] Epoch[167] Batch[5] avg_epoch_loss=1.990083\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:02 INFO 140624803325760] Epoch[167] Batch [5]#011Speed: 120.06 samples/sec#011loss=1.990083\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:04 INFO 139663579178816] Epoch[165] Batch[5] avg_epoch_loss=2.060910\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:04 INFO 139663579178816] Epoch[165] Batch [5]#011Speed: 153.55 samples/sec#011loss=2.060910\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:06 INFO 140252856112960] Epoch[164] Batch[5] avg_epoch_loss=1.906417\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:06 INFO 140252856112960] Epoch[164] Batch [5]#011Speed: 119.23 samples/sec#011loss=1.906417\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:06 INFO 140196492334912] Epoch[163] Batch[5] avg_epoch_loss=2.017484\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:06 INFO 140196492334912] Epoch[163] Batch [5]#011Speed: 115.81 samples/sec#011loss=2.017484\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:06 INFO 140624803325760] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10804.800033569336, \"sum\": 10804.800033569336, \"min\": 10804.800033569336}}, \"EndTime\": 1538409546.498635, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409535.693587}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:06 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.27915601 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:06 INFO 140624803325760] #progress_metric: host=algo-3, completed 67 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:07 INFO 139663579178816] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10221.092939376831, \"sum\": 10221.092939376831, \"min\": 10221.092939376831}}, \"EndTime\": 1538409547.814044, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409537.592722}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:07 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.468819795 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:07 INFO 139663579178816] #progress_metric: host=algo-2, completed 66 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:08 INFO 140624803325760] Epoch[168] Batch[0] avg_epoch_loss=1.724033\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:10 INFO 140252856112960] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10787.508010864258, \"sum\": 10787.508010864258, \"min\": 10787.508010864258}}, \"EndTime\": 1538409550.019981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409539.232274}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:10 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.151674847 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:10 INFO 140252856112960] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:10 INFO 139663579178816] Epoch[166] Batch[0] avg_epoch_loss=1.685964\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:10 INFO 140196492334912] processed a total of 1226 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11104.780912399292, \"sum\": 11104.780912399292, \"min\": 11104.780912399292}}, \"EndTime\": 1538409550.863516, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409539.758489}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.401604315 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 65 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:14 INFO 139663579178816] Epoch[166] Batch[5] avg_epoch_loss=1.811448\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:14 INFO 139663579178816] Epoch[166] Batch [5]#011Speed: 154.53 samples/sec#011loss=1.811448\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 15:59:11 INFO 140252856112960] Epoch[165] Batch[0] avg_epoch_loss=1.806842\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:12 INFO 140196492334912] Epoch[164] Batch[0] avg_epoch_loss=1.927571\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:13 INFO 140624803325760] Epoch[168] Batch[5] avg_epoch_loss=1.896586\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:13 INFO 140624803325760] Epoch[168] Batch [5]#011Speed: 120.28 samples/sec#011loss=1.896586\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:16 INFO 140252856112960] Epoch[165] Batch[5] avg_epoch_loss=1.893806\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:16 INFO 140252856112960] Epoch[165] Batch [5]#011Speed: 120.14 samples/sec#011loss=1.893806\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:17 INFO 140196492334912] Epoch[164] Batch[5] avg_epoch_loss=1.858090\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:17 INFO 140196492334912] Epoch[164] Batch [5]#011Speed: 117.73 samples/sec#011loss=1.858090\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:17 INFO 140624803325760] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10823.742151260376, \"sum\": 10823.742151260376, \"min\": 10823.742151260376}}, \"EndTime\": 1538409557.322741, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409546.498738}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:17 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.670049591 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:17 INFO 140624803325760] #progress_metric: host=algo-3, completed 67 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:18 INFO 139663579178816] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10249.787092208862, \"sum\": 10249.787092208862, \"min\": 10249.787092208862}}, \"EndTime\": 1538409558.064132, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409547.814119}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:18 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.6110097 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:18 INFO 139663579178816] #progress_metric: host=algo-2, completed 66 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:18 INFO 140624803325760] Epoch[169] Batch[0] avg_epoch_loss=2.060807\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:20 INFO 139663579178816] Epoch[167] Batch[0] avg_epoch_loss=1.876359\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:21 INFO 140252856112960] Epoch[165] Batch[10] avg_epoch_loss=2.093610\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:21 INFO 140252856112960] Epoch[165] Batch [10]#011Speed: 146.81 samples/sec#011loss=2.333374\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:21 INFO 140252856112960] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11265.393018722534, \"sum\": 11265.393018722534, \"min\": 11265.393018722534}}, \"EndTime\": 1538409561.285687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409550.020055}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.95278218 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:21 INFO 140196492334912] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10981.528043746948, \"sum\": 10981.528043746948, \"min\": 10981.528043746948}}, \"EndTime\": 1538409561.845399, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409550.863603}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:21 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.182922778 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:21 INFO 140196492334912] #progress_metric: host=algo-4, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:23 INFO 140196492334912] Epoch[165] Batch[0] avg_epoch_loss=1.874465\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:24 INFO 140624803325760] Epoch[169] Batch[5] avg_epoch_loss=2.059700\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:24 INFO 140624803325760] Epoch[169] Batch [5]#011Speed: 120.61 samples/sec#011loss=2.059700\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:23 INFO 140252856112960] Epoch[166] Batch[0] avg_epoch_loss=1.729911\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:24 INFO 139663579178816] Epoch[167] Batch[5] avg_epoch_loss=1.975116\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:24 INFO 139663579178816] Epoch[167] Batch [5]#011Speed: 156.76 samples/sec#011loss=1.975116\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:27 INFO 140252856112960] Epoch[166] Batch[5] avg_epoch_loss=1.875055\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:27 INFO 140252856112960] Epoch[166] Batch [5]#011Speed: 151.10 samples/sec#011loss=1.875055\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:28 INFO 140196492334912] Epoch[165] Batch[5] avg_epoch_loss=1.997668\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:28 INFO 140196492334912] Epoch[165] Batch [5]#011Speed: 122.42 samples/sec#011loss=1.997668\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:28 INFO 140624803325760] Epoch[169] Batch[10] avg_epoch_loss=2.097470\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:28 INFO 140624803325760] Epoch[169] Batch [10]#011Speed: 148.18 samples/sec#011loss=2.142794\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:28 INFO 140624803325760] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11251.10387802124, \"sum\": 11251.10387802124, \"min\": 11251.10387802124}}, \"EndTime\": 1538409568.574206, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409557.322839}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:28 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.209735276 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:28 INFO 140624803325760] #progress_metric: host=algo-3, completed 68 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:29 INFO 139663579178816] Epoch[167] Batch[10] avg_epoch_loss=1.706398\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:29 INFO 139663579178816] Epoch[167] Batch [10]#011Speed: 120.97 samples/sec#011loss=1.383937\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:29 INFO 139663579178816] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11700.440168380737, \"sum\": 11700.440168380737, \"min\": 11700.440168380737}}, \"EndTime\": 1538409569.764872, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409558.064207}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.653040256 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 67 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:30 INFO 140624803325760] Epoch[170] Batch[0] avg_epoch_loss=2.330606\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:31 INFO 139663579178816] Epoch[168] Batch[0] avg_epoch_loss=1.746032\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:32 INFO 140196492334912] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10619.32110786438, \"sum\": 10619.32110786438, \"min\": 10619.32110786438}}, \"EndTime\": 1538409572.46515, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409561.845564}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:32 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.296019798 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:32 INFO 140196492334912] #progress_metric: host=algo-4, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:31 INFO 140252856112960] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10494.661808013916, \"sum\": 10494.661808013916, \"min\": 10494.661808013916}}, \"EndTime\": 1538409571.780638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409561.285755}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:31 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.870168067 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:31 INFO 140252856112960] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:34 INFO 140196492334912] Epoch[166] Batch[0] avg_epoch_loss=1.888829\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:34 INFO 140252856112960] Epoch[167] Batch[0] avg_epoch_loss=1.871266\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:35 INFO 140624803325760] Epoch[170] Batch[5] avg_epoch_loss=1.980335\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:35 INFO 140624803325760] Epoch[170] Batch [5]#011Speed: 157.68 samples/sec#011loss=1.980335\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:36 INFO 139663579178816] Epoch[168] Batch[5] avg_epoch_loss=1.880105\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:36 INFO 139663579178816] Epoch[168] Batch [5]#011Speed: 123.86 samples/sec#011loss=1.880105\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:38 INFO 140252856112960] Epoch[167] Batch[5] avg_epoch_loss=2.026017\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:38 INFO 140252856112960] Epoch[167] Batch [5]#011Speed: 152.37 samples/sec#011loss=2.026017\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:39 INFO 140196492334912] Epoch[166] Batch[5] avg_epoch_loss=2.059346\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:39 INFO 140196492334912] Epoch[166] Batch [5]#011Speed: 123.26 samples/sec#011loss=2.059346\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:40 INFO 140624803325760] Epoch[170] Batch[10] avg_epoch_loss=1.938018\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:40 INFO 140624803325760] Epoch[170] Batch [10]#011Speed: 124.23 samples/sec#011loss=1.887239\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:40 INFO 140624803325760] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11590.595006942749, \"sum\": 11590.595006942749, \"min\": 11590.595006942749}}, \"EndTime\": 1538409580.165133, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409568.574289}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=114.919442065 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 68 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:40 INFO 139663579178816] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10488.030910491943, \"sum\": 10488.030910491943, \"min\": 10488.030910491943}}, \"EndTime\": 1538409580.253203, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409569.764942}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:40 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.849692962 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:40 INFO 139663579178816] #progress_metric: host=algo-2, completed 67 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:41 INFO 140624803325760] Epoch[171] Batch[0] avg_epoch_loss=1.973220\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:42 INFO 140252856112960] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10429.436922073364, \"sum\": 10429.436922073364, \"min\": 10429.436922073364}}, \"EndTime\": 1538409582.21038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409571.780715}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:42 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.098237572 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:42 INFO 140252856112960] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:43 INFO 140196492334912] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10590.397834777832, \"sum\": 10590.397834777832, \"min\": 10590.397834777832}}, \"EndTime\": 1538409583.055932, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409572.465238}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:43 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.729562587 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:43 INFO 140196492334912] #progress_metric: host=algo-4, completed 66 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 15:59:41 INFO 139663579178816] Epoch[169] Batch[0] avg_epoch_loss=1.820474\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:44 INFO 140196492334912] Epoch[167] Batch[0] avg_epoch_loss=2.335965\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:44 INFO 140252856112960] Epoch[168] Batch[0] avg_epoch_loss=1.999805\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:47 INFO 140624803325760] Epoch[171] Batch[5] avg_epoch_loss=1.983497\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:47 INFO 140624803325760] Epoch[171] Batch [5]#011Speed: 118.51 samples/sec#011loss=1.983497\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:47 INFO 139663579178816] Epoch[169] Batch[5] avg_epoch_loss=1.957652\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:47 INFO 139663579178816] Epoch[169] Batch [5]#011Speed: 122.57 samples/sec#011loss=1.957652\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:48 INFO 140252856112960] Epoch[168] Batch[5] avg_epoch_loss=2.041654\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:48 INFO 140252856112960] Epoch[168] Batch [5]#011Speed: 151.71 samples/sec#011loss=2.041654\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:50 INFO 140196492334912] Epoch[167] Batch[5] avg_epoch_loss=1.934102\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:50 INFO 140196492334912] Epoch[167] Batch [5]#011Speed: 115.40 samples/sec#011loss=1.934102\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:51 INFO 139663579178816] Epoch[169] Batch[10] avg_epoch_loss=1.879282\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:51 INFO 139663579178816] Epoch[169] Batch [10]#011Speed: 149.59 samples/sec#011loss=1.785238\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:51 INFO 139663579178816] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11118.826150894165, \"sum\": 11118.826150894165, \"min\": 11118.826150894165}}, \"EndTime\": 1538409591.372334, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409580.253276}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:51 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.827829807 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:51 INFO 139663579178816] #progress_metric: host=algo-2, completed 68 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:51 INFO 140624803325760] Epoch[171] Batch[10] avg_epoch_loss=1.975384\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:51 INFO 140624803325760] Epoch[171] Batch [10]#011Speed: 147.66 samples/sec#011loss=1.965648\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:51 INFO 140624803325760] processed a total of 1366 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11432.698011398315, \"sum\": 11432.698011398315, \"min\": 11432.698011398315}}, \"EndTime\": 1538409591.59817, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409580.165224}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.480539882 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 68 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:53 INFO 140624803325760] Epoch[172] Batch[0] avg_epoch_loss=1.893443\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:54 INFO 140196492334912] Epoch[167] Batch[10] avg_epoch_loss=2.008955\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:54 INFO 140196492334912] Epoch[167] Batch [10]#011Speed: 145.39 samples/sec#011loss=2.098779\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:54 INFO 140196492334912] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11583.041906356812, \"sum\": 11583.041906356812, \"min\": 11583.041906356812}}, \"EndTime\": 1538409594.639342, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409583.056024}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:54 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.821822663 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:54 INFO 140196492334912] #progress_metric: host=algo-4, completed 67 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:52 INFO 140252856112960] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10685.8069896698, \"sum\": 10685.8069896698, \"min\": 10685.8069896698}}, \"EndTime\": 1538409592.896544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409582.210457}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:52 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.409465913 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:52 INFO 140252856112960] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:53 INFO 139663579178816] Epoch[170] Batch[0] avg_epoch_loss=2.257197\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:55 INFO 140252856112960] Epoch[169] Batch[0] avg_epoch_loss=1.978745\u001b[0m\n",
      "\u001b[34m[10/01/2018 15:59:57 INFO 140196492334912] Epoch[168] Batch[0] avg_epoch_loss=2.072460\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:57 INFO 139663579178816] Epoch[170] Batch[5] avg_epoch_loss=2.090487\u001b[0m\n",
      "\u001b[32m[10/01/2018 15:59:57 INFO 139663579178816] Epoch[170] Batch [5]#011Speed: 155.89 samples/sec#011loss=2.090487\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:58 INFO 140624803325760] Epoch[172] Batch[5] avg_epoch_loss=1.940663\u001b[0m\n",
      "\u001b[33m[10/01/2018 15:59:58 INFO 140624803325760] Epoch[172] Batch [5]#011Speed: 152.93 samples/sec#011loss=1.940663\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:59 INFO 140252856112960] Epoch[169] Batch[5] avg_epoch_loss=1.933803\u001b[0m\n",
      "\u001b[31m[10/01/2018 15:59:59 INFO 140252856112960] Epoch[169] Batch [5]#011Speed: 145.16 samples/sec#011loss=1.933803\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:01 INFO 139663579178816] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10130.843877792358, \"sum\": 10130.843877792358, \"min\": 10130.843877792358}}, \"EndTime\": 1538409601.503463, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409591.3724}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=126.345470247 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:01 INFO 140196492334912] Epoch[168] Batch[5] avg_epoch_loss=2.041155\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:01 INFO 140196492334912] Epoch[168] Batch [5]#011Speed: 150.71 samples/sec#011loss=2.041155\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:01 INFO 140624803325760] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10336.9779586792, \"sum\": 10336.9779586792, \"min\": 10336.9779586792}}, \"EndTime\": 1538409601.935484, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409591.598255}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:01 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.277866983 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:01 INFO 140624803325760] #progress_metric: host=algo-3, completed 69 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:03 INFO 139663579178816] Epoch[171] Batch[0] avg_epoch_loss=1.948806\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:04 INFO 140624803325760] Epoch[173] Batch[0] avg_epoch_loss=2.000758\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:05 INFO 140252856112960] Epoch[169] Batch[10] avg_epoch_loss=1.642876\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:05 INFO 140252856112960] Epoch[169] Batch [10]#011Speed: 113.69 samples/sec#011loss=1.293764\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:05 INFO 140252856112960] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12531.934022903442, \"sum\": 12531.934022903442, \"min\": 12531.934022903442}}, \"EndTime\": 1538409605.428785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409592.89662}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:05 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=102.936119779 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:05 INFO 140252856112960] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:06 INFO 140196492334912] Epoch[168] Batch[10] avg_epoch_loss=1.774061\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:06 INFO 140196492334912] Epoch[168] Batch [10]#011Speed: 118.93 samples/sec#011loss=1.453549\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:06 INFO 140196492334912] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12040.334939956665, \"sum\": 12040.334939956665, \"min\": 12040.334939956665}}, \"EndTime\": 1538409606.680014, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409594.639421}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.879439195 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 67 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:07 INFO 140252856112960] Epoch[170] Batch[0] avg_epoch_loss=1.696892\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:07 INFO 139663579178816] Epoch[171] Batch[5] avg_epoch_loss=2.062596\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:07 INFO 139663579178816] Epoch[171] Batch [5]#011Speed: 156.84 samples/sec#011loss=2.062596\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:08 INFO 140196492334912] Epoch[169] Batch[0] avg_epoch_loss=2.060182\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:08 INFO 140624803325760] Epoch[173] Batch[5] avg_epoch_loss=2.004107\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:08 INFO 140624803325760] Epoch[173] Batch [5]#011Speed: 153.89 samples/sec#011loss=2.004107\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:00:12 INFO 140252856112960] Epoch[170] Batch[5] avg_epoch_loss=1.903103\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:12 INFO 140252856112960] Epoch[170] Batch [5]#011Speed: 115.57 samples/sec#011loss=1.903103\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:13 INFO 139663579178816] Epoch[171] Batch[10] avg_epoch_loss=2.199496\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:13 INFO 139663579178816] Epoch[171] Batch [10]#011Speed: 118.38 samples/sec#011loss=2.363776\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:13 INFO 139663579178816] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11828.121185302734, \"sum\": 11828.121185302734, \"min\": 11828.121185302734}}, \"EndTime\": 1538409613.331887, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409601.503537}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.737456889 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:13 INFO 140196492334912] Epoch[169] Batch[5] avg_epoch_loss=1.951356\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:13 INFO 140196492334912] Epoch[169] Batch [5]#011Speed: 116.44 samples/sec#011loss=1.951356\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:13 INFO 140624803325760] Epoch[173] Batch[10] avg_epoch_loss=1.686015\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:13 INFO 140624803325760] Epoch[173] Batch [10]#011Speed: 119.37 samples/sec#011loss=1.304305\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:13 INFO 140624803325760] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11825.544834136963, \"sum\": 11825.544834136963, \"min\": 11825.544834136963}}, \"EndTime\": 1538409613.761378, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409601.935572}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:13 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.323663376 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:13 INFO 140624803325760] #progress_metric: host=algo-3, completed 69 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:14 INFO 139663579178816] Epoch[172] Batch[0] avg_epoch_loss=1.703560\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:15 INFO 140624803325760] Epoch[174] Batch[0] avg_epoch_loss=1.944233\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:17 INFO 140252856112960] Epoch[170] Batch[10] avg_epoch_loss=1.871467\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:17 INFO 140252856112960] Epoch[170] Batch [10]#011Speed: 142.73 samples/sec#011loss=1.833504\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:17 INFO 140252856112960] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11794.676065444946, \"sum\": 11794.676065444946, \"min\": 11794.676065444946}}, \"EndTime\": 1538409617.223785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409605.428861}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.52478723 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:17 INFO 140196492334912] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11114.983797073364, \"sum\": 11114.983797073364, \"min\": 11114.983797073364}}, \"EndTime\": 1538409617.795326, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409606.680096}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:17 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.739886317 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:17 INFO 140196492334912] #progress_metric: host=algo-4, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:19 INFO 140196492334912] Epoch[170] Batch[0] avg_epoch_loss=2.066051\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:19 INFO 140252856112960] Epoch[171] Batch[0] avg_epoch_loss=2.030884\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:20 INFO 139663579178816] Epoch[172] Batch[5] avg_epoch_loss=1.834649\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:20 INFO 139663579178816] Epoch[172] Batch [5]#011Speed: 121.85 samples/sec#011loss=1.834649\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:20 INFO 140624803325760] Epoch[174] Batch[5] avg_epoch_loss=1.834416\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:20 INFO 140624803325760] Epoch[174] Batch [5]#011Speed: 122.83 samples/sec#011loss=1.834416\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:23 INFO 140252856112960] Epoch[171] Batch[5] avg_epoch_loss=1.920921\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:23 INFO 140252856112960] Epoch[171] Batch [5]#011Speed: 151.66 samples/sec#011loss=1.920921\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:24 INFO 139663579178816] Epoch[172] Batch[10] avg_epoch_loss=1.950102\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:24 INFO 139663579178816] Epoch[172] Batch [10]#011Speed: 150.38 samples/sec#011loss=2.088644\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:24 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11157.047986984253, \"sum\": 11157.047986984253, \"min\": 11157.047986984253}}, \"EndTime\": 1538409624.489239, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409613.331963}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:24 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.234219569 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:24 INFO 139663579178816] #progress_metric: host=algo-2, completed 69 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:24 INFO 140624803325760] Epoch[174] Batch[10] avg_epoch_loss=1.995567\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:24 INFO 140624803325760] Epoch[174] Batch [10]#011Speed: 150.39 samples/sec#011loss=2.188949\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:24 INFO 140624803325760] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11064.138889312744, \"sum\": 11064.138889312744, \"min\": 11064.138889312744}}, \"EndTime\": 1538409624.825848, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409613.761464}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:24 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.862730729 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:24 INFO 140624803325760] #progress_metric: host=algo-3, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:24 INFO 140196492334912] Epoch[170] Batch[5] avg_epoch_loss=1.721652\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:24 INFO 140196492334912] Epoch[170] Batch [5]#011Speed: 119.86 samples/sec#011loss=1.721652\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:26 INFO 139663579178816] Epoch[173] Batch[0] avg_epoch_loss=2.076996\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:27 INFO 140624803325760] Epoch[175] Batch[0] avg_epoch_loss=2.044465\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:29 INFO 140196492334912] Epoch[170] Batch[10] avg_epoch_loss=1.677091\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:29 INFO 140196492334912] Epoch[170] Batch [10]#011Speed: 146.93 samples/sec#011loss=1.623617\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:29 INFO 140196492334912] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11363.011121749878, \"sum\": 11363.011121749878, \"min\": 11363.011121749878}}, \"EndTime\": 1538409629.158654, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409617.795404}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.965259086 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:29 INFO 140252856112960] Epoch[171] Batch[10] avg_epoch_loss=1.942681\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:29 INFO 140252856112960] Epoch[171] Batch [10]#011Speed: 118.95 samples/sec#011loss=1.968794\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:29 INFO 140252856112960] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12110.219955444336, \"sum\": 12110.219955444336, \"min\": 12110.219955444336}}, \"EndTime\": 1538409629.334304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409617.223855}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:29 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.263639449 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:29 INFO 140252856112960] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:31 INFO 140624803325760] Epoch[175] Batch[5] avg_epoch_loss=2.017398\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:31 INFO 140624803325760] Epoch[175] Batch [5]#011Speed: 154.23 samples/sec#011loss=2.017398\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:30 INFO 140252856112960] Epoch[172] Batch[0] avg_epoch_loss=2.245531\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:30 INFO 139663579178816] Epoch[173] Batch[5] avg_epoch_loss=1.876427\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:30 INFO 139663579178816] Epoch[173] Batch [5]#011Speed: 155.23 samples/sec#011loss=1.876427\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:31 INFO 140196492334912] Epoch[171] Batch[0] avg_epoch_loss=2.039323\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:34 INFO 139663579178816] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10329.110860824585, \"sum\": 10329.110860824585, \"min\": 10329.110860824585}}, \"EndTime\": 1538409634.818649, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409624.489309}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:34 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.371266224 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:34 INFO 139663579178816] #progress_metric: host=algo-2, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:35 INFO 140196492334912] Epoch[171] Batch[5] avg_epoch_loss=1.939339\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:35 INFO 140196492334912] Epoch[171] Batch [5]#011Speed: 152.24 samples/sec#011loss=1.939339\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:00:35 INFO 140624803325760] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10230.955123901367, \"sum\": 10230.955123901367, \"min\": 10230.955123901367}}, \"EndTime\": 1538409635.057134, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409624.825932}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:35 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.131433999 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:35 INFO 140624803325760] #progress_metric: host=algo-3, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:36 INFO 140252856112960] Epoch[172] Batch[5] avg_epoch_loss=1.972101\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:36 INFO 140252856112960] Epoch[172] Batch [5]#011Speed: 117.63 samples/sec#011loss=1.972101\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:37 INFO 139663579178816] Epoch[174] Batch[0] avg_epoch_loss=2.343493\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:37 INFO 140624803325760] Epoch[176] Batch[0] avg_epoch_loss=2.298332\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:40 INFO 140252856112960] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10982.647895812988, \"sum\": 10982.647895812988, \"min\": 10982.647895812988}}, \"EndTime\": 1538409640.31727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409629.334396}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.999764334 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:41 INFO 140196492334912] Epoch[171] Batch[10] avg_epoch_loss=2.084587\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:41 INFO 140196492334912] Epoch[171] Batch [10]#011Speed: 118.72 samples/sec#011loss=2.258884\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:41 INFO 140196492334912] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11994.230031967163, \"sum\": 11994.230031967163, \"min\": 11994.230031967163}}, \"EndTime\": 1538409641.153173, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409629.158722}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.135790447 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 68 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:41 INFO 139663579178816] Epoch[174] Batch[5] avg_epoch_loss=2.162079\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:41 INFO 139663579178816] Epoch[174] Batch [5]#011Speed: 155.89 samples/sec#011loss=2.162079\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:41 INFO 140624803325760] Epoch[176] Batch[5] avg_epoch_loss=2.319551\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:41 INFO 140624803325760] Epoch[176] Batch [5]#011Speed: 150.69 samples/sec#011loss=2.319551\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:42 INFO 140252856112960] Epoch[173] Batch[0] avg_epoch_loss=2.098445\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:42 INFO 140196492334912] Epoch[172] Batch[0] avg_epoch_loss=2.173942\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:45 INFO 140624803325760] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10456.861019134521, \"sum\": 10456.861019134521, \"min\": 10456.861019134521}}, \"EndTime\": 1538409645.514347, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409635.057224}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:45 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.736738243 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:45 INFO 140624803325760] #progress_metric: host=algo-3, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:44 INFO 139663579178816] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10160.096883773804, \"sum\": 10160.096883773804, \"min\": 10160.096883773804}}, \"EndTime\": 1538409644.979053, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409634.818724}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.883226166 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:47 INFO 139663579178816] Epoch[175] Batch[0] avg_epoch_loss=2.049627\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:47 INFO 140624803325760] Epoch[177] Batch[0] avg_epoch_loss=2.054073\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:47 INFO 140252856112960] Epoch[173] Batch[5] avg_epoch_loss=2.156901\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:47 INFO 140252856112960] Epoch[173] Batch [5]#011Speed: 117.63 samples/sec#011loss=2.156901\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:48 INFO 140196492334912] Epoch[172] Batch[5] avg_epoch_loss=2.020850\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:48 INFO 140196492334912] Epoch[172] Batch [5]#011Speed: 116.47 samples/sec#011loss=2.020850\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:51 INFO 139663579178816] Epoch[175] Batch[5] avg_epoch_loss=2.024716\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:51 INFO 139663579178816] Epoch[175] Batch [5]#011Speed: 148.21 samples/sec#011loss=2.024716\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:52 INFO 140624803325760] Epoch[177] Batch[5] avg_epoch_loss=2.064959\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:52 INFO 140624803325760] Epoch[177] Batch [5]#011Speed: 151.37 samples/sec#011loss=2.064959\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:51 INFO 140252856112960] Epoch[173] Batch[10] avg_epoch_loss=2.134637\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:51 INFO 140252856112960] Epoch[173] Batch [10]#011Speed: 145.71 samples/sec#011loss=2.107920\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:51 INFO 140252856112960] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11517.276048660278, \"sum\": 11517.276048660278, \"min\": 11517.276048660278}}, \"EndTime\": 1538409651.834888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409640.317368}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.306813079 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:52 INFO 140196492334912] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11092.073917388916, \"sum\": 11092.073917388916, \"min\": 11092.073917388916}}, \"EndTime\": 1538409652.245557, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409641.153251}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:52 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.683366703 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:52 INFO 140196492334912] #progress_metric: host=algo-4, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:53 INFO 140196492334912] Epoch[173] Batch[0] avg_epoch_loss=1.826314\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:54 INFO 140252856112960] Epoch[174] Batch[0] avg_epoch_loss=1.884376\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:57 INFO 139663579178816] Epoch[175] Batch[10] avg_epoch_loss=1.920120\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:57 INFO 139663579178816] Epoch[175] Batch [10]#011Speed: 118.77 samples/sec#011loss=1.794605\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:57 INFO 139663579178816] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12070.072889328003, \"sum\": 12070.072889328003, \"min\": 12070.072889328003}}, \"EndTime\": 1538409657.049517, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409644.979131}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.028891282 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:57 INFO 140624803325760] Epoch[177] Batch[10] avg_epoch_loss=2.159816\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:57 INFO 140624803325760] Epoch[177] Batch [10]#011Speed: 119.02 samples/sec#011loss=2.273644\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:57 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12054.22592163086, \"sum\": 12054.22592163086, \"min\": 12054.22592163086}}, \"EndTime\": 1538409657.568916, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409645.514432}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:57 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.669789234 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:57 INFO 140624803325760] #progress_metric: host=algo-3, completed 71 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:00:58 INFO 139663579178816] Epoch[176] Batch[0] avg_epoch_loss=1.926422\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:00:59 INFO 140624803325760] Epoch[178] Batch[0] avg_epoch_loss=1.551948\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:58 INFO 140252856112960] Epoch[174] Batch[5] avg_epoch_loss=1.788182\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:00:58 INFO 140252856112960] Epoch[174] Batch [5]#011Speed: 151.48 samples/sec#011loss=1.788182\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:59 INFO 140196492334912] Epoch[173] Batch[5] avg_epoch_loss=1.892224\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:00:59 INFO 140196492334912] Epoch[173] Batch [5]#011Speed: 117.93 samples/sec#011loss=1.892224\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:02 INFO 140252856112960] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10482.21492767334, \"sum\": 10482.21492767334, \"min\": 10482.21492767334}}, \"EndTime\": 1538409662.317437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409651.834974}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.294142759 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:03 INFO 140196492334912] Epoch[173] Batch[10] avg_epoch_loss=1.926854\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:03 INFO 140196492334912] Epoch[173] Batch [10]#011Speed: 142.19 samples/sec#011loss=1.968411\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:03 INFO 140196492334912] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11594.648122787476, \"sum\": 11594.648122787476, \"min\": 11594.648122787476}}, \"EndTime\": 1538409663.840598, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409652.245647}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:03 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.154377358 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:03 INFO 140196492334912] #progress_metric: host=algo-4, completed 69 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:04 INFO 140624803325760] Epoch[178] Batch[5] avg_epoch_loss=1.905818\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:04 INFO 140624803325760] Epoch[178] Batch [5]#011Speed: 122.50 samples/sec#011loss=1.905818\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:04 INFO 139663579178816] Epoch[176] Batch[5] avg_epoch_loss=1.930737\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:04 INFO 139663579178816] Epoch[176] Batch [5]#011Speed: 119.53 samples/sec#011loss=1.930737\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:07 INFO 139663579178816] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10859.057903289795, \"sum\": 10859.057903289795, \"min\": 10859.057903289795}}, \"EndTime\": 1538409667.908889, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409657.049606}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:07 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.478515447 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:07 INFO 139663579178816] #progress_metric: host=algo-2, completed 70 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:08 INFO 140624803325760] processed a total of 1178 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10626.879930496216, \"sum\": 10626.879930496216, \"min\": 10626.879930496216}}, \"EndTime\": 1538409668.19615, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409657.569016}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.849636875 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 71 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:01:04 INFO 140252856112960] Epoch[175] Batch[0] avg_epoch_loss=2.089454\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:06 INFO 140196492334912] Epoch[174] Batch[0] avg_epoch_loss=1.962351\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:08 INFO 140252856112960] Epoch[175] Batch[5] avg_epoch_loss=2.150026\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:08 INFO 140252856112960] Epoch[175] Batch [5]#011Speed: 152.82 samples/sec#011loss=2.150026\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:09 INFO 139663579178816] Epoch[177] Batch[0] avg_epoch_loss=2.140271\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:09 INFO 140624803325760] Epoch[179] Batch[0] avg_epoch_loss=2.186928\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:10 INFO 140196492334912] Epoch[174] Batch[5] avg_epoch_loss=2.174617\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:10 INFO 140196492334912] Epoch[174] Batch [5]#011Speed: 146.71 samples/sec#011loss=2.174617\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:12 INFO 140252856112960] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10540.320873260498, \"sum\": 10540.320873260498, \"min\": 10540.320873260498}}, \"EndTime\": 1538409672.858101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409662.317523}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:12 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.962486105 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:12 INFO 140252856112960] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:14 INFO 139663579178816] Epoch[177] Batch[5] avg_epoch_loss=2.279227\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:14 INFO 139663579178816] Epoch[177] Batch [5]#011Speed: 119.47 samples/sec#011loss=2.279227\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:15 INFO 140252856112960] Epoch[176] Batch[0] avg_epoch_loss=2.030419\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:15 INFO 140624803325760] Epoch[179] Batch[5] avg_epoch_loss=2.267363\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:15 INFO 140624803325760] Epoch[179] Batch [5]#011Speed: 122.55 samples/sec#011loss=2.267363\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:16 INFO 140196492334912] Epoch[174] Batch[10] avg_epoch_loss=2.464233\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:16 INFO 140196492334912] Epoch[174] Batch [10]#011Speed: 113.07 samples/sec#011loss=2.811772\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:16 INFO 140196492334912] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12481.941938400269, \"sum\": 12481.941938400269, \"min\": 12481.941938400269}}, \"EndTime\": 1538409676.322882, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409663.840685}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:16 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=103.428373696 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:16 INFO 140196492334912] #progress_metric: host=algo-4, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:17 INFO 140196492334912] Epoch[175] Batch[0] avg_epoch_loss=2.222874\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:18 INFO 140624803325760] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10664.22700881958, \"sum\": 10664.22700881958, \"min\": 10664.22700881958}}, \"EndTime\": 1538409678.860765, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409668.196236}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.46193167 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 72 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:19 INFO 139663579178816] Epoch[177] Batch[10] avg_epoch_loss=1.979909\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:19 INFO 139663579178816] Epoch[177] Batch [10]#011Speed: 146.42 samples/sec#011loss=1.620728\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:19 INFO 139663579178816] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11382.629871368408, \"sum\": 11382.629871368408, \"min\": 11382.629871368408}}, \"EndTime\": 1538409679.29183, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409667.908959}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:19 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.559159724 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:19 INFO 139663579178816] #progress_metric: host=algo-2, completed 71 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:19 INFO 140252856112960] Epoch[176] Batch[5] avg_epoch_loss=2.100324\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:19 INFO 140252856112960] Epoch[176] Batch [5]#011Speed: 150.86 samples/sec#011loss=2.100324\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:20 INFO 140624803325760] Epoch[180] Batch[0] avg_epoch_loss=1.953920\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:21 INFO 139663579178816] Epoch[178] Batch[0] avg_epoch_loss=1.940501\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:23 INFO 140196492334912] Epoch[175] Batch[5] avg_epoch_loss=2.187698\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:23 INFO 140196492334912] Epoch[175] Batch [5]#011Speed: 115.40 samples/sec#011loss=2.187698\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:25 INFO 140252856112960] Epoch[176] Batch[10] avg_epoch_loss=2.146022\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:25 INFO 140252856112960] Epoch[176] Batch [10]#011Speed: 117.14 samples/sec#011loss=2.200860\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:25 INFO 140252856112960] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12172.137022018433, \"sum\": 12172.137022018433, \"min\": 12172.137022018433}}, \"EndTime\": 1538409685.030594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409672.858192}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:25 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.388876876 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:25 INFO 140252856112960] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:26 INFO 140252856112960] Epoch[177] Batch[0] avg_epoch_loss=1.891495\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:28 INFO 140196492334912] Epoch[175] Batch[10] avg_epoch_loss=2.003924\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:28 INFO 140196492334912] Epoch[175] Batch [10]#011Speed: 141.47 samples/sec#011loss=1.783395\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:28 INFO 140196492334912] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11734.771013259888, \"sum\": 11734.771013259888, \"min\": 11734.771013259888}}, \"EndTime\": 1538409688.057982, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409676.322965}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:28 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.843334857 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:28 INFO 140196492334912] #progress_metric: host=algo-4, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:25 INFO 139663579178816] Epoch[178] Batch[5] avg_epoch_loss=2.000011\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:25 INFO 139663579178816] Epoch[178] Batch [5]#011Speed: 151.45 samples/sec#011loss=2.000011\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:25 INFO 140624803325760] Epoch[180] Batch[5] avg_epoch_loss=2.019229\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:25 INFO 140624803325760] Epoch[180] Batch [5]#011Speed: 122.19 samples/sec#011loss=2.019229\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:29 INFO 139663579178816] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10413.511037826538, \"sum\": 10413.511037826538, \"min\": 10413.511037826538}}, \"EndTime\": 1538409689.705684, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409679.291916}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:29 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.266607444 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:29 INFO 139663579178816] #progress_metric: host=algo-2, completed 71 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:29 INFO 140624803325760] Epoch[180] Batch[10] avg_epoch_loss=2.051817\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:29 INFO 140624803325760] Epoch[180] Batch [10]#011Speed: 150.60 samples/sec#011loss=2.090923\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:29 INFO 140624803325760] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11079.74100112915, \"sum\": 11079.74100112915, \"min\": 11079.74100112915}}, \"EndTime\": 1538409689.940865, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409678.860864}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:29 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.517675404 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:29 INFO 140624803325760] #progress_metric: host=algo-3, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:30 INFO 140196492334912] Epoch[176] Batch[0] avg_epoch_loss=2.268636\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:32 INFO 140624803325760] Epoch[181] Batch[0] avg_epoch_loss=2.118021\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:32 INFO 140252856112960] Epoch[177] Batch[5] avg_epoch_loss=2.064960\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:32 INFO 140252856112960] Epoch[177] Batch [5]#011Speed: 119.14 samples/sec#011loss=2.064960\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:32 INFO 139663579178816] Epoch[179] Batch[0] avg_epoch_loss=2.107197\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:34 INFO 140196492334912] Epoch[176] Batch[5] avg_epoch_loss=1.971801\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:34 INFO 140196492334912] Epoch[176] Batch [5]#011Speed: 148.30 samples/sec#011loss=1.971801\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:35 INFO 140252856112960] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10961.27200126648, \"sum\": 10961.27200126648, \"min\": 10961.27200126648}}, \"EndTime\": 1538409695.992251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409685.030732}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:35 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.313915039 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:35 INFO 140252856112960] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:36 INFO 139663579178816] Epoch[179] Batch[5] avg_epoch_loss=2.056406\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:36 INFO 139663579178816] Epoch[179] Batch [5]#011Speed: 152.03 samples/sec#011loss=2.056406\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:01:36 INFO 140624803325760] Epoch[181] Batch[5] avg_epoch_loss=1.985710\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:36 INFO 140624803325760] Epoch[181] Batch [5]#011Speed: 154.16 samples/sec#011loss=1.985710\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:37 INFO 140252856112960] Epoch[178] Batch[0] avg_epoch_loss=1.822458\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:40 INFO 140624803325760] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10395.487070083618, \"sum\": 10395.487070083618, \"min\": 10395.487070083618}}, \"EndTime\": 1538409700.336677, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409689.940947}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.397560724 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:40 INFO 140196492334912] Epoch[176] Batch[10] avg_epoch_loss=2.111226\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:40 INFO 140196492334912] Epoch[176] Batch [10]#011Speed: 114.10 samples/sec#011loss=2.278537\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:40 INFO 140196492334912] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12335.515022277832, \"sum\": 12335.515022277832, \"min\": 12335.515022277832}}, \"EndTime\": 1538409700.393826, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409688.058065}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:40 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=104.331863068 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:40 INFO 140196492334912] #progress_metric: host=algo-4, completed 70 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:41 INFO 139663579178816] Epoch[179] Batch[10] avg_epoch_loss=1.960368\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:41 INFO 139663579178816] Epoch[179] Batch [10]#011Speed: 119.95 samples/sec#011loss=1.845122\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:41 INFO 139663579178816] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11974.081039428711, \"sum\": 11974.081039428711, \"min\": 11974.081039428711}}, \"EndTime\": 1538409701.680109, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409689.705774}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.322592765 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:42 INFO 140196492334912] Epoch[177] Batch[0] avg_epoch_loss=1.774927\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:42 INFO 140624803325760] Epoch[182] Batch[0] avg_epoch_loss=1.924103\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:42 INFO 140252856112960] Epoch[178] Batch[5] avg_epoch_loss=2.055822\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:42 INFO 140252856112960] Epoch[178] Batch [5]#011Speed: 121.84 samples/sec#011loss=2.055822\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:43 INFO 139663579178816] Epoch[180] Batch[0] avg_epoch_loss=1.925215\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:46 INFO 140624803325760] Epoch[182] Batch[5] avg_epoch_loss=2.032239\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:46 INFO 140624803325760] Epoch[182] Batch [5]#011Speed: 153.05 samples/sec#011loss=2.032239\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:47 INFO 140252856112960] Epoch[178] Batch[10] avg_epoch_loss=2.258839\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:47 INFO 140252856112960] Epoch[178] Batch [10]#011Speed: 148.16 samples/sec#011loss=2.502459\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:47 INFO 140252856112960] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11163.389921188354, \"sum\": 11163.389921188354, \"min\": 11163.389921188354}}, \"EndTime\": 1538409707.155953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409695.992325}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:47 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.092687011 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:47 INFO 140252856112960] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:47 INFO 140196492334912] Epoch[177] Batch[5] avg_epoch_loss=2.070481\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:47 INFO 140196492334912] Epoch[177] Batch [5]#011Speed: 118.12 samples/sec#011loss=2.070481\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:48 INFO 139663579178816] Epoch[180] Batch[5] avg_epoch_loss=2.049643\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:48 INFO 139663579178816] Epoch[180] Batch [5]#011Speed: 120.46 samples/sec#011loss=2.049643\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:49 INFO 140252856112960] Epoch[179] Batch[0] avg_epoch_loss=2.051970\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:51 INFO 140196492334912] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10898.524045944214, \"sum\": 10898.524045944214, \"min\": 10898.524045944214}}, \"EndTime\": 1538409711.29268, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409700.393907}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:51 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.151834514 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:51 INFO 140196492334912] #progress_metric: host=algo-4, completed 71 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:52 INFO 140624803325760] Epoch[182] Batch[10] avg_epoch_loss=2.001547\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:52 INFO 140624803325760] Epoch[182] Batch [10]#011Speed: 119.72 samples/sec#011loss=1.964716\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:52 INFO 140624803325760] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11806.196928024292, \"sum\": 11806.196928024292, \"min\": 11806.196928024292}}, \"EndTime\": 1538409712.143164, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409700.336748}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:52 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=109.602332845 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:52 INFO 140624803325760] #progress_metric: host=algo-3, completed 73 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:52 INFO 139663579178816] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10761.15608215332, \"sum\": 10761.15608215332, \"min\": 10761.15608215332}}, \"EndTime\": 1538409712.441594, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409701.680195}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:52 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.666072523 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:52 INFO 139663579178816] #progress_metric: host=algo-2, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:52 INFO 140196492334912] Epoch[178] Batch[0] avg_epoch_loss=1.765763\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:53 INFO 140624803325760] Epoch[183] Batch[0] avg_epoch_loss=2.154968\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:53 INFO 140252856112960] Epoch[179] Batch[5] avg_epoch_loss=2.075653\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:53 INFO 140252856112960] Epoch[179] Batch [5]#011Speed: 154.07 samples/sec#011loss=2.075653\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:53 INFO 139663579178816] Epoch[181] Batch[0] avg_epoch_loss=2.153252\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:57 INFO 140252856112960] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10261.58881187439, \"sum\": 10261.58881187439, \"min\": 10261.58881187439}}, \"EndTime\": 1538409717.417839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409707.156024}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.324961501 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:58 INFO 140196492334912] Epoch[178] Batch[5] avg_epoch_loss=1.928151\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:01:58 INFO 140196492334912] Epoch[178] Batch [5]#011Speed: 119.70 samples/sec#011loss=1.928151\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:58 INFO 140624803325760] Epoch[183] Batch[5] avg_epoch_loss=1.930011\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:01:58 INFO 140624803325760] Epoch[183] Batch [5]#011Speed: 124.35 samples/sec#011loss=1.930011\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:59 INFO 139663579178816] Epoch[181] Batch[5] avg_epoch_loss=1.955496\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:01:59 INFO 139663579178816] Epoch[181] Batch [5]#011Speed: 120.56 samples/sec#011loss=1.955496\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:01:59 INFO 140252856112960] Epoch[180] Batch[0] avg_epoch_loss=2.056201\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:02 INFO 140196492334912] Epoch[178] Batch[10] avg_epoch_loss=1.914174\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:02 INFO 140196492334912] Epoch[178] Batch [10]#011Speed: 146.12 samples/sec#011loss=1.897401\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:02 INFO 140196492334912] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11385.051012039185, \"sum\": 11385.051012039185, \"min\": 11385.051012039185}}, \"EndTime\": 1538409722.678082, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409711.292767}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:02 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.730707089 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:02 INFO 140196492334912] #progress_metric: host=algo-4, completed 71 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:03 INFO 140624803325760] Epoch[183] Batch[10] avg_epoch_loss=2.112588\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:03 INFO 140624803325760] Epoch[183] Batch [10]#011Speed: 153.05 samples/sec#011loss=2.331681\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:03 INFO 140624803325760] processed a total of 1343 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10966.02988243103, \"sum\": 10966.02988243103, \"min\": 10966.02988243103}}, \"EndTime\": 1538409723.109502, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409712.143249}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:03 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.467967039 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:03 INFO 140624803325760] #progress_metric: host=algo-3, completed 73 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:03 INFO 139663579178816] processed a total of 1206 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10706.077814102173, \"sum\": 10706.077814102173, \"min\": 10706.077814102173}}, \"EndTime\": 1538409723.148036, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409712.441684}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:03 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.644787393 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:03 INFO 139663579178816] #progress_metric: host=algo-2, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:03 INFO 140252856112960] Epoch[180] Batch[5] avg_epoch_loss=2.070012\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:03 INFO 140252856112960] Epoch[180] Batch [5]#011Speed: 152.82 samples/sec#011loss=2.070012\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:04 INFO 139663579178816] Epoch[182] Batch[0] avg_epoch_loss=2.003671\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:05 INFO 140196492334912] Epoch[179] Batch[0] avg_epoch_loss=1.931771\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:05 INFO 140624803325760] Epoch[184] Batch[0] avg_epoch_loss=2.195536\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:09 INFO 140624803325760] Epoch[184] Batch[5] avg_epoch_loss=2.286106\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:09 INFO 140624803325760] Epoch[184] Batch [5]#011Speed: 155.34 samples/sec#011loss=2.286106\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:02:09 INFO 140252856112960] Epoch[180] Batch[10] avg_epoch_loss=2.254334\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:09 INFO 140252856112960] Epoch[180] Batch [10]#011Speed: 119.94 samples/sec#011loss=2.475521\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:09 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11863.477945327759, \"sum\": 11863.477945327759, \"min\": 11863.477945327759}}, \"EndTime\": 1538409729.281622, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409717.417912}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.663318896 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:09 INFO 140196492334912] Epoch[179] Batch[5] avg_epoch_loss=2.343714\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:09 INFO 140196492334912] Epoch[179] Batch [5]#011Speed: 149.72 samples/sec#011loss=2.343714\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:10 INFO 139663579178816] Epoch[182] Batch[5] avg_epoch_loss=2.310881\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:10 INFO 139663579178816] Epoch[182] Batch [5]#011Speed: 123.21 samples/sec#011loss=2.310881\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:10 INFO 140252856112960] Epoch[181] Batch[0] avg_epoch_loss=2.133631\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:13 INFO 140624803325760] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10236.467838287354, \"sum\": 10236.467838287354, \"min\": 10236.467838287354}}, \"EndTime\": 1538409733.346261, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409723.10957}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:13 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.961963996 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:13 INFO 140624803325760] #progress_metric: host=algo-3, completed 74 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:14 INFO 139663579178816] Epoch[182] Batch[10] avg_epoch_loss=2.179896\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:14 INFO 139663579178816] Epoch[182] Batch [10]#011Speed: 150.09 samples/sec#011loss=2.022715\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:14 INFO 139663579178816] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11168.226957321167, \"sum\": 11168.226957321167, \"min\": 11168.226957321167}}, \"EndTime\": 1538409734.316614, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409723.148137}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:14 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.474796016 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:14 INFO 139663579178816] #progress_metric: host=algo-2, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:14 INFO 140196492334912] Epoch[179] Batch[10] avg_epoch_loss=2.189594\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:14 INFO 140196492334912] Epoch[179] Batch [10]#011Speed: 117.89 samples/sec#011loss=2.004649\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:14 INFO 140196492334912] processed a total of 1345 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12168.437004089355, \"sum\": 12168.437004089355, \"min\": 12168.437004089355}}, \"EndTime\": 1538409734.846858, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409722.678168}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.530727225 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 72 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:15 INFO 140624803325760] Epoch[185] Batch[0] avg_epoch_loss=2.124562\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:16 INFO 140252856112960] Epoch[181] Batch[5] avg_epoch_loss=2.092912\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:16 INFO 140252856112960] Epoch[181] Batch [5]#011Speed: 122.25 samples/sec#011loss=2.092912\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:16 INFO 139663579178816] Epoch[183] Batch[0] avg_epoch_loss=2.341812\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:16 INFO 140196492334912] Epoch[180] Batch[0] avg_epoch_loss=2.534337\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:20 INFO 140252856112960] Epoch[181] Batch[10] avg_epoch_loss=2.010274\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:20 INFO 140252856112960] Epoch[181] Batch [10]#011Speed: 149.98 samples/sec#011loss=1.911108\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:20 INFO 140252856112960] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11124.686002731323, \"sum\": 11124.686002731323, \"min\": 11124.686002731323}}, \"EndTime\": 1538409740.406613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409729.281694}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.203903913 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:20 INFO 139663579178816] Epoch[183] Batch[5] avg_epoch_loss=2.489725\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:20 INFO 139663579178816] Epoch[183] Batch [5]#011Speed: 153.98 samples/sec#011loss=2.489725\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:19 INFO 140624803325760] Epoch[185] Batch[5] avg_epoch_loss=2.375524\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:19 INFO 140624803325760] Epoch[185] Batch [5]#011Speed: 154.96 samples/sec#011loss=2.375524\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:21 INFO 140196492334912] Epoch[180] Batch[5] avg_epoch_loss=2.543749\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:21 INFO 140196492334912] Epoch[180] Batch [5]#011Speed: 117.55 samples/sec#011loss=2.543749\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:22 INFO 140252856112960] Epoch[182] Batch[0] avg_epoch_loss=2.205663\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:25 INFO 140624803325760] Epoch[185] Batch[10] avg_epoch_loss=2.459615\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:25 INFO 140624803325760] Epoch[185] Batch [10]#011Speed: 121.80 samples/sec#011loss=2.560523\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:25 INFO 140624803325760] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11778.000116348267, \"sum\": 11778.000116348267, \"min\": 11778.000116348267}}, \"EndTime\": 1538409745.12457, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409733.346335}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:25 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.157070387 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:25 INFO 140624803325760] #progress_metric: host=algo-3, completed 74 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:26 INFO 139663579178816] Epoch[183] Batch[10] avg_epoch_loss=2.332524\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:26 INFO 139663579178816] Epoch[183] Batch [10]#011Speed: 121.25 samples/sec#011loss=2.143884\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:26 INFO 139663579178816] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11811.670064926147, \"sum\": 11811.670064926147, \"min\": 11811.670064926147}}, \"EndTime\": 1538409746.128635, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409734.316699}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:26 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.429856673 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:26 INFO 139663579178816] #progress_metric: host=algo-2, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:26 INFO 140196492334912] Epoch[180] Batch[10] avg_epoch_loss=2.525258\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:26 INFO 140196492334912] Epoch[180] Batch [10]#011Speed: 144.17 samples/sec#011loss=2.503069\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:26 INFO 140196492334912] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11508.869171142578, \"sum\": 11508.869171142578, \"min\": 11508.869171142578}}, \"EndTime\": 1538409746.356055, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409734.846942}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:26 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.781350037 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:26 INFO 140196492334912] #progress_metric: host=algo-4, completed 72 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:26 INFO 140624803325760] Epoch[186] Batch[0] avg_epoch_loss=2.354700\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:26 INFO 140252856112960] Epoch[182] Batch[5] avg_epoch_loss=2.260600\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:26 INFO 140252856112960] Epoch[182] Batch [5]#011Speed: 153.38 samples/sec#011loss=2.260600\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:27 INFO 139663579178816] Epoch[184] Batch[0] avg_epoch_loss=2.683755\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:28 INFO 140196492334912] Epoch[181] Batch[0] avg_epoch_loss=2.188019\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:02:31 INFO 140624803325760] Epoch[186] Batch[5] avg_epoch_loss=2.235078\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:31 INFO 140624803325760] Epoch[186] Batch [5]#011Speed: 124.05 samples/sec#011loss=2.235078\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:32 INFO 140252856112960] Epoch[182] Batch[10] avg_epoch_loss=2.503285\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:32 INFO 140252856112960] Epoch[182] Batch [10]#011Speed: 119.05 samples/sec#011loss=2.794507\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:32 INFO 140252856112960] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11940.50908088684, \"sum\": 11940.50908088684, \"min\": 11940.50908088684}}, \"EndTime\": 1538409752.347473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409740.406736}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:32 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.714572572 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:32 INFO 140252856112960] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:32 INFO 139663579178816] Epoch[184] Batch[5] avg_epoch_loss=2.266062\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:32 INFO 139663579178816] Epoch[184] Batch [5]#011Speed: 122.33 samples/sec#011loss=2.266062\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:33 INFO 140196492334912] Epoch[181] Batch[5] avg_epoch_loss=2.273920\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:33 INFO 140196492334912] Epoch[181] Batch [5]#011Speed: 149.95 samples/sec#011loss=2.273920\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:34 INFO 140252856112960] Epoch[183] Batch[0] avg_epoch_loss=2.522255\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:35 INFO 140624803325760] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10428.242921829224, \"sum\": 10428.242921829224, \"min\": 10428.242921829224}}, \"EndTime\": 1538409755.553142, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409745.124653}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:35 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.481576845 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:35 INFO 140624803325760] #progress_metric: host=algo-3, completed 74 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:36 INFO 139663579178816] processed a total of 1217 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10675.060033798218, \"sum\": 10675.060033798218, \"min\": 10675.060033798218}}, \"EndTime\": 1538409756.804025, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409746.128719}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:36 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.002461548 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:36 INFO 139663579178816] #progress_metric: host=algo-2, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:36 INFO 140196492334912] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10629.289865493774, \"sum\": 10629.289865493774, \"min\": 10629.289865493774}}, \"EndTime\": 1538409756.985679, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409746.35614}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:36 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.59807159 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:36 INFO 140196492334912] #progress_metric: host=algo-4, completed 72 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:37 INFO 140624803325760] Epoch[187] Batch[0] avg_epoch_loss=2.079596\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:38 INFO 139663579178816] Epoch[185] Batch[0] avg_epoch_loss=2.010908\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:39 INFO 140196492334912] Epoch[182] Batch[0] avg_epoch_loss=2.003663\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:39 INFO 140252856112960] Epoch[183] Batch[5] avg_epoch_loss=2.209163\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:39 INFO 140252856112960] Epoch[183] Batch [5]#011Speed: 116.72 samples/sec#011loss=2.209163\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:42 INFO 140624803325760] Epoch[187] Batch[5] avg_epoch_loss=2.058155\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:42 INFO 140624803325760] Epoch[187] Batch [5]#011Speed: 120.34 samples/sec#011loss=2.058155\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:43 INFO 139663579178816] Epoch[185] Batch[5] avg_epoch_loss=1.987567\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:43 INFO 139663579178816] Epoch[185] Batch [5]#011Speed: 123.96 samples/sec#011loss=1.987567\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:43 INFO 140196492334912] Epoch[182] Batch[5] avg_epoch_loss=2.179428\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:43 INFO 140196492334912] Epoch[182] Batch [5]#011Speed: 150.40 samples/sec#011loss=2.179428\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:43 INFO 140252856112960] Epoch[183] Batch[10] avg_epoch_loss=2.107360\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:43 INFO 140252856112960] Epoch[183] Batch [10]#011Speed: 143.97 samples/sec#011loss=1.985196\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:43 INFO 140252856112960] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11625.046968460083, \"sum\": 11625.046968460083, \"min\": 11625.046968460083}}, \"EndTime\": 1538409763.97283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409752.347546}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:43 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.514696026 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:43 INFO 140252856112960] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:46 INFO 140624803325760] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10657.26613998413, \"sum\": 10657.26613998413, \"min\": 10657.26613998413}}, \"EndTime\": 1538409766.210762, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409755.553243}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.0080222 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:46 INFO 140252856112960] Epoch[184] Batch[0] avg_epoch_loss=2.152168\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:47 INFO 139663579178816] Epoch[185] Batch[10] avg_epoch_loss=1.991530\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:47 INFO 139663579178816] Epoch[185] Batch [10]#011Speed: 150.24 samples/sec#011loss=1.996284\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:47 INFO 139663579178816] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11097.611904144287, \"sum\": 11097.611904144287, \"min\": 11097.611904144287}}, \"EndTime\": 1538409767.901996, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409756.80413}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:47 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.69043597 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:47 INFO 139663579178816] #progress_metric: host=algo-2, completed 74 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:47 INFO 140624803325760] Epoch[188] Batch[0] avg_epoch_loss=2.003427\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:49 INFO 140196492334912] Epoch[182] Batch[10] avg_epoch_loss=1.965952\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:49 INFO 140196492334912] Epoch[182] Batch [10]#011Speed: 117.18 samples/sec#011loss=1.709781\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:49 INFO 140196492334912] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12120.94497680664, \"sum\": 12120.94497680664, \"min\": 12120.94497680664}}, \"EndTime\": 1538409769.106976, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409756.985769}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:49 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.736247952 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:49 INFO 140196492334912] #progress_metric: host=algo-4, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:50 INFO 140196492334912] Epoch[183] Batch[0] avg_epoch_loss=1.667222\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:50 INFO 139663579178816] Epoch[186] Batch[0] avg_epoch_loss=1.927445\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:50 INFO 140252856112960] Epoch[184] Batch[5] avg_epoch_loss=2.014549\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:50 INFO 140252856112960] Epoch[184] Batch [5]#011Speed: 147.61 samples/sec#011loss=2.014549\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:53 INFO 140624803325760] Epoch[188] Batch[5] avg_epoch_loss=2.075711\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:53 INFO 140624803325760] Epoch[188] Batch [5]#011Speed: 122.62 samples/sec#011loss=2.075711\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:54 INFO 139663579178816] Epoch[186] Batch[5] avg_epoch_loss=2.095547\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:54 INFO 139663579178816] Epoch[186] Batch [5]#011Speed: 155.27 samples/sec#011loss=2.095547\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:54 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10806.929111480713, \"sum\": 10806.929111480713, \"min\": 10806.929111480713}}, \"EndTime\": 1538409774.78005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409763.972898}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:54 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.145827138 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:54 INFO 140252856112960] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:56 INFO 140196492334912] Epoch[183] Batch[5] avg_epoch_loss=2.064936\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:02:56 INFO 140196492334912] Epoch[183] Batch [5]#011Speed: 119.20 samples/sec#011loss=2.064936\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:57 INFO 140624803325760] Epoch[188] Batch[10] avg_epoch_loss=2.111583\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:57 INFO 140624803325760] Epoch[188] Batch [10]#011Speed: 152.75 samples/sec#011loss=2.154630\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:57 INFO 140624803325760] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11081.423044204712, \"sum\": 11081.423044204712, \"min\": 11081.423044204712}}, \"EndTime\": 1538409777.292544, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409766.210847}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:57 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.575759571 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:57 INFO 140624803325760] #progress_metric: host=algo-3, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:02:57 INFO 140252856112960] Epoch[185] Batch[0] avg_epoch_loss=2.407357\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:58 INFO 139663579178816] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10274.041891098022, \"sum\": 10274.041891098022, \"min\": 10274.041891098022}}, \"EndTime\": 1538409778.176367, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409767.90208}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:58 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.00986779 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:02:58 INFO 139663579178816] #progress_metric: host=algo-2, completed 74 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:02:59 INFO 140624803325760] Epoch[189] Batch[0] avg_epoch_loss=1.997934\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:00 INFO 139663579178816] Epoch[187] Batch[0] avg_epoch_loss=2.005714\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:03 INFO 140624803325760] Epoch[189] Batch[5] avg_epoch_loss=2.133346\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:03 INFO 140624803325760] Epoch[189] Batch [5]#011Speed: 159.97 samples/sec#011loss=2.133346\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:03:00 INFO 140196492334912] Epoch[183] Batch[10] avg_epoch_loss=2.422468\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:00 INFO 140196492334912] Epoch[183] Batch [10]#011Speed: 146.68 samples/sec#011loss=2.851506\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:00 INFO 140196492334912] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11374.859094619751, \"sum\": 11374.859094619751, \"min\": 11374.859094619751}}, \"EndTime\": 1538409780.482162, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409769.107058}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:00 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.31886183 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:00 INFO 140196492334912] #progress_metric: host=algo-4, completed 73 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:01 INFO 140252856112960] Epoch[185] Batch[5] avg_epoch_loss=2.121124\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:01 INFO 140252856112960] Epoch[185] Batch [5]#011Speed: 147.88 samples/sec#011loss=2.121124\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:02 INFO 140196492334912] Epoch[184] Batch[0] avg_epoch_loss=1.922699\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:04 INFO 139663579178816] Epoch[187] Batch[5] avg_epoch_loss=2.157341\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:04 INFO 139663579178816] Epoch[187] Batch [5]#011Speed: 152.82 samples/sec#011loss=2.157341\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:05 INFO 140252856112960] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10776.625871658325, \"sum\": 10776.625871658325, \"min\": 10776.625871658325}}, \"EndTime\": 1538409785.556987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409774.780127}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:05 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.691455068 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:05 INFO 140252856112960] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:07 INFO 140196492334912] Epoch[184] Batch[5] avg_epoch_loss=2.096753\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:07 INFO 140196492334912] Epoch[184] Batch [5]#011Speed: 149.94 samples/sec#011loss=2.096753\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:07 INFO 140252856112960] Epoch[186] Batch[0] avg_epoch_loss=2.067481\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:08 INFO 139663579178816] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10263.170003890991, \"sum\": 10263.170003890991, \"min\": 10263.170003890991}}, \"EndTime\": 1538409788.439858, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409778.176443}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:08 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.260117791 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:08 INFO 139663579178816] #progress_metric: host=algo-2, completed 75 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:08 INFO 140624803325760] Epoch[189] Batch[10] avg_epoch_loss=2.272701\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:08 INFO 140624803325760] Epoch[189] Batch [10]#011Speed: 125.97 samples/sec#011loss=2.439927\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:08 INFO 140624803325760] processed a total of 1351 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11389.814853668213, \"sum\": 11389.814853668213, \"min\": 11389.814853668213}}, \"EndTime\": 1538409788.682651, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409777.292611}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.613651408 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 76 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:10 INFO 140624803325760] Epoch[190] Batch[0] avg_epoch_loss=2.228630\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:12 INFO 140252856112960] Epoch[186] Batch[5] avg_epoch_loss=2.087228\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:12 INFO 140252856112960] Epoch[186] Batch [5]#011Speed: 146.34 samples/sec#011loss=2.087228\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:10 INFO 139663579178816] Epoch[188] Batch[0] avg_epoch_loss=2.443787\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:12 INFO 140196492334912] Epoch[184] Batch[10] avg_epoch_loss=2.003585\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:12 INFO 140196492334912] Epoch[184] Batch [10]#011Speed: 119.94 samples/sec#011loss=1.891782\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:12 INFO 140196492334912] processed a total of 1337 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12011.704206466675, \"sum\": 12011.704206466675, \"min\": 12011.704206466675}}, \"EndTime\": 1538409792.494194, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409780.482244}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:12 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.306944568 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:12 INFO 140196492334912] #progress_metric: host=algo-4, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:14 INFO 140196492334912] Epoch[185] Batch[0] avg_epoch_loss=1.942645\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:14 INFO 139663579178816] Epoch[188] Batch[5] avg_epoch_loss=2.191658\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:14 INFO 139663579178816] Epoch[188] Batch [5]#011Speed: 156.27 samples/sec#011loss=2.191658\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:15 INFO 140624803325760] Epoch[190] Batch[5] avg_epoch_loss=2.265225\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:15 INFO 140624803325760] Epoch[190] Batch [5]#011Speed: 126.52 samples/sec#011loss=2.265225\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:17 INFO 140252856112960] Epoch[186] Batch[10] avg_epoch_loss=1.770957\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:17 INFO 140252856112960] Epoch[186] Batch [10]#011Speed: 114.88 samples/sec#011loss=1.391431\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:17 INFO 140252856112960] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12372.950792312622, \"sum\": 12372.950792312622, \"min\": 12372.950792312622}}, \"EndTime\": 1538409797.930248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409785.557065}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=103.69292159 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:18 INFO 139663579178816] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10143.988132476807, \"sum\": 10143.988132476807, \"min\": 10143.988132476807}}, \"EndTime\": 1538409798.584153, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409788.439932}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:18 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.16836302 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:18 INFO 139663579178816] #progress_metric: host=algo-2, completed 75 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:18 INFO 140624803325760] processed a total of 1214 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10312.780141830444, \"sum\": 10312.780141830444, \"min\": 10312.780141830444}}, \"EndTime\": 1538409798.995741, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409788.682721}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.716963793 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:19 INFO 140196492334912] Epoch[185] Batch[5] avg_epoch_loss=2.181058\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:19 INFO 140196492334912] Epoch[185] Batch [5]#011Speed: 119.39 samples/sec#011loss=2.181058\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:19 INFO 140252856112960] Epoch[187] Batch[0] avg_epoch_loss=2.076853\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:20 INFO 139663579178816] Epoch[189] Batch[0] avg_epoch_loss=1.919278\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:23 INFO 140196492334912] Epoch[185] Batch[10] avg_epoch_loss=1.955590\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:23 INFO 140196492334912] Epoch[185] Batch [10]#011Speed: 144.23 samples/sec#011loss=1.685028\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:23 INFO 140196492334912] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11440.376996994019, \"sum\": 11440.376996994019, \"min\": 11440.376996994019}}, \"EndTime\": 1538409803.934897, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409792.494279}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:23 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.330626651 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:23 INFO 140196492334912] #progress_metric: host=algo-4, completed 74 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:03:20 INFO 140624803325760] Epoch[191] Batch[0] avg_epoch_loss=2.146328\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:24 INFO 140252856112960] Epoch[187] Batch[5] avg_epoch_loss=2.026344\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:24 INFO 140252856112960] Epoch[187] Batch [5]#011Speed: 122.00 samples/sec#011loss=2.026344\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:25 INFO 139663579178816] Epoch[189] Batch[5] avg_epoch_loss=1.964507\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:25 INFO 139663579178816] Epoch[189] Batch [5]#011Speed: 152.82 samples/sec#011loss=1.964507\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:25 INFO 140624803325760] Epoch[191] Batch[5] avg_epoch_loss=2.200893\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:25 INFO 140624803325760] Epoch[191] Batch [5]#011Speed: 126.24 samples/sec#011loss=2.200893\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:26 INFO 140196492334912] Epoch[186] Batch[0] avg_epoch_loss=1.883654\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:29 INFO 140624803325760] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10255.445003509521, \"sum\": 10255.445003509521, \"min\": 10255.445003509521}}, \"EndTime\": 1538409809.251465, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409798.995803}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:29 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.347832573 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:29 INFO 140624803325760] #progress_metric: host=algo-3, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:28 INFO 140252856112960] processed a total of 1212 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10613.543033599854, \"sum\": 10613.543033599854, \"min\": 10613.543033599854}}, \"EndTime\": 1538409808.544095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409797.930334}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:28 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.192787346 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:28 INFO 140252856112960] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:30 INFO 140252856112960] Epoch[188] Batch[0] avg_epoch_loss=2.565842\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:30 INFO 139663579178816] Epoch[189] Batch[10] avg_epoch_loss=2.095188\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:30 INFO 139663579178816] Epoch[189] Batch [10]#011Speed: 118.34 samples/sec#011loss=2.252005\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:30 INFO 139663579178816] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11927.886009216309, \"sum\": 11927.886009216309, \"min\": 11927.886009216309}}, \"EndTime\": 1538409810.512342, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409798.584229}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:30 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=110.831703376 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:30 INFO 139663579178816] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:30 INFO 140196492334912] Epoch[186] Batch[5] avg_epoch_loss=2.153734\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:30 INFO 140196492334912] Epoch[186] Batch [5]#011Speed: 150.66 samples/sec#011loss=2.153734\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:32 INFO 139663579178816] Epoch[190] Batch[0] avg_epoch_loss=2.234572\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:30 INFO 140624803325760] Epoch[192] Batch[0] avg_epoch_loss=2.162778\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:36 INFO 140196492334912] Epoch[186] Batch[10] avg_epoch_loss=1.848430\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:36 INFO 140196492334912] Epoch[186] Batch [10]#011Speed: 117.26 samples/sec#011loss=1.482065\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:36 INFO 140196492334912] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12083.78005027771, \"sum\": 12083.78005027771, \"min\": 12083.78005027771}}, \"EndTime\": 1538409816.019009, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409803.934982}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:36 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=107.663855072 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:36 INFO 140196492334912] #progress_metric: host=algo-4, completed 74 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:35 INFO 140624803325760] Epoch[192] Batch[5] avg_epoch_loss=1.997816\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:35 INFO 140624803325760] Epoch[192] Batch [5]#011Speed: 127.19 samples/sec#011loss=1.997816\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:35 INFO 140252856112960] Epoch[188] Batch[5] avg_epoch_loss=2.194528\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:35 INFO 140252856112960] Epoch[188] Batch [5]#011Speed: 115.97 samples/sec#011loss=2.194528\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:37 INFO 139663579178816] Epoch[190] Batch[5] avg_epoch_loss=1.984007\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:37 INFO 139663579178816] Epoch[190] Batch [5]#011Speed: 123.73 samples/sec#011loss=1.984007\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:37 INFO 140196492334912] Epoch[187] Batch[0] avg_epoch_loss=2.059368\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:39 INFO 140624803325760] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10435.406923294067, \"sum\": 10435.406923294067, \"min\": 10435.406923294067}}, \"EndTime\": 1538409819.687169, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409809.251536}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:39 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.729167528 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:39 INFO 140624803325760] #progress_metric: host=algo-3, completed 77 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:39 INFO 140252856112960] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11070.890188217163, \"sum\": 11070.890188217163, \"min\": 11070.890188217163}}, \"EndTime\": 1538409819.615294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409808.544153}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:39 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.72050071 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:39 INFO 140252856112960] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:41 INFO 140624803325760] Epoch[193] Batch[0] avg_epoch_loss=1.814624\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:41 INFO 139663579178816] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10665.028095245361, \"sum\": 10665.028095245361, \"min\": 10665.028095245361}}, \"EndTime\": 1538409821.177679, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409810.512417}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.579326866 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:41 INFO 140252856112960] Epoch[189] Batch[0] avg_epoch_loss=1.670613\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:42 INFO 139663579178816] Epoch[191] Batch[0] avg_epoch_loss=1.890845\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:43 INFO 140196492334912] Epoch[187] Batch[5] avg_epoch_loss=1.957121\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:43 INFO 140196492334912] Epoch[187] Batch [5]#011Speed: 118.57 samples/sec#011loss=1.957121\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:46 INFO 140624803325760] Epoch[193] Batch[5] avg_epoch_loss=1.938338\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:46 INFO 140624803325760] Epoch[193] Batch [5]#011Speed: 127.20 samples/sec#011loss=1.938338\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:46 INFO 140252856112960] Epoch[189] Batch[5] avg_epoch_loss=1.875207\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:46 INFO 140252856112960] Epoch[189] Batch [5]#011Speed: 118.18 samples/sec#011loss=1.875207\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:47 INFO 140196492334912] Epoch[187] Batch[10] avg_epoch_loss=1.680289\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:47 INFO 140196492334912] Epoch[187] Batch [10]#011Speed: 145.65 samples/sec#011loss=1.348090\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:47 INFO 140196492334912] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11436.122179031372, \"sum\": 11436.122179031372, \"min\": 11436.122179031372}}, \"EndTime\": 1538409827.455462, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409816.019094}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.449467063 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 75 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:48 INFO 139663579178816] Epoch[191] Batch[5] avg_epoch_loss=1.990946\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:48 INFO 139663579178816] Epoch[191] Batch [5]#011Speed: 121.67 samples/sec#011loss=1.990946\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:49 INFO 140196492334912] Epoch[188] Batch[0] avg_epoch_loss=2.063016\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:49 INFO 140624803325760] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10196.599006652832, \"sum\": 10196.599006652832, \"min\": 10196.599006652832}}, \"EndTime\": 1538409829.884094, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409819.687243}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:49 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=125.236521766 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:49 INFO 140624803325760] #progress_metric: host=algo-3, completed 77 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:51 INFO 140624803325760] Epoch[194] Batch[0] avg_epoch_loss=1.931722\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:51 INFO 140252856112960] Epoch[189] Batch[10] avg_epoch_loss=1.720615\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:51 INFO 140252856112960] Epoch[189] Batch [10]#011Speed: 144.65 samples/sec#011loss=1.535104\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:51 INFO 140252856112960] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11544.188976287842, \"sum\": 11544.188976287842, \"min\": 11544.188976287842}}, \"EndTime\": 1538409831.159816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409819.615371}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:51 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.294814689 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:51 INFO 140252856112960] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:52 INFO 139663579178816] Epoch[191] Batch[10] avg_epoch_loss=2.116945\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:52 INFO 139663579178816] Epoch[191] Batch [10]#011Speed: 146.88 samples/sec#011loss=2.268143\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:52 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11259.63807106018, \"sum\": 11259.63807106018, \"min\": 11259.63807106018}}, \"EndTime\": 1538409832.437635, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409821.177755}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:52 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.166093927 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:52 INFO 139663579178816] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:03:54 INFO 140196492334912] Epoch[188] Batch[5] avg_epoch_loss=1.961730\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:54 INFO 140196492334912] Epoch[188] Batch [5]#011Speed: 150.10 samples/sec#011loss=1.961730\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:53 INFO 140252856112960] Epoch[190] Batch[0] avg_epoch_loss=2.101797\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:54 INFO 139663579178816] Epoch[192] Batch[0] avg_epoch_loss=1.924188\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:56 INFO 140624803325760] Epoch[194] Batch[5] avg_epoch_loss=1.901670\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:03:56 INFO 140624803325760] Epoch[194] Batch [5]#011Speed: 125.34 samples/sec#011loss=1.901670\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:57 INFO 140196492334912] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10534.652948379517, \"sum\": 10534.652948379517, \"min\": 10534.652948379517}}, \"EndTime\": 1538409837.990444, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409827.455545}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:57 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.793630239 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:03:57 INFO 140196492334912] #progress_metric: host=algo-4, completed 75 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:57 INFO 140252856112960] Epoch[190] Batch[5] avg_epoch_loss=2.021465\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:03:57 INFO 140252856112960] Epoch[190] Batch [5]#011Speed: 149.04 samples/sec#011loss=2.021465\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:58 INFO 139663579178816] Epoch[192] Batch[5] avg_epoch_loss=2.049779\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:03:58 INFO 139663579178816] Epoch[192] Batch [5]#011Speed: 151.77 samples/sec#011loss=2.049779\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:00 INFO 140624803325760] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10360.666990280151, \"sum\": 10360.666990280151, \"min\": 10360.666990280151}}, \"EndTime\": 1538409840.245159, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409829.884165}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:00 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.164780972 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:00 INFO 140624803325760] #progress_metric: host=algo-3, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:00 INFO 140196492334912] Epoch[189] Batch[0] avg_epoch_loss=1.704460\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:01 INFO 140252856112960] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10716.05396270752, \"sum\": 10716.05396270752, \"min\": 10716.05396270752}}, \"EndTime\": 1538409841.876211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409831.159904}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:01 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.258826505 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:01 INFO 140252856112960] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:04 INFO 140252856112960] Epoch[191] Batch[0] avg_epoch_loss=1.727881\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:04 INFO 140196492334912] Epoch[189] Batch[5] avg_epoch_loss=1.920561\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:04 INFO 140196492334912] Epoch[189] Batch [5]#011Speed: 149.22 samples/sec#011loss=1.920561\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:01 INFO 140624803325760] Epoch[195] Batch[0] avg_epoch_loss=1.849515\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:02 INFO 139663579178816] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10436.044931411743, \"sum\": 10436.044931411743, \"min\": 10436.044931411743}}, \"EndTime\": 1538409842.873974, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409832.437703}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:02 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.488453251 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:02 INFO 139663579178816] #progress_metric: host=algo-2, completed 77 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:05 INFO 139663579178816] Epoch[193] Batch[0] avg_epoch_loss=1.823863\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:06 INFO 140624803325760] Epoch[195] Batch[5] avg_epoch_loss=1.897220\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:06 INFO 140624803325760] Epoch[195] Batch [5]#011Speed: 124.36 samples/sec#011loss=1.897220\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:08 INFO 140252856112960] Epoch[191] Batch[5] avg_epoch_loss=1.872962\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:08 INFO 140252856112960] Epoch[191] Batch [5]#011Speed: 148.77 samples/sec#011loss=1.872962\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:09 INFO 139663579178816] Epoch[193] Batch[5] avg_epoch_loss=1.958938\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:09 INFO 139663579178816] Epoch[193] Batch [5]#011Speed: 150.37 samples/sec#011loss=1.958938\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:10 INFO 140196492334912] Epoch[189] Batch[10] avg_epoch_loss=2.048637\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:10 INFO 140196492334912] Epoch[189] Batch [10]#011Speed: 116.89 samples/sec#011loss=2.202328\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:10 INFO 140196492334912] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12137.1591091156, \"sum\": 12137.1591091156, \"min\": 12137.1591091156}}, \"EndTime\": 1538409850.127939, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409837.990531}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:10 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.448847549 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:10 INFO 140196492334912] #progress_metric: host=algo-4, completed 76 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:11 INFO 140624803325760] Epoch[195] Batch[10] avg_epoch_loss=2.296962\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:11 INFO 140624803325760] Epoch[195] Batch [10]#011Speed: 154.15 samples/sec#011loss=2.776652\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:11 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10880.985975265503, \"sum\": 10880.985975265503, \"min\": 10880.985975265503}}, \"EndTime\": 1538409851.126445, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409840.245231}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:11 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.749033313 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:11 INFO 140624803325760] #progress_metric: host=algo-3, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:11 INFO 140196492334912] Epoch[190] Batch[0] avg_epoch_loss=2.356624\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:12 INFO 140252856112960] processed a total of 1214 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10625.684976577759, \"sum\": 10625.684976577759, \"min\": 10625.684976577759}}, \"EndTime\": 1538409852.502245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409841.876299}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:12 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.249996707 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:12 INFO 140252856112960] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:13 INFO 140624803325760] Epoch[196] Batch[0] avg_epoch_loss=1.888506\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:14 INFO 139663579178816] Epoch[193] Batch[10] avg_epoch_loss=2.309674\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:14 INFO 139663579178816] Epoch[193] Batch [10]#011Speed: 118.99 samples/sec#011loss=2.730557\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:14 INFO 139663579178816] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12019.712924957275, \"sum\": 12019.712924957275, \"min\": 12019.712924957275}}, \"EndTime\": 1538409854.894013, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409842.874048}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:14 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.569048277 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:14 INFO 139663579178816] #progress_metric: host=algo-2, completed 77 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:14 INFO 140252856112960] Epoch[192] Batch[0] avg_epoch_loss=1.412123\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:16 INFO 139663579178816] Epoch[194] Batch[0] avg_epoch_loss=2.210876\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:17 INFO 140196492334912] Epoch[190] Batch[5] avg_epoch_loss=2.227156\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:17 INFO 140196492334912] Epoch[190] Batch [5]#011Speed: 118.47 samples/sec#011loss=2.227156\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:17 INFO 140624803325760] Epoch[196] Batch[5] avg_epoch_loss=2.199414\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:17 INFO 140624803325760] Epoch[196] Batch [5]#011Speed: 158.01 samples/sec#011loss=2.199414\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:19 INFO 140252856112960] Epoch[192] Batch[5] avg_epoch_loss=2.015306\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:19 INFO 140252856112960] Epoch[192] Batch [5]#011Speed: 148.81 samples/sec#011loss=2.015306\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:21 INFO 140196492334912] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10873.508930206299, \"sum\": 10873.508930206299, \"min\": 10873.508930206299}}, \"EndTime\": 1538409861.001775, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409850.128024}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:21 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.956894234 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:21 INFO 140196492334912] #progress_metric: host=algo-4, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:21 INFO 139663579178816] Epoch[194] Batch[5] avg_epoch_loss=2.020001\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:21 INFO 139663579178816] Epoch[194] Batch [5]#011Speed: 119.98 samples/sec#011loss=2.020001\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:22 INFO 140196492334912] Epoch[191] Batch[0] avg_epoch_loss=2.325667\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:25 INFO 139663579178816] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10793.339967727661, \"sum\": 10793.339967727661, \"min\": 10793.339967727661}}, \"EndTime\": 1538409865.687655, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409854.894086}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:25 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.79190603 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:25 INFO 139663579178816] #progress_metric: host=algo-2, completed 78 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:04:22 INFO 140624803325760] Epoch[196] Batch[10] avg_epoch_loss=1.882363\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:22 INFO 140624803325760] Epoch[196] Batch [10]#011Speed: 122.04 samples/sec#011loss=1.501901\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:22 INFO 140624803325760] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11613.851070404053, \"sum\": 11613.851070404053, \"min\": 11613.851070404053}}, \"EndTime\": 1538409862.740587, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409851.126515}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:22 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.450771624 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:22 INFO 140624803325760] #progress_metric: host=algo-3, completed 78 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:23 INFO 140252856112960] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10621.195077896118, \"sum\": 10621.195077896118, \"min\": 10621.195077896118}}, \"EndTime\": 1538409863.123791, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409852.502336}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:23 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.759012974 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:23 INFO 140252856112960] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:24 INFO 140624803325760] Epoch[197] Batch[0] avg_epoch_loss=2.167541\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:25 INFO 140252856112960] Epoch[193] Batch[0] avg_epoch_loss=1.851652\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:27 INFO 139663579178816] Epoch[195] Batch[0] avg_epoch_loss=1.937628\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:27 INFO 140196492334912] Epoch[191] Batch[5] avg_epoch_loss=1.978516\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:27 INFO 140196492334912] Epoch[191] Batch [5]#011Speed: 118.90 samples/sec#011loss=1.978516\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:29 INFO 140624803325760] Epoch[197] Batch[5] avg_epoch_loss=1.964253\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:29 INFO 140624803325760] Epoch[197] Batch [5]#011Speed: 125.86 samples/sec#011loss=1.964253\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:29 INFO 140252856112960] Epoch[193] Batch[5] avg_epoch_loss=2.217422\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:29 INFO 140252856112960] Epoch[193] Batch [5]#011Speed: 150.33 samples/sec#011loss=2.217422\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:31 INFO 140196492334912] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10818.0091381073, \"sum\": 10818.0091381073, \"min\": 10818.0091381073}}, \"EndTime\": 1538409871.820145, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409861.001865}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:31 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.067604661 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:31 INFO 140196492334912] #progress_metric: host=algo-4, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:32 INFO 139663579178816] Epoch[195] Batch[5] avg_epoch_loss=1.987411\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:32 INFO 139663579178816] Epoch[195] Batch [5]#011Speed: 120.11 samples/sec#011loss=1.987411\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:36 INFO 139663579178816] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10817.559003829956, \"sum\": 10817.559003829956, \"min\": 10817.559003829956}}, \"EndTime\": 1538409876.505533, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409865.687726}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:36 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.291022755 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:36 INFO 139663579178816] #progress_metric: host=algo-2, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:33 INFO 140196492334912] Epoch[192] Batch[0] avg_epoch_loss=2.087852\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:33 INFO 140624803325760] Epoch[197] Batch[10] avg_epoch_loss=2.193834\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:33 INFO 140624803325760] Epoch[197] Batch [10]#011Speed: 153.16 samples/sec#011loss=2.469332\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:33 INFO 140624803325760] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10846.688032150269, \"sum\": 10846.688032150269, \"min\": 10846.688032150269}}, \"EndTime\": 1538409873.587582, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409862.740674}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:33 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.744786667 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:33 INFO 140624803325760] #progress_metric: host=algo-3, completed 79 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:35 INFO 140252856112960] Epoch[193] Batch[10] avg_epoch_loss=1.875360\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:35 INFO 140252856112960] Epoch[193] Batch [10]#011Speed: 116.68 samples/sec#011loss=1.464886\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:35 INFO 140252856112960] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12139.729022979736, \"sum\": 12139.729022979736, \"min\": 12139.729022979736}}, \"EndTime\": 1538409875.263866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409863.12388}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:35 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.673422594 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:35 INFO 140252856112960] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:35 INFO 140624803325760] Epoch[198] Batch[0] avg_epoch_loss=2.472024\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:36 INFO 140252856112960] Epoch[194] Batch[0] avg_epoch_loss=1.946080\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:38 INFO 139663579178816] Epoch[196] Batch[0] avg_epoch_loss=2.214694\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:38 INFO 140196492334912] Epoch[192] Batch[5] avg_epoch_loss=2.160762\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:38 INFO 140196492334912] Epoch[192] Batch [5]#011Speed: 119.09 samples/sec#011loss=2.160762\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:39 INFO 140624803325760] Epoch[198] Batch[5] avg_epoch_loss=2.133627\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:39 INFO 140624803325760] Epoch[198] Batch [5]#011Speed: 158.62 samples/sec#011loss=2.133627\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:42 INFO 140252856112960] Epoch[194] Batch[5] avg_epoch_loss=2.232683\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:42 INFO 140252856112960] Epoch[194] Batch [5]#011Speed: 120.60 samples/sec#011loss=2.232683\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:43 INFO 140196492334912] Epoch[192] Batch[10] avg_epoch_loss=2.146673\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:43 INFO 140196492334912] Epoch[192] Batch [10]#011Speed: 144.57 samples/sec#011loss=2.129766\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:43 INFO 140196492334912] processed a total of 1320 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11418.549060821533, \"sum\": 11418.549060821533, \"min\": 11418.549060821533}}, \"EndTime\": 1538409883.239061, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409871.820238}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:43 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.600103012 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:43 INFO 140196492334912] #progress_metric: host=algo-4, completed 77 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:43 INFO 140624803325760] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10051.919937133789, \"sum\": 10051.919937133789, \"min\": 10051.919937133789}}, \"EndTime\": 1538409883.639797, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409873.587652}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=126.342614187 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 79 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:46 INFO 140252856112960] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10745.354890823364, \"sum\": 10745.354890823364, \"min\": 10745.354890823364}}, \"EndTime\": 1538409886.009553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409875.263951}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.676436479 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 16:04:43 INFO 139663579178816] Epoch[196] Batch[5] avg_epoch_loss=2.229066\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:43 INFO 139663579178816] Epoch[196] Batch [5]#011Speed: 120.17 samples/sec#011loss=2.229066\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:45 INFO 140196492334912] Epoch[193] Batch[0] avg_epoch_loss=2.378969\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:45 INFO 140624803325760] Epoch[199] Batch[0] avg_epoch_loss=2.267865\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:47 INFO 139663579178816] processed a total of 1218 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10916.921854019165, \"sum\": 10916.921854019165, \"min\": 10916.921854019165}}, \"EndTime\": 1538409887.422801, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409876.505621}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:47 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.56850756 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:47 INFO 139663579178816] #progress_metric: host=algo-2, completed 78 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:47 INFO 140252856112960] Epoch[195] Batch[0] avg_epoch_loss=2.210809\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:49 INFO 139663579178816] Epoch[197] Batch[0] avg_epoch_loss=2.445876\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:49 INFO 140196492334912] Epoch[193] Batch[5] avg_epoch_loss=2.167408\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:49 INFO 140196492334912] Epoch[193] Batch [5]#011Speed: 150.34 samples/sec#011loss=2.167408\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:49 INFO 140624803325760] Epoch[199] Batch[5] avg_epoch_loss=2.044742\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:49 INFO 140624803325760] Epoch[199] Batch [5]#011Speed: 158.32 samples/sec#011loss=2.044742\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:52 INFO 140252856112960] Epoch[195] Batch[5] avg_epoch_loss=2.151768\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:52 INFO 140252856112960] Epoch[195] Batch [5]#011Speed: 121.02 samples/sec#011loss=2.151768\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:53 INFO 140624803325760] processed a total of 1196 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10086.50517463684, \"sum\": 10086.50517463684, \"min\": 10086.50517463684}}, \"EndTime\": 1538409893.726605, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409883.639874}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.572990337 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 80 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:54 INFO 139663579178816] Epoch[197] Batch[5] avg_epoch_loss=2.108973\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:54 INFO 139663579178816] Epoch[197] Batch [5]#011Speed: 119.75 samples/sec#011loss=2.108973\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:55 INFO 140196492334912] Epoch[193] Batch[10] avg_epoch_loss=1.878754\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:55 INFO 140196492334912] Epoch[193] Batch [10]#011Speed: 117.21 samples/sec#011loss=1.532370\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:55 INFO 140196492334912] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12165.188074111938, \"sum\": 12165.188074111938, \"min\": 12165.188074111938}}, \"EndTime\": 1538409895.404579, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409883.239144}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:55 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.532331823 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:55 INFO 140196492334912] #progress_metric: host=algo-4, completed 77 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:04:56 INFO 140624803325760] Epoch[200] Batch[0] avg_epoch_loss=2.177768\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:04:57 INFO 140196492334912] Epoch[194] Batch[0] avg_epoch_loss=2.263720\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:57 INFO 140252856112960] Epoch[195] Batch[10] avg_epoch_loss=1.917835\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:57 INFO 140252856112960] Epoch[195] Batch [10]#011Speed: 148.16 samples/sec#011loss=1.637116\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:57 INFO 140252856112960] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11252.563953399658, \"sum\": 11252.563953399658, \"min\": 11252.563953399658}}, \"EndTime\": 1538409897.262484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409886.009643}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:57 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.194916169 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:57 INFO 140252856112960] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:58 INFO 139663579178816] Epoch[197] Batch[10] avg_epoch_loss=2.335832\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:58 INFO 139663579178816] Epoch[197] Batch [10]#011Speed: 148.87 samples/sec#011loss=2.608063\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:58 INFO 139663579178816] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11282.294034957886, \"sum\": 11282.294034957886, \"min\": 11282.294034957886}}, \"EndTime\": 1538409898.705455, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409887.422895}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:58 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.059766319 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:04:58 INFO 139663579178816] #progress_metric: host=algo-2, completed 79 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:00 INFO 140624803325760] Epoch[200] Batch[5] avg_epoch_loss=2.157620\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:00 INFO 140624803325760] Epoch[200] Batch [5]#011Speed: 154.95 samples/sec#011loss=2.157620\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:04:59 INFO 140252856112960] Epoch[196] Batch[0] avg_epoch_loss=2.439443\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:01 INFO 139663579178816] Epoch[198] Batch[0] avg_epoch_loss=1.792190\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:02 INFO 140196492334912] Epoch[194] Batch[5] avg_epoch_loss=2.182682\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:02 INFO 140196492334912] Epoch[194] Batch [5]#011Speed: 118.72 samples/sec#011loss=2.182682\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:03 INFO 140252856112960] Epoch[196] Batch[5] avg_epoch_loss=2.124391\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:03 INFO 140252856112960] Epoch[196] Batch [5]#011Speed: 153.37 samples/sec#011loss=2.124391\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:05 INFO 139663579178816] Epoch[198] Batch[5] avg_epoch_loss=2.154844\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:05 INFO 139663579178816] Epoch[198] Batch [5]#011Speed: 149.25 samples/sec#011loss=2.154844\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:06 INFO 140196492334912] Epoch[194] Batch[10] avg_epoch_loss=2.141087\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:06 INFO 140196492334912] Epoch[194] Batch [10]#011Speed: 145.86 samples/sec#011loss=2.091173\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:06 INFO 140196492334912] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11548.346042633057, \"sum\": 11548.346042633057, \"min\": 11548.346042633057}}, \"EndTime\": 1538409906.953263, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409895.40467}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:06 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.474019445 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:06 INFO 140196492334912] #progress_metric: host=algo-4, completed 78 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:05 INFO 140624803325760] Epoch[200] Batch[10] avg_epoch_loss=2.421789\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:05 INFO 140624803325760] Epoch[200] Batch [10]#011Speed: 122.91 samples/sec#011loss=2.738793\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:05 INFO 140624803325760] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11705.510139465332, \"sum\": 11705.510139465332, \"min\": 11705.510139465332}}, \"EndTime\": 1538409905.432426, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409893.726679}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:05 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.484919269 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:05 INFO 140624803325760] #progress_metric: host=algo-3, completed 80 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:07 INFO 140624803325760] Epoch[201] Batch[0] avg_epoch_loss=2.094097\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:09 INFO 140196492334912] Epoch[195] Batch[0] avg_epoch_loss=2.328627\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:09 INFO 140252856112960] Epoch[196] Batch[10] avg_epoch_loss=1.900216\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:09 INFO 140252856112960] Epoch[196] Batch [10]#011Speed: 119.42 samples/sec#011loss=1.631205\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:09 INFO 140252856112960] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11956.068992614746, \"sum\": 11956.068992614746, \"min\": 11956.068992614746}}, \"EndTime\": 1538409909.218989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409897.262567}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=109.650230935 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:10 INFO 139663579178816] Epoch[198] Batch[10] avg_epoch_loss=2.081326\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:10 INFO 139663579178816] Epoch[198] Batch [10]#011Speed: 117.60 samples/sec#011loss=1.993105\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:10 INFO 139663579178816] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12101.232051849365, \"sum\": 12101.232051849365, \"min\": 12101.232051849365}}, \"EndTime\": 1538409910.807024, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409898.705541}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:10 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=107.508523861 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:10 INFO 139663579178816] #progress_metric: host=algo-2, completed 79 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:10 INFO 140252856112960] Epoch[197] Batch[0] avg_epoch_loss=2.026072\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:12 INFO 140624803325760] Epoch[201] Batch[5] avg_epoch_loss=2.016884\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:12 INFO 140624803325760] Epoch[201] Batch [5]#011Speed: 125.29 samples/sec#011loss=2.016884\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:12 INFO 139663579178816] Epoch[199] Batch[0] avg_epoch_loss=2.140536\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:15 INFO 140624803325760] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10400.743007659912, \"sum\": 10400.743007659912, \"min\": 10400.743007659912}}, \"EndTime\": 1538409915.833468, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409905.432496}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:15 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.317145916 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:15 INFO 140624803325760] #progress_metric: host=algo-3, completed 80 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:05:13 INFO 140196492334912] Epoch[195] Batch[5] avg_epoch_loss=2.095457\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:13 INFO 140196492334912] Epoch[195] Batch [5]#011Speed: 150.42 samples/sec#011loss=2.095457\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:16 INFO 140252856112960] Epoch[197] Batch[5] avg_epoch_loss=2.056808\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:16 INFO 140252856112960] Epoch[197] Batch [5]#011Speed: 120.73 samples/sec#011loss=2.056808\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:17 INFO 139663579178816] Epoch[199] Batch[5] avg_epoch_loss=2.112006\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:17 INFO 139663579178816] Epoch[199] Batch [5]#011Speed: 121.41 samples/sec#011loss=2.112006\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:17 INFO 140196492334912] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10490.547895431519, \"sum\": 10490.547895431519, \"min\": 10490.547895431519}}, \"EndTime\": 1538409917.444191, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409906.953346}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:17 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.437549371 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:17 INFO 140196492334912] #progress_metric: host=algo-4, completed 78 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:17 INFO 140624803325760] Epoch[202] Batch[0] avg_epoch_loss=2.151254\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:19 INFO 140196492334912] Epoch[196] Batch[0] avg_epoch_loss=1.929832\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:20 INFO 140252856112960] Epoch[197] Batch[10] avg_epoch_loss=2.071699\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:20 INFO 140252856112960] Epoch[197] Batch [10]#011Speed: 148.19 samples/sec#011loss=2.089568\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:20 INFO 140252856112960] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11307.871103286743, \"sum\": 11307.871103286743, \"min\": 11307.871103286743}}, \"EndTime\": 1538409920.52719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409909.219076}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.343848189 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:22 INFO 139663579178816] Epoch[199] Batch[10] avg_epoch_loss=2.129205\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:22 INFO 139663579178816] Epoch[199] Batch [10]#011Speed: 147.47 samples/sec#011loss=2.149845\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:22 INFO 139663579178816] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11210.470199584961, \"sum\": 11210.470199584961, \"min\": 11210.470199584961}}, \"EndTime\": 1538409922.017874, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409910.807114}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:22 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.248099239 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:22 INFO 139663579178816] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:22 INFO 140624803325760] Epoch[202] Batch[5] avg_epoch_loss=1.893501\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:22 INFO 140624803325760] Epoch[202] Batch [5]#011Speed: 125.77 samples/sec#011loss=1.893501\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:22 INFO 140252856112960] Epoch[198] Batch[0] avg_epoch_loss=2.018936\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:24 INFO 140196492334912] Epoch[196] Batch[5] avg_epoch_loss=1.912996\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:24 INFO 140196492334912] Epoch[196] Batch [5]#011Speed: 152.09 samples/sec#011loss=1.912996\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:26 INFO 140624803325760] Epoch[202] Batch[10] avg_epoch_loss=2.159160\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:26 INFO 140624803325760] Epoch[202] Batch [10]#011Speed: 155.13 samples/sec#011loss=2.477951\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:26 INFO 140624803325760] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10857.688903808594, \"sum\": 10857.688903808594, \"min\": 10857.688903808594}}, \"EndTime\": 1538409926.691462, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409915.833541}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:26 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.768952358 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:26 INFO 140624803325760] #progress_metric: host=algo-3, completed 81 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:24 INFO 139663579178816] Epoch[200] Batch[0] avg_epoch_loss=1.991413\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:27 INFO 140252856112960] Epoch[198] Batch[5] avg_epoch_loss=2.031023\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:27 INFO 140252856112960] Epoch[198] Batch [5]#011Speed: 151.22 samples/sec#011loss=2.031023\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:28 INFO 139663579178816] Epoch[200] Batch[5] avg_epoch_loss=2.068807\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:28 INFO 139663579178816] Epoch[200] Batch [5]#011Speed: 154.14 samples/sec#011loss=2.068807\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:28 INFO 140624803325760] Epoch[203] Batch[0] avg_epoch_loss=2.074743\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:29 INFO 140196492334912] Epoch[196] Batch[10] avg_epoch_loss=2.221628\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:29 INFO 140196492334912] Epoch[196] Batch [10]#011Speed: 117.00 samples/sec#011loss=2.591987\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:29 INFO 140196492334912] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12078.848123550415, \"sum\": 12078.848123550415, \"min\": 12078.848123550415}}, \"EndTime\": 1538409929.523382, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409917.444279}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.797166944 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:31 INFO 140196492334912] Epoch[197] Batch[0] avg_epoch_loss=2.325489\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:30 INFO 140252856112960] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10421.410083770752, \"sum\": 10421.410083770752, \"min\": 10421.410083770752}}, \"EndTime\": 1538409930.948939, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409920.527275}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.231750525 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:32 INFO 140624803325760] Epoch[203] Batch[5] avg_epoch_loss=1.983610\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:32 INFO 140624803325760] Epoch[203] Batch [5]#011Speed: 159.95 samples/sec#011loss=1.983610\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:33 INFO 140252856112960] Epoch[199] Batch[0] avg_epoch_loss=1.923549\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:33 INFO 139663579178816] Epoch[200] Batch[10] avg_epoch_loss=2.235638\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:33 INFO 139663579178816] Epoch[200] Batch [10]#011Speed: 118.75 samples/sec#011loss=2.435834\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:33 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11878.046035766602, \"sum\": 11878.046035766602, \"min\": 11878.046035766602}}, \"EndTime\": 1538409933.896251, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409922.017958}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=108.349997395 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:35 INFO 139663579178816] Epoch[201] Batch[0] avg_epoch_loss=2.039441\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:36 INFO 140196492334912] Epoch[197] Batch[5] avg_epoch_loss=2.036464\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:36 INFO 140196492334912] Epoch[197] Batch [5]#011Speed: 118.96 samples/sec#011loss=2.036464\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:38 INFO 140624803325760] Epoch[203] Batch[10] avg_epoch_loss=2.056848\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:38 INFO 140624803325760] Epoch[203] Batch [10]#011Speed: 123.83 samples/sec#011loss=2.144733\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:38 INFO 140624803325760] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11464.596033096313, \"sum\": 11464.596033096313, \"min\": 11464.596033096313}}, \"EndTime\": 1538409938.156371, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409926.691533}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:38 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.042467792 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:38 INFO 140624803325760] #progress_metric: host=algo-3, completed 81 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:37 INFO 140252856112960] Epoch[199] Batch[5] avg_epoch_loss=2.057688\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:37 INFO 140252856112960] Epoch[199] Batch [5]#011Speed: 145.33 samples/sec#011loss=2.057688\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:39 INFO 140624803325760] Epoch[204] Batch[0] avg_epoch_loss=2.574508\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:40 INFO 139663579178816] Epoch[201] Batch[5] avg_epoch_loss=2.226434\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:40 INFO 139663579178816] Epoch[201] Batch [5]#011Speed: 121.26 samples/sec#011loss=2.226434\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:40 INFO 140196492334912] Epoch[197] Batch[10] avg_epoch_loss=2.085733\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:40 INFO 140196492334912] Epoch[197] Batch [10]#011Speed: 148.09 samples/sec#011loss=2.144856\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:40 INFO 140196492334912] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11327.972888946533, \"sum\": 11327.972888946533, \"min\": 11327.972888946533}}, \"EndTime\": 1538409940.851679, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409929.523465}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:40 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.200273977 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:40 INFO 140196492334912] #progress_metric: host=algo-4, completed 79 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:41 INFO 140252856112960] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10678.90191078186, \"sum\": 10678.90191078186, \"min\": 10678.90191078186}}, \"EndTime\": 1538409941.628183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409930.949025}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:41 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.456341142 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:41 INFO 140252856112960] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:43 INFO 140196492334912] Epoch[198] Batch[0] avg_epoch_loss=2.062045\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:44 INFO 140252856112960] Epoch[200] Batch[0] avg_epoch_loss=1.984582\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:44 INFO 139663579178816] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10716.624021530151, \"sum\": 10716.624021530151, \"min\": 10716.624021530151}}, \"EndTime\": 1538409944.6132, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409933.896338}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.173451739 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 80 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:44 INFO 140624803325760] Epoch[204] Batch[5] avg_epoch_loss=2.210190\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:44 INFO 140624803325760] Epoch[204] Batch [5]#011Speed: 126.77 samples/sec#011loss=2.210190\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:48 INFO 140624803325760] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10270.330905914307, \"sum\": 10270.330905914307, \"min\": 10270.330905914307}}, \"EndTime\": 1538409948.427011, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409938.156457}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.929615725 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:48 INFO 140252856112960] Epoch[200] Batch[5] avg_epoch_loss=1.914053\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:48 INFO 140252856112960] Epoch[200] Batch [5]#011Speed: 150.83 samples/sec#011loss=1.914053\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 16:05:46 INFO 139663579178816] Epoch[202] Batch[0] avg_epoch_loss=2.077344\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:47 INFO 140196492334912] Epoch[198] Batch[5] avg_epoch_loss=2.093919\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:47 INFO 140196492334912] Epoch[198] Batch [5]#011Speed: 150.90 samples/sec#011loss=2.093919\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:50 INFO 140624803325760] Epoch[205] Batch[0] avg_epoch_loss=2.094544\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:51 INFO 140196492334912] processed a total of 1226 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10452.822923660278, \"sum\": 10452.822923660278, \"min\": 10452.822923660278}}, \"EndTime\": 1538409951.304831, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409940.851762}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:51 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.287362639 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:51 INFO 140196492334912] #progress_metric: host=algo-4, completed 79 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:51 INFO 139663579178816] Epoch[202] Batch[5] avg_epoch_loss=2.111343\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:51 INFO 139663579178816] Epoch[202] Batch [5]#011Speed: 120.91 samples/sec#011loss=2.111343\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:52 INFO 140252856112960] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10554.961919784546, \"sum\": 10554.961919784546, \"min\": 10554.961919784546}}, \"EndTime\": 1538409952.183497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409941.628275}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:52 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=115.962933897 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:52 INFO 140252856112960] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:53 INFO 140196492334912] Epoch[199] Batch[0] avg_epoch_loss=1.929458\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:54 INFO 140252856112960] Epoch[201] Batch[0] avg_epoch_loss=1.864373\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:55 INFO 139663579178816] processed a total of 1208 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10707.907915115356, \"sum\": 10707.907915115356, \"min\": 10707.907915115356}}, \"EndTime\": 1538409955.321412, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409944.613273}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:55 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.812712638 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:55 INFO 139663579178816] #progress_metric: host=algo-2, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:57 INFO 140196492334912] Epoch[199] Batch[5] avg_epoch_loss=1.974935\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:05:57 INFO 140196492334912] Epoch[199] Batch [5]#011Speed: 150.74 samples/sec#011loss=1.974935\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:55 INFO 140624803325760] Epoch[205] Batch[5] avg_epoch_loss=1.951208\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:55 INFO 140624803325760] Epoch[205] Batch [5]#011Speed: 126.99 samples/sec#011loss=1.951208\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:05:56 INFO 139663579178816] Epoch[203] Batch[0] avg_epoch_loss=2.068723\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:59 INFO 140624803325760] Epoch[205] Batch[10] avg_epoch_loss=2.023027\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:59 INFO 140624803325760] Epoch[205] Batch [10]#011Speed: 155.88 samples/sec#011loss=2.109210\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:59 INFO 140624803325760] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10742.032051086426, \"sum\": 10742.032051086426, \"min\": 10742.032051086426}}, \"EndTime\": 1538409959.169343, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409948.427084}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:59 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.856573366 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:05:59 INFO 140624803325760] #progress_metric: host=algo-3, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:58 INFO 140252856112960] Epoch[201] Batch[5] avg_epoch_loss=2.128987\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:05:58 INFO 140252856112960] Epoch[201] Batch [5]#011Speed: 149.97 samples/sec#011loss=2.128987\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:01 INFO 140624803325760] Epoch[206] Batch[0] avg_epoch_loss=2.004621\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:01 INFO 140196492334912] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10460.917949676514, \"sum\": 10460.917949676514, \"min\": 10460.917949676514}}, \"EndTime\": 1538409961.766091, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409951.304922}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:01 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.351182502 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:01 INFO 140196492334912] #progress_metric: host=algo-4, completed 80 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:02 INFO 139663579178816] Epoch[203] Batch[5] avg_epoch_loss=2.091231\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:02 INFO 139663579178816] Epoch[203] Batch [5]#011Speed: 120.55 samples/sec#011loss=2.091231\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:02 INFO 140252856112960] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10535.624027252197, \"sum\": 10535.624027252197, \"min\": 10535.624027252197}}, \"EndTime\": 1538409962.71947, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409952.183588}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.738476743 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:04 INFO 140196492334912] Epoch[200] Batch[0] avg_epoch_loss=1.944762\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:05 INFO 140624803325760] Epoch[206] Batch[5] avg_epoch_loss=1.950401\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:05 INFO 140624803325760] Epoch[206] Batch [5]#011Speed: 159.92 samples/sec#011loss=1.950401\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:05 INFO 140252856112960] Epoch[202] Batch[0] avg_epoch_loss=1.964257\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:06 INFO 139663579178816] Epoch[203] Batch[10] avg_epoch_loss=2.163807\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:06 INFO 139663579178816] Epoch[203] Batch [10]#011Speed: 148.22 samples/sec#011loss=2.250899\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:06 INFO 139663579178816] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11212.73398399353, \"sum\": 11212.73398399353, \"min\": 11212.73398399353}}, \"EndTime\": 1538409966.534459, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409955.321481}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.600546016 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 81 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:09 INFO 140252856112960] Epoch[202] Batch[5] avg_epoch_loss=2.076325\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:09 INFO 140252856112960] Epoch[202] Batch [5]#011Speed: 148.53 samples/sec#011loss=2.076325\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:08 INFO 139663579178816] Epoch[204] Batch[0] avg_epoch_loss=1.805549\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:08 INFO 140196492334912] Epoch[200] Batch[5] avg_epoch_loss=1.996625\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:08 INFO 140196492334912] Epoch[200] Batch [5]#011Speed: 150.04 samples/sec#011loss=1.996625\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:10 INFO 140624803325760] Epoch[206] Batch[10] avg_epoch_loss=1.937814\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:10 INFO 140624803325760] Epoch[206] Batch [10]#011Speed: 123.85 samples/sec#011loss=1.922709\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:10 INFO 140624803325760] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11453.90510559082, \"sum\": 11453.90510559082, \"min\": 11453.90510559082}}, \"EndTime\": 1538409970.623545, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409959.169414}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:10 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.53693632 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:10 INFO 140624803325760] #progress_metric: host=algo-3, completed 82 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:12 INFO 140624803325760] Epoch[207] Batch[0] avg_epoch_loss=1.624140\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:12 INFO 139663579178816] Epoch[204] Batch[5] avg_epoch_loss=1.968251\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:12 INFO 139663579178816] Epoch[204] Batch [5]#011Speed: 155.97 samples/sec#011loss=1.968251\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:13 INFO 140196492334912] Epoch[200] Batch[10] avg_epoch_loss=2.065138\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:13 INFO 140196492334912] Epoch[200] Batch [10]#011Speed: 118.11 samples/sec#011loss=2.147353\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:13 INFO 140196492334912] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12080.655813217163, \"sum\": 12080.655813217163, \"min\": 12080.655813217163}}, \"EndTime\": 1538409973.847099, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409961.766182}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:13 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=106.698319847 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:13 INFO 140196492334912] #progress_metric: host=algo-4, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:14 INFO 140252856112960] Epoch[202] Batch[10] avg_epoch_loss=2.206092\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:14 INFO 140252856112960] Epoch[202] Batch [10]#011Speed: 117.16 samples/sec#011loss=2.361813\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:14 INFO 140252856112960] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12200.278043746948, \"sum\": 12200.278043746948, \"min\": 12200.278043746948}}, \"EndTime\": 1538409974.920093, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409962.71956}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=105.89814145 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:15 INFO 140196492334912] Epoch[201] Batch[0] avg_epoch_loss=2.122707\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:16 INFO 140252856112960] Epoch[203] Batch[0] avg_epoch_loss=1.739297\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 16:06:16 INFO 139663579178816] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10219.334125518799, \"sum\": 10219.334125518799, \"min\": 10219.334125518799}}, \"EndTime\": 1538409976.754123, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409966.534544}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.783376117 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 82 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:17 INFO 140624803325760] Epoch[207] Batch[5] avg_epoch_loss=1.817725\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:17 INFO 140624803325760] Epoch[207] Batch [5]#011Speed: 124.08 samples/sec#011loss=1.817725\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:19 INFO 139663579178816] Epoch[205] Batch[0] avg_epoch_loss=1.993370\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:20 INFO 140196492334912] Epoch[201] Batch[5] avg_epoch_loss=2.119348\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:20 INFO 140196492334912] Epoch[201] Batch [5]#011Speed: 119.45 samples/sec#011loss=2.119348\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:21 INFO 140624803325760] Epoch[207] Batch[10] avg_epoch_loss=1.950309\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:21 INFO 140624803325760] Epoch[207] Batch [10]#011Speed: 152.42 samples/sec#011loss=2.109409\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:21 INFO 140624803325760] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11091.753005981445, \"sum\": 11091.753005981445, \"min\": 11091.753005981445}}, \"EndTime\": 1538409981.715611, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409970.623624}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:21 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.383387942 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:21 INFO 140624803325760] #progress_metric: host=algo-3, completed 83 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:21 INFO 140252856112960] Epoch[203] Batch[5] avg_epoch_loss=1.980109\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:21 INFO 140252856112960] Epoch[203] Batch [5]#011Speed: 119.38 samples/sec#011loss=1.980109\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:23 INFO 139663579178816] Epoch[205] Batch[5] avg_epoch_loss=1.924649\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:23 INFO 139663579178816] Epoch[205] Batch [5]#011Speed: 156.17 samples/sec#011loss=1.924649\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:24 INFO 140624803325760] Epoch[208] Batch[0] avg_epoch_loss=1.801587\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:24 INFO 140196492334912] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10801.943063735962, \"sum\": 10801.943063735962, \"min\": 10801.943063735962}}, \"EndTime\": 1538409984.649388, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409973.847192}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:24 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.792388243 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:24 INFO 140196492334912] #progress_metric: host=algo-4, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:26 INFO 140196492334912] Epoch[202] Batch[0] avg_epoch_loss=1.731297\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:26 INFO 140252856112960] Epoch[203] Batch[10] avg_epoch_loss=2.052596\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:26 INFO 140252856112960] Epoch[203] Batch [10]#011Speed: 148.81 samples/sec#011loss=2.139581\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:26 INFO 140252856112960] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11319.773197174072, \"sum\": 11319.773197174072, \"min\": 11319.773197174072}}, \"EndTime\": 1538409986.24019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409974.920177}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:26 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.488846084 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:26 INFO 140252856112960] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:28 INFO 140624803325760] Epoch[208] Batch[5] avg_epoch_loss=2.028437\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:28 INFO 140624803325760] Epoch[208] Batch [5]#011Speed: 154.66 samples/sec#011loss=2.028437\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:28 INFO 139663579178816] Epoch[205] Batch[10] avg_epoch_loss=2.262991\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:28 INFO 139663579178816] Epoch[205] Batch [10]#011Speed: 121.55 samples/sec#011loss=2.669003\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:28 INFO 139663579178816] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11743.689060211182, \"sum\": 11743.689060211182, \"min\": 11743.689060211182}}, \"EndTime\": 1538409988.498159, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409976.754212}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:28 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=109.334036906 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:28 INFO 139663579178816] #progress_metric: host=algo-2, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:28 INFO 140252856112960] Epoch[204] Batch[0] avg_epoch_loss=2.363949\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:30 INFO 139663579178816] Epoch[206] Batch[0] avg_epoch_loss=2.247411\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:31 INFO 140196492334912] Epoch[202] Batch[5] avg_epoch_loss=1.944184\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:31 INFO 140196492334912] Epoch[202] Batch [5]#011Speed: 120.69 samples/sec#011loss=1.944184\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:32 INFO 140624803325760] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10303.004026412964, \"sum\": 10303.004026412964, \"min\": 10303.004026412964}}, \"EndTime\": 1538409992.018911, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409981.715682}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:32 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.516649405 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:32 INFO 140624803325760] #progress_metric: host=algo-3, completed 83 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:32 INFO 140252856112960] Epoch[204] Batch[5] avg_epoch_loss=2.078315\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:32 INFO 140252856112960] Epoch[204] Batch [5]#011Speed: 154.34 samples/sec#011loss=2.078315\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:34 INFO 140624803325760] Epoch[209] Batch[0] avg_epoch_loss=2.113703\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:35 INFO 139663579178816] Epoch[206] Batch[5] avg_epoch_loss=2.013136\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:35 INFO 139663579178816] Epoch[206] Batch [5]#011Speed: 125.16 samples/sec#011loss=2.013136\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:35 INFO 140196492334912] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10761.034965515137, \"sum\": 10761.034965515137, \"min\": 10761.034965515137}}, \"EndTime\": 1538409995.410822, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409984.64951}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.38870142 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 81 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:36 INFO 140252856112960] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10128.741025924683, \"sum\": 10128.741025924683, \"min\": 10128.741025924683}}, \"EndTime\": 1538409996.369229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409986.240263}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:36 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=126.371626482 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:36 INFO 140252856112960] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:38 INFO 139663579178816] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10396.253824234009, \"sum\": 10396.253824234009, \"min\": 10396.253824234009}}, \"EndTime\": 1538409998.894742, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409988.49825}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:38 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.25247558 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:38 INFO 139663579178816] #progress_metric: host=algo-2, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:38 INFO 140252856112960] Epoch[205] Batch[0] avg_epoch_loss=1.973058\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:06:37 INFO 140196492334912] Epoch[203] Batch[0] avg_epoch_loss=2.462960\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:38 INFO 140624803325760] Epoch[209] Batch[5] avg_epoch_loss=2.083109\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:38 INFO 140624803325760] Epoch[209] Batch [5]#011Speed: 153.75 samples/sec#011loss=2.083109\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:40 INFO 139663579178816] Epoch[207] Batch[0] avg_epoch_loss=2.355019\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:42 INFO 140196492334912] Epoch[203] Batch[5] avg_epoch_loss=2.142973\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:42 INFO 140196492334912] Epoch[203] Batch [5]#011Speed: 120.35 samples/sec#011loss=2.142973\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:42 INFO 140252856112960] Epoch[205] Batch[5] avg_epoch_loss=2.069051\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:42 INFO 140252856112960] Epoch[205] Batch [5]#011Speed: 152.92 samples/sec#011loss=2.069051\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:43 INFO 140624803325760] Epoch[209] Batch[10] avg_epoch_loss=2.134417\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:43 INFO 140624803325760] Epoch[209] Batch [10]#011Speed: 120.62 samples/sec#011loss=2.195986\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:43 INFO 140624803325760] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11811.002016067505, \"sum\": 11811.002016067505, \"min\": 11811.002016067505}}, \"EndTime\": 1538410003.830227, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409992.018987}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.319587994 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 84 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:45 INFO 140624803325760] Epoch[210] Batch[0] avg_epoch_loss=1.731927\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:45 INFO 139663579178816] Epoch[207] Batch[5] avg_epoch_loss=2.007328\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:45 INFO 139663579178816] Epoch[207] Batch [5]#011Speed: 124.45 samples/sec#011loss=2.007328\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:46 INFO 140196492334912] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10736.943006515503, \"sum\": 10736.943006515503, \"min\": 10736.943006515503}}, \"EndTime\": 1538410006.148103, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409995.410907}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:46 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.188635445 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:46 INFO 140196492334912] #progress_metric: host=algo-4, completed 81 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:49 INFO 139663579178816] Epoch[207] Batch[10] avg_epoch_loss=1.823426\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:49 INFO 139663579178816] Epoch[207] Batch [10]#011Speed: 152.48 samples/sec#011loss=1.602743\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:49 INFO 139663579178816] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10962.893962860107, \"sum\": 10962.893962860107, \"min\": 10962.893962860107}}, \"EndTime\": 1538410009.857952, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409998.89482}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.040069088 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:47 INFO 140196492334912] Epoch[204] Batch[0] avg_epoch_loss=2.312635\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:48 INFO 140252856112960] Epoch[205] Batch[10] avg_epoch_loss=2.047721\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:48 INFO 140252856112960] Epoch[205] Batch [10]#011Speed: 121.04 samples/sec#011loss=2.022126\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:48 INFO 140252856112960] processed a total of 1343 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11807.248830795288, \"sum\": 11807.248830795288, \"min\": 11807.248830795288}}, \"EndTime\": 1538410008.176787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538409996.369308}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:48 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.742670535 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:48 INFO 140252856112960] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:49 INFO 140252856112960] Epoch[206] Batch[0] avg_epoch_loss=1.723072\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:50 INFO 140624803325760] Epoch[210] Batch[5] avg_epoch_loss=1.852023\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:50 INFO 140624803325760] Epoch[210] Batch [5]#011Speed: 119.83 samples/sec#011loss=1.852023\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:52 INFO 139663579178816] Epoch[208] Batch[0] avg_epoch_loss=1.900312\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:53 INFO 140196492334912] Epoch[204] Batch[5] avg_epoch_loss=1.885571\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:53 INFO 140196492334912] Epoch[204] Batch [5]#011Speed: 121.43 samples/sec#011loss=1.885571\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:55 INFO 140624803325760] Epoch[210] Batch[10] avg_epoch_loss=1.620055\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:55 INFO 140624803325760] Epoch[210] Batch [10]#011Speed: 146.74 samples/sec#011loss=1.341692\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:55 INFO 140624803325760] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11334.327936172485, \"sum\": 11334.327936172485, \"min\": 11334.327936172485}}, \"EndTime\": 1538410015.16489, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410003.83032}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:55 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.988720734 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:55 INFO 140624803325760] #progress_metric: host=algo-3, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:54 INFO 140252856112960] Epoch[206] Batch[5] avg_epoch_loss=1.848759\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:54 INFO 140252856112960] Epoch[206] Batch [5]#011Speed: 123.71 samples/sec#011loss=1.848759\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:56 INFO 139663579178816] Epoch[208] Batch[5] avg_epoch_loss=1.894621\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:06:56 INFO 139663579178816] Epoch[208] Batch [5]#011Speed: 158.92 samples/sec#011loss=1.894621\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:57 INFO 140196492334912] Epoch[204] Batch[10] avg_epoch_loss=1.962488\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:57 INFO 140196492334912] Epoch[204] Batch [10]#011Speed: 148.48 samples/sec#011loss=2.054789\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:57 INFO 140196492334912] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11187.633991241455, \"sum\": 11187.633991241455, \"min\": 11187.633991241455}}, \"EndTime\": 1538410017.336092, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410006.148188}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:57 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.857706311 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:57 INFO 140196492334912] #progress_metric: host=algo-4, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:59 INFO 140252856112960] Epoch[206] Batch[10] avg_epoch_loss=1.970728\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:59 INFO 140252856112960] Epoch[206] Batch [10]#011Speed: 150.97 samples/sec#011loss=2.117090\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:59 INFO 140252856112960] processed a total of 1357 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11019.673109054565, \"sum\": 11019.673109054565, \"min\": 11019.673109054565}}, \"EndTime\": 1538410019.19677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410008.17686}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:59 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=123.142237149 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:06:59 INFO 140252856112960] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:06:57 INFO 140624803325760] Epoch[211] Batch[0] avg_epoch_loss=1.999370\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:06:59 INFO 140196492334912] Epoch[205] Batch[0] avg_epoch_loss=2.224122\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:01 INFO 139663579178816] Epoch[208] Batch[10] avg_epoch_loss=1.703426\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:01 INFO 139663579178816] Epoch[208] Batch [10]#011Speed: 123.06 samples/sec#011loss=1.473992\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:01 INFO 139663579178816] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11527.681827545166, \"sum\": 11527.681827545166, \"min\": 11527.681827545166}}, \"EndTime\": 1538410021.385926, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410009.85802}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.643208271 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 83 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:01 INFO 140624803325760] Epoch[211] Batch[5] avg_epoch_loss=2.135577\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:01 INFO 140624803325760] Epoch[211] Batch [5]#011Speed: 151.53 samples/sec#011loss=2.135577\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:01 INFO 140252856112960] Epoch[207] Batch[0] avg_epoch_loss=2.439666\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:03 INFO 139663579178816] Epoch[209] Batch[0] avg_epoch_loss=2.070014\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:03 INFO 140196492334912] Epoch[205] Batch[5] avg_epoch_loss=2.084550\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:03 INFO 140196492334912] Epoch[205] Batch [5]#011Speed: 152.97 samples/sec#011loss=2.084550\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:05 INFO 140624803325760] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10386.603116989136, \"sum\": 10386.603116989136, \"min\": 10386.603116989136}}, \"EndTime\": 1538410025.551831, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410015.164974}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:05 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=119.768089283 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:05 INFO 140624803325760] #progress_metric: host=algo-3, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:05 INFO 140252856112960] Epoch[207] Batch[5] avg_epoch_loss=2.265951\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:05 INFO 140252856112960] Epoch[207] Batch [5]#011Speed: 148.96 samples/sec#011loss=2.265951\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:07:07 INFO 140624803325760] Epoch[212] Batch[0] avg_epoch_loss=1.977759\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:08 INFO 139663579178816] Epoch[209] Batch[5] avg_epoch_loss=2.049541\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:08 INFO 139663579178816] Epoch[209] Batch [5]#011Speed: 120.45 samples/sec#011loss=2.049541\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:09 INFO 140196492334912] Epoch[205] Batch[10] avg_epoch_loss=2.059805\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:09 INFO 140196492334912] Epoch[205] Batch [10]#011Speed: 118.72 samples/sec#011loss=2.030112\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:09 INFO 140196492334912] processed a total of 1342 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11933.883905410767, \"sum\": 11933.883905410767, \"min\": 11933.883905410767}}, \"EndTime\": 1538410029.270308, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410017.336174}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:09 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.451876601 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:09 INFO 140196492334912] #progress_metric: host=algo-4, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:09 INFO 140252856112960] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10639.594078063965, \"sum\": 10639.594078063965, \"min\": 10639.594078063965}}, \"EndTime\": 1538410029.836673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410019.196841}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.450334151 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:10 INFO 140196492334912] Epoch[206] Batch[0] avg_epoch_loss=2.548647\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:11 INFO 140624803325760] Epoch[212] Batch[5] avg_epoch_loss=1.976323\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:11 INFO 140624803325760] Epoch[212] Batch [5]#011Speed: 157.33 samples/sec#011loss=1.976323\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:12 INFO 140252856112960] Epoch[208] Batch[0] avg_epoch_loss=2.227660\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:12 INFO 139663579178816] Epoch[209] Batch[10] avg_epoch_loss=2.098742\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:12 INFO 139663579178816] Epoch[209] Batch [10]#011Speed: 147.52 samples/sec#011loss=2.157784\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:12 INFO 139663579178816] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11321.454048156738, \"sum\": 11321.454048156738, \"min\": 11321.454048156738}}, \"EndTime\": 1538410032.707676, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410021.386005}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:12 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.825195407 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:12 INFO 139663579178816] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:15 INFO 139663579178816] Epoch[210] Batch[0] avg_epoch_loss=2.083401\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:16 INFO 140196492334912] Epoch[206] Batch[5] avg_epoch_loss=2.019473\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:16 INFO 140196492334912] Epoch[206] Batch [5]#011Speed: 122.02 samples/sec#011loss=2.019473\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:15 INFO 140624803325760] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10083.836078643799, \"sum\": 10083.836078643799, \"min\": 10083.836078643799}}, \"EndTime\": 1538410035.63601, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410025.551922}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:15 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.281841316 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:15 INFO 140624803325760] #progress_metric: host=algo-3, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:16 INFO 140252856112960] Epoch[208] Batch[5] avg_epoch_loss=1.933793\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:16 INFO 140252856112960] Epoch[208] Batch [5]#011Speed: 152.13 samples/sec#011loss=1.933793\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:17 INFO 140624803325760] Epoch[213] Batch[0] avg_epoch_loss=1.661194\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:19 INFO 139663579178816] Epoch[210] Batch[5] avg_epoch_loss=2.088345\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:19 INFO 139663579178816] Epoch[210] Batch [5]#011Speed: 152.39 samples/sec#011loss=2.088345\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:19 INFO 140196492334912] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10603.57117652893, \"sum\": 10603.57117652893, \"min\": 10603.57117652893}}, \"EndTime\": 1538410039.874164, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410029.270372}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.97784175 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:21 INFO 140196492334912] Epoch[207] Batch[0] avg_epoch_loss=1.865903\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:21 INFO 140624803325760] Epoch[213] Batch[5] avg_epoch_loss=1.910890\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:21 INFO 140624803325760] Epoch[213] Batch [5]#011Speed: 158.56 samples/sec#011loss=1.910890\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:21 INFO 140252856112960] Epoch[208] Batch[10] avg_epoch_loss=1.687919\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:21 INFO 140252856112960] Epoch[208] Batch [10]#011Speed: 119.34 samples/sec#011loss=1.392871\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:21 INFO 140252856112960] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11918.3828830719, \"sum\": 11918.3828830719, \"min\": 11918.3828830719}}, \"EndTime\": 1538410041.755402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410029.836764}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.087731838 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:23 INFO 140252856112960] Epoch[209] Batch[0] avg_epoch_loss=1.891981\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:23 INFO 139663579178816] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10440.719842910767, \"sum\": 10440.719842910767, \"min\": 10440.719842910767}}, \"EndTime\": 1538410043.148695, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410032.707746}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:23 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.35048358 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:23 INFO 139663579178816] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:25 INFO 139663579178816] Epoch[211] Batch[0] avg_epoch_loss=2.264074\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:26 INFO 140196492334912] Epoch[207] Batch[5] avg_epoch_loss=1.980516\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:26 INFO 140196492334912] Epoch[207] Batch [5]#011Speed: 122.90 samples/sec#011loss=1.980516\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:27 INFO 140624803325760] Epoch[213] Batch[10] avg_epoch_loss=1.907428\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:27 INFO 140624803325760] Epoch[213] Batch [10]#011Speed: 124.74 samples/sec#011loss=1.903274\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:27 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11490.14401435852, \"sum\": 11490.14401435852, \"min\": 11490.14401435852}}, \"EndTime\": 1538410047.126502, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410035.636087}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:27 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.054278944 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:27 INFO 140624803325760] #progress_metric: host=algo-3, completed 85 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:28 INFO 140624803325760] Epoch[214] Batch[0] avg_epoch_loss=1.812418\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:30 INFO 140196492334912] Epoch[207] Batch[10] avg_epoch_loss=2.023452\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:30 INFO 140196492334912] Epoch[207] Batch [10]#011Speed: 150.51 samples/sec#011loss=2.074975\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:30 INFO 140196492334912] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11060.595989227295, \"sum\": 11060.595989227295, \"min\": 11060.595989227295}}, \"EndTime\": 1538410050.9351, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410039.874235}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.437156878 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 83 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:07:28 INFO 140252856112960] Epoch[209] Batch[5] avg_epoch_loss=2.075814\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:28 INFO 140252856112960] Epoch[209] Batch [5]#011Speed: 120.93 samples/sec#011loss=2.075814\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:29 INFO 139663579178816] Epoch[211] Batch[5] avg_epoch_loss=2.029456\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:29 INFO 139663579178816] Epoch[211] Batch [5]#011Speed: 156.77 samples/sec#011loss=2.029456\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:33 INFO 140196492334912] Epoch[208] Batch[0] avg_epoch_loss=2.054261\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:33 INFO 140252856112960] Epoch[209] Batch[10] avg_epoch_loss=2.060142\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:33 INFO 140252856112960] Epoch[209] Batch [10]#011Speed: 147.85 samples/sec#011loss=2.041337\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:33 INFO 140252856112960] processed a total of 1366 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11254.994869232178, \"sum\": 11254.994869232178, \"min\": 11254.994869232178}}, \"EndTime\": 1538410053.010726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410041.755486}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:33 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.366932348 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:33 INFO 140252856112960] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:33 INFO 139663579178816] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10122.326135635376, \"sum\": 10122.326135635376, \"min\": 10122.326135635376}}, \"EndTime\": 1538410053.27133, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410043.148771}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.77232508 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:33 INFO 140624803325760] Epoch[214] Batch[5] avg_epoch_loss=2.038672\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:33 INFO 140624803325760] Epoch[214] Batch [5]#011Speed: 126.73 samples/sec#011loss=2.038672\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:35 INFO 140252856112960] Epoch[210] Batch[0] avg_epoch_loss=2.144070\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:35 INFO 139663579178816] Epoch[212] Batch[0] avg_epoch_loss=2.179902\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:37 INFO 140624803325760] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10263.906955718994, \"sum\": 10263.906955718994, \"min\": 10263.906955718994}}, \"EndTime\": 1538410057.390687, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410047.126556}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:37 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.733136206 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:37 INFO 140624803325760] #progress_metric: host=algo-3, completed 86 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:39 INFO 139663579178816] Epoch[212] Batch[5] avg_epoch_loss=2.100094\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:39 INFO 139663579178816] Epoch[212] Batch [5]#011Speed: 159.61 samples/sec#011loss=2.100094\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:37 INFO 140196492334912] Epoch[208] Batch[5] avg_epoch_loss=1.954429\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:37 INFO 140196492334912] Epoch[208] Batch [5]#011Speed: 152.13 samples/sec#011loss=1.954429\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:39 INFO 140624803325760] Epoch[215] Batch[0] avg_epoch_loss=2.376069\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:39 INFO 140252856112960] Epoch[210] Batch[5] avg_epoch_loss=1.895620\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:39 INFO 140252856112960] Epoch[210] Batch [5]#011Speed: 149.38 samples/sec#011loss=1.895620\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:41 INFO 140196492334912] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10325.057029724121, \"sum\": 10325.057029724121, \"min\": 10325.057029724121}}, \"EndTime\": 1538410061.260492, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410050.935183}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.900893442 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:43 INFO 140196492334912] Epoch[209] Batch[0] avg_epoch_loss=2.107328\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:44 INFO 140624803325760] Epoch[215] Batch[5] avg_epoch_loss=2.027037\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:44 INFO 140624803325760] Epoch[215] Batch [5]#011Speed: 124.13 samples/sec#011loss=2.027037\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:44 INFO 139663579178816] Epoch[212] Batch[10] avg_epoch_loss=2.124435\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:44 INFO 139663579178816] Epoch[212] Batch [10]#011Speed: 126.90 samples/sec#011loss=2.153645\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:44 INFO 139663579178816] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11386.908054351807, \"sum\": 11386.908054351807, \"min\": 11386.908054351807}}, \"EndTime\": 1538410064.658547, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410053.271404}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.955520361 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:45 INFO 140252856112960] Epoch[210] Batch[10] avg_epoch_loss=1.972860\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:45 INFO 140252856112960] Epoch[210] Batch [10]#011Speed: 118.25 samples/sec#011loss=2.065549\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:45 INFO 140252856112960] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12053.620100021362, \"sum\": 12053.620100021362, \"min\": 12053.620100021362}}, \"EndTime\": 1538410065.064688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410053.010811}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:45 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=107.601494099 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:45 INFO 140252856112960] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:46 INFO 139663579178816] Epoch[213] Batch[0] avg_epoch_loss=2.340822\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:46 INFO 140252856112960] Epoch[211] Batch[0] avg_epoch_loss=1.966850\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:47 INFO 140196492334912] Epoch[209] Batch[5] avg_epoch_loss=2.023632\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:47 INFO 140196492334912] Epoch[209] Batch [5]#011Speed: 152.67 samples/sec#011loss=2.023632\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:48 INFO 140624803325760] Epoch[215] Batch[10] avg_epoch_loss=2.127734\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:48 INFO 140624803325760] Epoch[215] Batch [10]#011Speed: 149.34 samples/sec#011loss=2.248569\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:48 INFO 140624803325760] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11158.31184387207, \"sum\": 11158.31184387207, \"min\": 11158.31184387207}}, \"EndTime\": 1538410068.549355, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410057.390766}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:48 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.697185024 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:48 INFO 140624803325760] #progress_metric: host=algo-3, completed 86 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:50 INFO 140624803325760] Epoch[216] Batch[0] avg_epoch_loss=1.793544\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:51 INFO 139663579178816] Epoch[213] Batch[5] avg_epoch_loss=1.876358\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:51 INFO 139663579178816] Epoch[213] Batch [5]#011Speed: 129.61 samples/sec#011loss=1.876358\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:51 INFO 140252856112960] Epoch[211] Batch[5] avg_epoch_loss=2.027496\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:51 INFO 140252856112960] Epoch[211] Batch [5]#011Speed: 121.52 samples/sec#011loss=2.027496\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:53 INFO 140196492334912] Epoch[209] Batch[10] avg_epoch_loss=2.130589\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:53 INFO 140196492334912] Epoch[209] Batch [10]#011Speed: 121.82 samples/sec#011loss=2.258936\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:53 INFO 140196492334912] processed a total of 1320 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11839.734077453613, \"sum\": 11839.734077453613, \"min\": 11839.734077453613}}, \"EndTime\": 1538410073.100575, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410061.260584}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:53 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=111.487814088 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:53 INFO 140196492334912] #progress_metric: host=algo-4, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:54 INFO 140196492334912] Epoch[210] Batch[0] avg_epoch_loss=1.804966\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:54 INFO 140624803325760] Epoch[216] Batch[5] avg_epoch_loss=1.967565\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:54 INFO 140624803325760] Epoch[216] Batch [5]#011Speed: 159.28 samples/sec#011loss=1.967565\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:55 INFO 139663579178816] Epoch[213] Batch[10] avg_epoch_loss=1.899479\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:55 INFO 139663579178816] Epoch[213] Batch [10]#011Speed: 157.01 samples/sec#011loss=1.927223\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:55 INFO 139663579178816] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10634.184122085571, \"sum\": 10634.184122085571, \"min\": 10634.184122085571}}, \"EndTime\": 1538410075.293046, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410064.658618}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:55 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.502902668 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:55 INFO 139663579178816] #progress_metric: host=algo-2, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:56 INFO 140252856112960] Epoch[211] Batch[10] avg_epoch_loss=2.074332\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:56 INFO 140252856112960] Epoch[211] Batch [10]#011Speed: 145.30 samples/sec#011loss=2.130534\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:56 INFO 140252856112960] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11230.507135391235, \"sum\": 11230.507135391235, \"min\": 11230.507135391235}}, \"EndTime\": 1538410076.295506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410065.064764}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:56 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.200049316 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:07:56 INFO 140252856112960] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:07:57 INFO 139663579178816] Epoch[214] Batch[0] avg_epoch_loss=1.792793\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:58 INFO 140624803325760] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10016.355037689209, \"sum\": 10016.355037689209, \"min\": 10016.355037689209}}, \"EndTime\": 1538410078.566053, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410068.54944}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:58 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=127.78926354 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:07:58 INFO 140624803325760] #progress_metric: host=algo-3, completed 86 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:01 INFO 139663579178816] Epoch[214] Batch[5] avg_epoch_loss=1.834625\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:01 INFO 139663579178816] Epoch[214] Batch [5]#011Speed: 161.33 samples/sec#011loss=1.834625\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:07:58 INFO 140252856112960] Epoch[212] Batch[0] avg_epoch_loss=1.710264\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:59 INFO 140196492334912] Epoch[210] Batch[5] avg_epoch_loss=1.940591\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:07:59 INFO 140196492334912] Epoch[210] Batch [5]#011Speed: 123.90 samples/sec#011loss=1.940591\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:00 INFO 140624803325760] Epoch[217] Batch[0] avg_epoch_loss=2.200224\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:02 INFO 140252856112960] Epoch[212] Batch[5] avg_epoch_loss=2.043559\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:02 INFO 140252856112960] Epoch[212] Batch [5]#011Speed: 152.90 samples/sec#011loss=2.043559\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:03 INFO 140196492334912] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10498.838901519775, \"sum\": 10498.838901519775, \"min\": 10498.838901519775}}, \"EndTime\": 1538410083.59975, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410073.100657}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:03 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.535556745 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:03 INFO 140196492334912] #progress_metric: host=algo-4, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:05 INFO 140196492334912] Epoch[211] Batch[0] avg_epoch_loss=2.437177\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:04 INFO 140624803325760] Epoch[217] Batch[5] avg_epoch_loss=1.981649\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:04 INFO 140624803325760] Epoch[217] Batch [5]#011Speed: 157.20 samples/sec#011loss=1.981649\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:05 INFO 139663579178816] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9879.56690788269, \"sum\": 9879.56690788269, \"min\": 9879.56690788269}}, \"EndTime\": 1538410085.172914, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410075.293115}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:05 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=128.749135204 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:05 INFO 139663579178816] #progress_metric: host=algo-2, completed 86 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:06 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10325.607061386108, \"sum\": 10325.607061386108, \"min\": 10325.607061386108}}, \"EndTime\": 1538410086.621446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410076.295591}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:06 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=121.444104594 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:06 INFO 140252856112960] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:07 INFO 139663579178816] Epoch[215] Batch[0] avg_epoch_loss=1.951044\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:08 INFO 140252856112960] Epoch[213] Batch[0] avg_epoch_loss=1.931045\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:08 INFO 140624803325760] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10184.432029724121, \"sum\": 10184.432029724121, \"min\": 10184.432029724121}}, \"EndTime\": 1538410088.750837, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410078.566144}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.029323136 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:10 INFO 140196492334912] Epoch[211] Batch[5] avg_epoch_loss=2.110536\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:10 INFO 140196492334912] Epoch[211] Batch [5]#011Speed: 123.14 samples/sec#011loss=2.110536\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:11 INFO 140624803325760] Epoch[218] Batch[0] avg_epoch_loss=1.833248\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:11 INFO 139663579178816] Epoch[215] Batch[5] avg_epoch_loss=1.898338\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:11 INFO 139663579178816] Epoch[215] Batch [5]#011Speed: 160.07 samples/sec#011loss=1.898338\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:13 INFO 140252856112960] Epoch[213] Batch[5] avg_epoch_loss=1.955500\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:13 INFO 140252856112960] Epoch[213] Batch [5]#011Speed: 152.60 samples/sec#011loss=1.955500\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:14 INFO 140196492334912] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10548.446893692017, \"sum\": 10548.446893692017, \"min\": 10548.446893692017}}, \"EndTime\": 1538410094.148547, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410083.599836}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.740988139 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 84 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:15 INFO 140624803325760] Epoch[218] Batch[5] avg_epoch_loss=1.897135\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:15 INFO 140624803325760] Epoch[218] Batch [5]#011Speed: 156.46 samples/sec#011loss=1.897135\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:15 INFO 140196492334912] Epoch[212] Batch[0] avg_epoch_loss=2.048973\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:16 INFO 139663579178816] Epoch[215] Batch[10] avg_epoch_loss=1.873026\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:16 INFO 139663579178816] Epoch[215] Batch [10]#011Speed: 120.93 samples/sec#011loss=1.842652\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:16 INFO 139663579178816] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11573.240041732788, \"sum\": 11573.240041732788, \"min\": 11573.240041732788}}, \"EndTime\": 1538410096.746479, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410085.172989}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.03094655 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 86 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:18 INFO 140252856112960] Epoch[213] Batch[10] avg_epoch_loss=2.167384\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:18 INFO 140252856112960] Epoch[213] Batch [10]#011Speed: 119.49 samples/sec#011loss=2.421646\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:18 INFO 140252856112960] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11830.91402053833, \"sum\": 11830.91402053833, \"min\": 11830.91402053833}}, \"EndTime\": 1538410098.452704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410086.621534}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:18 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.387580368 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:18 INFO 140252856112960] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:18 INFO 139663579178816] Epoch[216] Batch[0] avg_epoch_loss=2.064940\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:20 INFO 140252856112960] Epoch[214] Batch[0] avg_epoch_loss=2.194995\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:20 INFO 140624803325760] Epoch[218] Batch[10] avg_epoch_loss=1.994549\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:20 INFO 140624803325760] Epoch[218] Batch [10]#011Speed: 123.31 samples/sec#011loss=2.111445\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:20 INFO 140624803325760] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11581.255912780762, \"sum\": 11581.255912780762, \"min\": 11581.255912780762}}, \"EndTime\": 1538410100.332441, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410088.750926}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:20 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=111.471891766 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:20 INFO 140624803325760] #progress_metric: host=algo-3, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:20 INFO 140196492334912] Epoch[212] Batch[5] avg_epoch_loss=2.089473\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:20 INFO 140196492334912] Epoch[212] Batch [5]#011Speed: 123.09 samples/sec#011loss=2.089473\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:21 INFO 140624803325760] Epoch[219] Batch[0] avg_epoch_loss=1.778445\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:23 INFO 139663579178816] Epoch[216] Batch[5] avg_epoch_loss=1.900033\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:23 INFO 139663579178816] Epoch[216] Batch [5]#011Speed: 123.79 samples/sec#011loss=1.900033\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:24 INFO 140196492334912] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10511.852025985718, \"sum\": 10511.852025985718, \"min\": 10511.852025985718}}, \"EndTime\": 1538410104.66075, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410094.148635}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:24 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.816487211 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:24 INFO 140196492334912] #progress_metric: host=algo-4, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:25 INFO 140252856112960] Epoch[214] Batch[5] avg_epoch_loss=1.864484\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:25 INFO 140252856112960] Epoch[214] Batch [5]#011Speed: 122.39 samples/sec#011loss=1.864484\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:26 INFO 140196492334912] Epoch[213] Batch[0] avg_epoch_loss=1.778836\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:27 INFO 140624803325760] Epoch[219] Batch[5] avg_epoch_loss=1.901314\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:27 INFO 140624803325760] Epoch[219] Batch [5]#011Speed: 124.12 samples/sec#011loss=1.901314\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:27 INFO 139663579178816] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10514.49990272522, \"sum\": 10514.49990272522, \"min\": 10514.49990272522}}, \"EndTime\": 1538410107.261274, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410096.746555}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:27 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.35493016 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:27 INFO 139663579178816] #progress_metric: host=algo-2, completed 86 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:28 INFO 139663579178816] Epoch[217] Batch[0] avg_epoch_loss=2.510739\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:29 INFO 140252856112960] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10680.263996124268, \"sum\": 10680.263996124268, \"min\": 10680.263996124268}}, \"EndTime\": 1538410109.133296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410098.452787}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:29 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.598482716 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:29 INFO 140252856112960] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:31 INFO 140624803325760] Epoch[219] Batch[10] avg_epoch_loss=2.011125\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:31 INFO 140624803325760] Epoch[219] Batch [10]#011Speed: 155.27 samples/sec#011loss=2.142899\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:31 INFO 140624803325760] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10920.383214950562, \"sum\": 10920.383214950562, \"min\": 10920.383214950562}}, \"EndTime\": 1538410111.253166, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410100.332537}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:31 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.239758354 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:31 INFO 140624803325760] #progress_metric: host=algo-3, completed 88 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:08:30 INFO 140252856112960] Epoch[215] Batch[0] avg_epoch_loss=2.170018\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:31 INFO 140196492334912] Epoch[213] Batch[5] avg_epoch_loss=1.881146\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:31 INFO 140196492334912] Epoch[213] Batch [5]#011Speed: 123.78 samples/sec#011loss=1.881146\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:33 INFO 139663579178816] Epoch[217] Batch[5] avg_epoch_loss=2.117128\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:33 INFO 139663579178816] Epoch[217] Batch [5]#011Speed: 129.63 samples/sec#011loss=2.117128\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:33 INFO 140624803325760] Epoch[220] Batch[0] avg_epoch_loss=2.000148\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:35 INFO 140196492334912] Epoch[213] Batch[10] avg_epoch_loss=1.972235\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:35 INFO 140196492334912] Epoch[213] Batch [10]#011Speed: 152.04 samples/sec#011loss=2.081543\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:35 INFO 140196492334912] processed a total of 1336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11031.578063964844, \"sum\": 11031.578063964844, \"min\": 11031.578063964844}}, \"EndTime\": 1538410115.692696, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410104.660864}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.105463927 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 85 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:36 INFO 140252856112960] Epoch[215] Batch[5] avg_epoch_loss=2.065460\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:36 INFO 140252856112960] Epoch[215] Batch [5]#011Speed: 121.34 samples/sec#011loss=2.065460\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:37 INFO 139663579178816] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10123.172044754028, \"sum\": 10123.172044754028, \"min\": 10123.172044754028}}, \"EndTime\": 1538410117.38475, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410107.261352}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:37 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=126.243967754 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:37 INFO 139663579178816] #progress_metric: host=algo-2, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:37 INFO 140196492334912] Epoch[214] Batch[0] avg_epoch_loss=1.863829\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:37 INFO 140624803325760] Epoch[220] Batch[5] avg_epoch_loss=2.136550\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:37 INFO 140624803325760] Epoch[220] Batch [5]#011Speed: 159.93 samples/sec#011loss=2.136550\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:38 INFO 139663579178816] Epoch[218] Batch[0] avg_epoch_loss=1.938904\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:40 INFO 140252856112960] Epoch[215] Batch[10] avg_epoch_loss=2.161151\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:40 INFO 140252856112960] Epoch[215] Batch [10]#011Speed: 146.95 samples/sec#011loss=2.275980\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:40 INFO 140252856112960] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11226.40085220337, \"sum\": 11226.40085220337, \"min\": 11226.40085220337}}, \"EndTime\": 1538410120.360047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410109.133387}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:40 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.104977829 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:40 INFO 140252856112960] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:42 INFO 140196492334912] Epoch[214] Batch[5] avg_epoch_loss=1.975949\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:42 INFO 140196492334912] Epoch[214] Batch [5]#011Speed: 155.33 samples/sec#011loss=1.975949\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:42 INFO 140624803325760] Epoch[220] Batch[10] avg_epoch_loss=2.069432\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:42 INFO 140624803325760] Epoch[220] Batch [10]#011Speed: 125.62 samples/sec#011loss=1.988892\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:42 INFO 140624803325760] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11480.431079864502, \"sum\": 11480.431079864502, \"min\": 11480.431079864502}}, \"EndTime\": 1538410122.733931, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410111.253251}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:42 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.935173128 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:42 INFO 140624803325760] #progress_metric: host=algo-3, completed 88 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:42 INFO 140252856112960] Epoch[216] Batch[0] avg_epoch_loss=1.797961\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:43 INFO 139663579178816] Epoch[218] Batch[5] avg_epoch_loss=1.985501\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:43 INFO 139663579178816] Epoch[218] Batch [5]#011Speed: 127.76 samples/sec#011loss=1.985501\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:44 INFO 140624803325760] Epoch[221] Batch[0] avg_epoch_loss=1.833124\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:46 INFO 140252856112960] Epoch[216] Batch[5] avg_epoch_loss=1.940724\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:46 INFO 140252856112960] Epoch[216] Batch [5]#011Speed: 155.34 samples/sec#011loss=1.940724\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:48 INFO 139663579178816] Epoch[218] Batch[10] avg_epoch_loss=2.024104\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:48 INFO 139663579178816] Epoch[218] Batch [10]#011Speed: 157.90 samples/sec#011loss=2.070428\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:48 INFO 139663579178816] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10641.55912399292, \"sum\": 10641.55912399292, \"min\": 10641.55912399292}}, \"EndTime\": 1538410128.026603, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410117.384805}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:48 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.221675802 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:48 INFO 139663579178816] #progress_metric: host=algo-2, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:47 INFO 140196492334912] Epoch[214] Batch[10] avg_epoch_loss=2.289161\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:47 INFO 140196492334912] Epoch[214] Batch [10]#011Speed: 119.74 samples/sec#011loss=2.665015\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:47 INFO 140196492334912] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11761.096000671387, \"sum\": 11761.096000671387, \"min\": 11761.096000671387}}, \"EndTime\": 1538410127.454126, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410115.692783}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=108.917272388 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:49 INFO 140196492334912] Epoch[215] Batch[0] avg_epoch_loss=1.904686\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:49 INFO 140624803325760] Epoch[221] Batch[5] avg_epoch_loss=1.904049\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:49 INFO 140624803325760] Epoch[221] Batch [5]#011Speed: 128.95 samples/sec#011loss=1.904049\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:50 INFO 139663579178816] Epoch[219] Batch[0] avg_epoch_loss=2.029005\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:52 INFO 140252856112960] Epoch[216] Batch[10] avg_epoch_loss=1.925045\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:52 INFO 140252856112960] Epoch[216] Batch [10]#011Speed: 121.02 samples/sec#011loss=1.906230\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:52 INFO 140252856112960] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11807.869911193848, \"sum\": 11807.869911193848, \"min\": 11807.869911193848}}, \"EndTime\": 1538410132.168212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410120.360115}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:52 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.994132706 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:52 INFO 140252856112960] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:54 INFO 140196492334912] Epoch[215] Batch[5] avg_epoch_loss=1.894517\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:54 INFO 140196492334912] Epoch[215] Batch [5]#011Speed: 123.16 samples/sec#011loss=1.894517\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:53 INFO 140624803325760] Epoch[221] Batch[10] avg_epoch_loss=2.015754\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:53 INFO 140624803325760] Epoch[221] Batch [10]#011Speed: 158.51 samples/sec#011loss=2.149799\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:53 INFO 140624803325760] processed a total of 1353 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10850.864171981812, \"sum\": 10850.864171981812, \"min\": 10850.864171981812}}, \"EndTime\": 1538410133.58513, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410122.734016}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.689335065 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 88 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:53 INFO 140252856112960] Epoch[217] Batch[0] avg_epoch_loss=2.006109\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:54 INFO 139663579178816] Epoch[219] Batch[5] avg_epoch_loss=1.860869\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:54 INFO 139663579178816] Epoch[219] Batch [5]#011Speed: 162.30 samples/sec#011loss=1.860869\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:55 INFO 140624803325760] Epoch[222] Batch[0] avg_epoch_loss=1.740886\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:57 INFO 139663579178816] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9845.504999160767, \"sum\": 9845.504999160767, \"min\": 9845.504999160767}}, \"EndTime\": 1538410137.872408, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410128.026672}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:57 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.217724611 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:08:57 INFO 139663579178816] #progress_metric: host=algo-2, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:58 INFO 140196492334912] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10599.933862686157, \"sum\": 10599.933862686157, \"min\": 10599.933862686157}}, \"EndTime\": 1538410138.054394, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410127.454209}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:58 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.735127175 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:58 INFO 140196492334912] #progress_metric: host=algo-4, completed 86 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:59 INFO 140252856112960] Epoch[217] Batch[5] avg_epoch_loss=1.919146\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:08:59 INFO 140252856112960] Epoch[217] Batch [5]#011Speed: 120.57 samples/sec#011loss=1.919146\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:08:59 INFO 140196492334912] Epoch[216] Batch[0] avg_epoch_loss=1.639013\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:59 INFO 140624803325760] Epoch[222] Batch[5] avg_epoch_loss=1.974982\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:08:59 INFO 140624803325760] Epoch[222] Batch [5]#011Speed: 161.51 samples/sec#011loss=1.974982\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:00 INFO 139663579178816] Epoch[220] Batch[0] avg_epoch_loss=1.409488\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:03 INFO 140624803325760] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9859.325885772705, \"sum\": 9859.325885772705, \"min\": 9859.325885772705}}, \"EndTime\": 1538410143.444756, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410133.585201}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:03 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=129.824807056 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:03 INFO 140624803325760] #progress_metric: host=algo-3, completed 89 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:03 INFO 140252856112960] Epoch[217] Batch[10] avg_epoch_loss=1.707649\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:03 INFO 140252856112960] Epoch[217] Batch [10]#011Speed: 147.27 samples/sec#011loss=1.453852\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:03 INFO 140252856112960] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11331.326961517334, \"sum\": 11331.326961517334, \"min\": 11331.326961517334}}, \"EndTime\": 1538410143.499839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410132.168284}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:03 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.989868744 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:03 INFO 140252856112960] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 16:09:04 INFO 139663579178816] Epoch[220] Batch[5] avg_epoch_loss=1.889557\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:04 INFO 139663579178816] Epoch[220] Batch [5]#011Speed: 153.92 samples/sec#011loss=1.889557\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:04 INFO 140196492334912] Epoch[216] Batch[5] avg_epoch_loss=2.030371\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:04 INFO 140196492334912] Epoch[216] Batch [5]#011Speed: 121.62 samples/sec#011loss=2.030371\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:05 INFO 140624803325760] Epoch[223] Batch[0] avg_epoch_loss=1.779669\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:05 INFO 140252856112960] Epoch[218] Batch[0] avg_epoch_loss=2.413937\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:08 INFO 140196492334912] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10596.646785736084, \"sum\": 10596.646785736084, \"min\": 10596.646785736084}}, \"EndTime\": 1538410148.651392, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410138.054481}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:08 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.508272115 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:08 INFO 140196492334912] #progress_metric: host=algo-4, completed 86 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:09 INFO 139663579178816] Epoch[220] Batch[10] avg_epoch_loss=1.978574\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:09 INFO 139663579178816] Epoch[220] Batch [10]#011Speed: 124.17 samples/sec#011loss=2.085394\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:09 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11616.854906082153, \"sum\": 11616.854906082153, \"min\": 11616.854906082153}}, \"EndTime\": 1538410149.489572, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410137.872484}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:09 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=112.594011578 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:09 INFO 139663579178816] #progress_metric: host=algo-2, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:10 INFO 140196492334912] Epoch[217] Batch[0] avg_epoch_loss=2.335635\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:09 INFO 140624803325760] Epoch[223] Batch[5] avg_epoch_loss=1.918677\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:09 INFO 140624803325760] Epoch[223] Batch [5]#011Speed: 161.01 samples/sec#011loss=1.918677\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:10 INFO 140252856112960] Epoch[218] Batch[5] avg_epoch_loss=1.989930\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:10 INFO 140252856112960] Epoch[218] Batch [5]#011Speed: 151.22 samples/sec#011loss=1.989930\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:11 INFO 139663579178816] Epoch[221] Batch[0] avg_epoch_loss=2.120417\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:14 INFO 140624803325760] Epoch[223] Batch[10] avg_epoch_loss=1.746258\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:14 INFO 140624803325760] Epoch[223] Batch [10]#011Speed: 126.00 samples/sec#011loss=1.539355\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:14 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11424.098014831543, \"sum\": 11424.098014831543, \"min\": 11424.098014831543}}, \"EndTime\": 1538410154.869172, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410143.444835}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:14 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.719283766 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:14 INFO 140624803325760] #progress_metric: host=algo-3, completed 89 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:15 INFO 140252856112960] Epoch[218] Batch[10] avg_epoch_loss=1.975167\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:15 INFO 140252856112960] Epoch[218] Batch [10]#011Speed: 118.85 samples/sec#011loss=1.957450\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:15 INFO 140252856112960] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12054.167985916138, \"sum\": 12054.167985916138, \"min\": 12054.167985916138}}, \"EndTime\": 1538410155.554304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410143.499911}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:15 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=110.085434507 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:15 INFO 140252856112960] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:16 INFO 139663579178816] Epoch[221] Batch[5] avg_epoch_loss=1.968126\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:16 INFO 139663579178816] Epoch[221] Batch [5]#011Speed: 128.49 samples/sec#011loss=1.968126\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:15 INFO 140196492334912] Epoch[217] Batch[5] avg_epoch_loss=2.074698\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:15 INFO 140196492334912] Epoch[217] Batch [5]#011Speed: 122.49 samples/sec#011loss=2.074698\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:16 INFO 140624803325760] Epoch[224] Batch[0] avg_epoch_loss=2.180103\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:17 INFO 140252856112960] Epoch[219] Batch[0] avg_epoch_loss=2.039923\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:19 INFO 140196492334912] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10651.551008224487, \"sum\": 10651.551008224487, \"min\": 10651.551008224487}}, \"EndTime\": 1538410159.3033, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410148.651485}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.699438495 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 87 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:20 INFO 139663579178816] Epoch[221] Batch[10] avg_epoch_loss=1.976798\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:20 INFO 139663579178816] Epoch[221] Batch [10]#011Speed: 155.76 samples/sec#011loss=1.987204\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:20 INFO 139663579178816] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10704.283952713013, \"sum\": 10704.283952713013, \"min\": 10704.283952713013}}, \"EndTime\": 1538410160.194177, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410149.489641}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:20 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.566581634 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:20 INFO 139663579178816] #progress_metric: host=algo-2, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:20 INFO 140196492334912] Epoch[218] Batch[0] avg_epoch_loss=1.964736\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:21 INFO 140624803325760] Epoch[224] Batch[5] avg_epoch_loss=1.883956\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:21 INFO 140624803325760] Epoch[224] Batch [5]#011Speed: 127.77 samples/sec#011loss=1.883956\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:22 INFO 140252856112960] Epoch[219] Batch[5] avg_epoch_loss=1.962001\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:22 INFO 140252856112960] Epoch[219] Batch [5]#011Speed: 121.18 samples/sec#011loss=1.962001\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:22 INFO 139663579178816] Epoch[222] Batch[0] avg_epoch_loss=1.889896\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:25 INFO 140624803325760] Epoch[224] Batch[10] avg_epoch_loss=2.064280\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:25 INFO 140624803325760] Epoch[224] Batch [10]#011Speed: 156.37 samples/sec#011loss=2.280669\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:25 INFO 140624803325760] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10671.57006263733, \"sum\": 10671.57006263733, \"min\": 10671.57006263733}}, \"EndTime\": 1538410165.541024, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410154.869236}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:25 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.131127277 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:25 INFO 140624803325760] #progress_metric: host=algo-3, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:26 INFO 140252856112960] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10758.234977722168, \"sum\": 10758.234977722168, \"min\": 10758.234977722168}}, \"EndTime\": 1538410166.312848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410155.554376}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:26 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.118322099 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:26 INFO 140252856112960] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:09:26 INFO 140196492334912] Epoch[218] Batch[5] avg_epoch_loss=2.011069\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:26 INFO 140196492334912] Epoch[218] Batch [5]#011Speed: 122.55 samples/sec#011loss=2.011069\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:26 INFO 139663579178816] Epoch[222] Batch[5] avg_epoch_loss=1.895731\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:26 INFO 139663579178816] Epoch[222] Batch [5]#011Speed: 160.22 samples/sec#011loss=1.895731\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:27 INFO 140624803325760] Epoch[225] Batch[0] avg_epoch_loss=1.563392\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:27 INFO 140252856112960] Epoch[220] Batch[0] avg_epoch_loss=1.493527\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:29 INFO 140196492334912] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10658.653974533081, \"sum\": 10658.653974533081, \"min\": 10658.653974533081}}, \"EndTime\": 1538410169.962305, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410159.303385}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:29 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.399807884 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:29 INFO 140196492334912] #progress_metric: host=algo-4, completed 87 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:30 INFO 139663579178816] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9966.123104095459, \"sum\": 9966.123104095459, \"min\": 9966.123104095459}}, \"EndTime\": 1538410170.160599, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410160.194246}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:30 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.420117357 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:30 INFO 139663579178816] #progress_metric: host=algo-2, completed 89 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:31 INFO 140624803325760] Epoch[225] Batch[5] avg_epoch_loss=1.932008\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:31 INFO 140624803325760] Epoch[225] Batch [5]#011Speed: 158.10 samples/sec#011loss=1.932008\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:31 INFO 140196492334912] Epoch[219] Batch[0] avg_epoch_loss=1.959731\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:32 INFO 139663579178816] Epoch[223] Batch[0] avg_epoch_loss=1.890689\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:33 INFO 140252856112960] Epoch[220] Batch[5] avg_epoch_loss=1.758391\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:33 INFO 140252856112960] Epoch[220] Batch [5]#011Speed: 121.03 samples/sec#011loss=1.758391\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:35 INFO 140624803325760] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10047.795057296753, \"sum\": 10047.795057296753, \"min\": 10047.795057296753}}, \"EndTime\": 1538410175.589116, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410165.541094}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:35 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=124.802123919 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:35 INFO 140624803325760] #progress_metric: host=algo-3, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:36 INFO 139663579178816] Epoch[223] Batch[5] avg_epoch_loss=1.949558\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:36 INFO 139663579178816] Epoch[223] Batch [5]#011Speed: 160.35 samples/sec#011loss=1.949558\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:36 INFO 140196492334912] Epoch[219] Batch[5] avg_epoch_loss=1.958315\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:36 INFO 140196492334912] Epoch[219] Batch [5]#011Speed: 122.60 samples/sec#011loss=1.958315\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:37 INFO 140252856112960] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10774.11699295044, \"sum\": 10774.11699295044, \"min\": 10774.11699295044}}, \"EndTime\": 1538410177.087295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410166.312928}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:37 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.253798513 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:37 INFO 140252856112960] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:37 INFO 140624803325760] Epoch[226] Batch[0] avg_epoch_loss=1.948630\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:38 INFO 140252856112960] Epoch[221] Batch[0] avg_epoch_loss=2.015561\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:41 INFO 140196492334912] Epoch[219] Batch[10] avg_epoch_loss=1.883251\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:41 INFO 140196492334912] Epoch[219] Batch [10]#011Speed: 147.07 samples/sec#011loss=1.793174\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:41 INFO 140196492334912] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11201.776027679443, \"sum\": 11201.776027679443, \"min\": 11201.776027679443}}, \"EndTime\": 1538410181.164432, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410169.962408}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.15904527 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 88 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:41 INFO 139663579178816] Epoch[223] Batch[10] avg_epoch_loss=1.964961\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:41 INFO 139663579178816] Epoch[223] Batch [10]#011Speed: 125.54 samples/sec#011loss=1.983445\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:41 INFO 139663579178816] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11361.529111862183, \"sum\": 11361.529111862183, \"min\": 11361.529111862183}}, \"EndTime\": 1538410181.522433, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410170.160675}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:41 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.2441018 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:41 INFO 139663579178816] #progress_metric: host=algo-2, completed 89 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:42 INFO 140624803325760] Epoch[226] Batch[5] avg_epoch_loss=2.054509\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:42 INFO 140624803325760] Epoch[226] Batch [5]#011Speed: 155.77 samples/sec#011loss=2.054509\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:43 INFO 139663579178816] Epoch[224] Batch[0] avg_epoch_loss=2.053428\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:45 INFO 140624803325760] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10201.393127441406, \"sum\": 10201.393127441406, \"min\": 10201.393127441406}}, \"EndTime\": 1538410185.790823, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410175.589191}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:45 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.216834878 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:45 INFO 140624803325760] #progress_metric: host=algo-3, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:43 INFO 140196492334912] Epoch[220] Batch[0] avg_epoch_loss=2.101550\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:44 INFO 140252856112960] Epoch[221] Batch[5] avg_epoch_loss=2.037324\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:44 INFO 140252856112960] Epoch[221] Batch [5]#011Speed: 119.00 samples/sec#011loss=2.037324\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:47 INFO 140196492334912] Epoch[220] Batch[5] avg_epoch_loss=2.002332\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:47 INFO 140196492334912] Epoch[220] Batch [5]#011Speed: 154.43 samples/sec#011loss=2.002332\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:48 INFO 140624803325760] Epoch[227] Batch[0] avg_epoch_loss=1.861049\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:47 INFO 140252856112960] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10728.132009506226, \"sum\": 10728.132009506226, \"min\": 10728.132009506226}}, \"EndTime\": 1538410187.815777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410177.087394}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:47 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.79429374 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:47 INFO 140252856112960] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:48 INFO 139663579178816] Epoch[224] Batch[5] avg_epoch_loss=2.068590\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:48 INFO 139663579178816] Epoch[224] Batch [5]#011Speed: 127.16 samples/sec#011loss=2.068590\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:49 INFO 140252856112960] Epoch[222] Batch[0] avg_epoch_loss=2.123429\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:51 INFO 140196492334912] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10286.327838897705, \"sum\": 10286.327838897705, \"min\": 10286.327838897705}}, \"EndTime\": 1538410191.451092, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410181.164515}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:51 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=123.560521603 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:51 INFO 140196492334912] #progress_metric: host=algo-4, completed 88 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:52 INFO 140624803325760] Epoch[227] Batch[5] avg_epoch_loss=1.986307\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:52 INFO 140624803325760] Epoch[227] Batch [5]#011Speed: 156.47 samples/sec#011loss=1.986307\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:52 INFO 139663579178816] Epoch[224] Batch[10] avg_epoch_loss=1.866874\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:52 INFO 139663579178816] Epoch[224] Batch [10]#011Speed: 155.22 samples/sec#011loss=1.624814\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:52 INFO 139663579178816] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10781.859874725342, \"sum\": 10781.859874725342, \"min\": 10781.859874725342}}, \"EndTime\": 1538410192.304597, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410181.522508}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:52 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.777452455 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:52 INFO 139663579178816] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:53 INFO 140196492334912] Epoch[221] Batch[0] avg_epoch_loss=2.067050\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:09:54 INFO 140252856112960] Epoch[222] Batch[5] avg_epoch_loss=2.159380\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:54 INFO 140252856112960] Epoch[222] Batch [5]#011Speed: 124.52 samples/sec#011loss=2.159380\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:54 INFO 139663579178816] Epoch[225] Batch[0] avg_epoch_loss=1.870959\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:55 INFO 140624803325760] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10114.509105682373, \"sum\": 10114.509105682373, \"min\": 10114.509105682373}}, \"EndTime\": 1538410195.905685, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410185.790916}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:55 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.583206791 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:55 INFO 140624803325760] #progress_metric: host=algo-3, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:58 INFO 140196492334912] Epoch[221] Batch[5] avg_epoch_loss=1.952952\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:09:58 INFO 140196492334912] Epoch[221] Batch [5]#011Speed: 153.79 samples/sec#011loss=1.952952\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:09:58 INFO 140624803325760] Epoch[228] Batch[0] avg_epoch_loss=2.130737\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:58 INFO 140252856112960] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10480.905055999756, \"sum\": 10480.905055999756, \"min\": 10480.905055999756}}, \"EndTime\": 1538410198.297032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410187.815865}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:58 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.885123578 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:58 INFO 140252856112960] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:58 INFO 139663579178816] Epoch[225] Batch[5] avg_epoch_loss=2.003922\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:09:58 INFO 139663579178816] Epoch[225] Batch [5]#011Speed: 160.49 samples/sec#011loss=2.003922\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:09:59 INFO 140252856112960] Epoch[223] Batch[0] avg_epoch_loss=1.954116\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:02 INFO 140624803325760] Epoch[228] Batch[5] avg_epoch_loss=2.038269\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:02 INFO 140624803325760] Epoch[228] Batch [5]#011Speed: 154.02 samples/sec#011loss=2.038269\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:02 INFO 139663579178816] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9941.141128540039, \"sum\": 9941.141128540039, \"min\": 9941.141128540039}}, \"EndTime\": 1538410202.246039, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410192.304667}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:02 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.839316908 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:02 INFO 139663579178816] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:04 INFO 139663579178816] Epoch[226] Batch[0] avg_epoch_loss=1.912721\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:03 INFO 140196492334912] Epoch[221] Batch[10] avg_epoch_loss=1.983413\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:03 INFO 140196492334912] Epoch[221] Batch [10]#011Speed: 121.06 samples/sec#011loss=2.019967\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:03 INFO 140196492334912] processed a total of 1373 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11872.610092163086, \"sum\": 11872.610092163086, \"min\": 11872.610092163086}}, \"EndTime\": 1538410203.324045, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410191.451178}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:03 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.64303317 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:03 INFO 140196492334912] #progress_metric: host=algo-4, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:04 INFO 140196492334912] Epoch[222] Batch[0] avg_epoch_loss=1.966023\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:05 INFO 140252856112960] Epoch[223] Batch[5] avg_epoch_loss=1.883064\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:05 INFO 140252856112960] Epoch[223] Batch [5]#011Speed: 123.27 samples/sec#011loss=1.883064\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:07 INFO 140624803325760] Epoch[228] Batch[10] avg_epoch_loss=1.965746\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:07 INFO 140624803325760] Epoch[228] Batch [10]#011Speed: 122.61 samples/sec#011loss=1.878718\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:07 INFO 140624803325760] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11658.146142959595, \"sum\": 11658.146142959595, \"min\": 11658.146142959595}}, \"EndTime\": 1538410207.564183, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410195.905777}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:07 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.194658927 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:07 INFO 140624803325760] #progress_metric: host=algo-3, completed 91 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:08 INFO 139663579178816] Epoch[226] Batch[5] avg_epoch_loss=1.909563\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:08 INFO 139663579178816] Epoch[226] Batch [5]#011Speed: 158.96 samples/sec#011loss=1.909563\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:09 INFO 140624803325760] Epoch[229] Batch[0] avg_epoch_loss=1.754719\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:09 INFO 140252856112960] Epoch[223] Batch[10] avg_epoch_loss=2.078830\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:09 INFO 140252856112960] Epoch[223] Batch [10]#011Speed: 150.55 samples/sec#011loss=2.313750\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:09 INFO 140252856112960] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11056.979179382324, \"sum\": 11056.979179382324, \"min\": 11056.979179382324}}, \"EndTime\": 1538410209.354341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410198.297112}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.204775421 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:10 INFO 140196492334912] Epoch[222] Batch[5] avg_epoch_loss=1.987539\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:10 INFO 140196492334912] Epoch[222] Batch [5]#011Speed: 122.01 samples/sec#011loss=1.987539\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:11 INFO 140252856112960] Epoch[224] Batch[0] avg_epoch_loss=1.862323\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:13 INFO 139663579178816] Epoch[226] Batch[10] avg_epoch_loss=1.911410\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:13 INFO 139663579178816] Epoch[226] Batch [10]#011Speed: 124.76 samples/sec#011loss=1.913627\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:13 INFO 139663579178816] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11479.995012283325, \"sum\": 11479.995012283325, \"min\": 11479.995012283325}}, \"EndTime\": 1538410213.726344, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410202.246113}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:13 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=116.026630987 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:13 INFO 139663579178816] #progress_metric: host=algo-2, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:14 INFO 140196492334912] processed a total of 1204 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10700.371980667114, \"sum\": 10700.371980667114, \"min\": 10700.371980667114}}, \"EndTime\": 1538410214.024764, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410203.324136}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:14 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=112.518063891 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:14 INFO 140196492334912] #progress_metric: host=algo-4, completed 89 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:15 INFO 139663579178816] Epoch[227] Batch[0] avg_epoch_loss=1.823029\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:15 INFO 140252856112960] Epoch[224] Batch[5] avg_epoch_loss=1.854942\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:15 INFO 140252856112960] Epoch[224] Batch [5]#011Speed: 149.20 samples/sec#011loss=1.854942\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:10:14 INFO 140624803325760] Epoch[229] Batch[5] avg_epoch_loss=1.878899\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:14 INFO 140624803325760] Epoch[229] Batch [5]#011Speed: 125.18 samples/sec#011loss=1.878899\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:15 INFO 140196492334912] Epoch[223] Batch[0] avg_epoch_loss=1.668557\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:18 INFO 140624803325760] Epoch[229] Batch[10] avg_epoch_loss=1.950682\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:18 INFO 140624803325760] Epoch[229] Batch [10]#011Speed: 153.58 samples/sec#011loss=2.036822\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:18 INFO 140624803325760] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10893.119096755981, \"sum\": 10893.119096755981, \"min\": 10893.119096755981}}, \"EndTime\": 1538410218.457675, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410207.564302}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:18 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.359592182 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:18 INFO 140624803325760] #progress_metric: host=algo-3, completed 92 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:20 INFO 139663579178816] Epoch[227] Batch[5] avg_epoch_loss=1.995843\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:20 INFO 139663579178816] Epoch[227] Batch [5]#011Speed: 126.37 samples/sec#011loss=1.995843\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:20 INFO 140196492334912] Epoch[223] Batch[5] avg_epoch_loss=1.948902\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:20 INFO 140196492334912] Epoch[223] Batch [5]#011Speed: 121.01 samples/sec#011loss=1.948902\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:20 INFO 140624803325760] Epoch[230] Batch[0] avg_epoch_loss=1.950799\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:21 INFO 140252856112960] Epoch[224] Batch[10] avg_epoch_loss=1.988693\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:21 INFO 140252856112960] Epoch[224] Batch [10]#011Speed: 114.39 samples/sec#011loss=2.149194\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:21 INFO 140252856112960] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12238.177061080933, \"sum\": 12238.177061080933, \"min\": 12238.177061080933}}, \"EndTime\": 1538410221.592809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410209.354411}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:21 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.959405429 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:21 INFO 140252856112960] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:23 INFO 140252856112960] Epoch[225] Batch[0] avg_epoch_loss=1.716771\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:24 INFO 139663579178816] Epoch[227] Batch[10] avg_epoch_loss=2.209748\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:24 INFO 139663579178816] Epoch[227] Batch [10]#011Speed: 156.76 samples/sec#011loss=2.466434\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:24 INFO 139663579178816] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10772.002935409546, \"sum\": 10772.002935409546, \"min\": 10772.002935409546}}, \"EndTime\": 1538410224.498676, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410213.726432}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:24 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.559855527 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:24 INFO 139663579178816] #progress_metric: host=algo-2, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:25 INFO 140196492334912] Epoch[223] Batch[10] avg_epoch_loss=1.994909\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:25 INFO 140196492334912] Epoch[223] Batch [10]#011Speed: 146.74 samples/sec#011loss=2.050117\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:25 INFO 140196492334912] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11268.776893615723, \"sum\": 11268.776893615723, \"min\": 11268.776893615723}}, \"EndTime\": 1538410225.294152, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410214.024852}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:25 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.119318936 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:25 INFO 140196492334912] #progress_metric: host=algo-4, completed 89 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:24 INFO 140624803325760] Epoch[230] Batch[5] avg_epoch_loss=2.012651\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:24 INFO 140624803325760] Epoch[230] Batch [5]#011Speed: 155.56 samples/sec#011loss=2.012651\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:26 INFO 139663579178816] Epoch[228] Batch[0] avg_epoch_loss=2.035262\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:27 INFO 140196492334912] Epoch[224] Batch[0] avg_epoch_loss=1.859017\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:28 INFO 140624803325760] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10120.070934295654, \"sum\": 10120.070934295654, \"min\": 10120.070934295654}}, \"EndTime\": 1538410228.578087, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410218.457762}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:28 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=122.922412061 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:28 INFO 140624803325760] #progress_metric: host=algo-3, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:28 INFO 140252856112960] Epoch[225] Batch[5] avg_epoch_loss=2.014672\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:28 INFO 140252856112960] Epoch[225] Batch [5]#011Speed: 118.26 samples/sec#011loss=2.014672\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:30 INFO 139663579178816] Epoch[228] Batch[5] avg_epoch_loss=2.024669\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:30 INFO 139663579178816] Epoch[228] Batch [5]#011Speed: 156.35 samples/sec#011loss=2.024669\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:30 INFO 140624803325760] Epoch[231] Batch[0] avg_epoch_loss=2.177011\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:31 INFO 140196492334912] Epoch[224] Batch[5] avg_epoch_loss=2.042445\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:31 INFO 140196492334912] Epoch[224] Batch [5]#011Speed: 152.19 samples/sec#011loss=2.042445\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:32 INFO 140252856112960] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11051.50818824768, \"sum\": 11051.50818824768, \"min\": 11051.50818824768}}, \"EndTime\": 1538410232.644625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410221.592886}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:32 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.553341433 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:32 INFO 140252856112960] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:34 INFO 140252856112960] Epoch[226] Batch[0] avg_epoch_loss=2.233144\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:35 INFO 140624803325760] Epoch[231] Batch[5] avg_epoch_loss=2.135810\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:35 INFO 140624803325760] Epoch[231] Batch [5]#011Speed: 156.46 samples/sec#011loss=2.135810\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:35 INFO 139663579178816] Epoch[228] Batch[10] avg_epoch_loss=1.988846\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:35 INFO 139663579178816] Epoch[228] Batch [10]#011Speed: 125.33 samples/sec#011loss=1.945859\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:35 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11491.80293083191, \"sum\": 11491.80293083191, \"min\": 11491.80293083191}}, \"EndTime\": 1538410235.990771, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410224.498746}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:35 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=113.819218849 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:35 INFO 139663579178816] #progress_metric: host=algo-2, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:35 INFO 140196492334912] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10381.845951080322, \"sum\": 10381.845951080322, \"min\": 10381.845951080322}}, \"EndTime\": 1538410235.676333, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410225.294238}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:35 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.978886681 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:35 INFO 140196492334912] #progress_metric: host=algo-4, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:37 INFO 139663579178816] Epoch[229] Batch[0] avg_epoch_loss=1.717114\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:38 INFO 140196492334912] Epoch[225] Batch[0] avg_epoch_loss=1.694811\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:40 INFO 140624803325760] Epoch[231] Batch[10] avg_epoch_loss=2.110909\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:40 INFO 140624803325760] Epoch[231] Batch [10]#011Speed: 123.97 samples/sec#011loss=2.081028\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:40 INFO 140624803325760] processed a total of 1364 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11621.822834014893, \"sum\": 11621.822834014893, \"min\": 11621.822834014893}}, \"EndTime\": 1538410240.200258, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410228.578177}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:40 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.363791874 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:40 INFO 140624803325760] #progress_metric: host=algo-3, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:39 INFO 140252856112960] Epoch[226] Batch[5] avg_epoch_loss=1.995583\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:39 INFO 140252856112960] Epoch[226] Batch [5]#011Speed: 117.57 samples/sec#011loss=1.995583\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:42 INFO 140196492334912] Epoch[225] Batch[5] avg_epoch_loss=1.777539\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:42 INFO 140196492334912] Epoch[225] Batch [5]#011Speed: 152.52 samples/sec#011loss=1.777539\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:41 INFO 140624803325760] Epoch[232] Batch[0] avg_epoch_loss=1.718241\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:42 INFO 139663579178816] Epoch[229] Batch[5] avg_epoch_loss=1.909971\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:42 INFO 139663579178816] Epoch[229] Batch [5]#011Speed: 128.16 samples/sec#011loss=1.909971\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:44 INFO 140252856112960] Epoch[226] Batch[10] avg_epoch_loss=1.963631\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:44 INFO 140252856112960] Epoch[226] Batch [10]#011Speed: 146.22 samples/sec#011loss=1.925289\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:44 INFO 140252856112960] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11511.47985458374, \"sum\": 11511.47985458374, \"min\": 11511.47985458374}}, \"EndTime\": 1538410244.156429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410232.644701}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:44 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.40647758 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:44 INFO 140252856112960] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:46 INFO 139663579178816] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10165.110111236572, \"sum\": 10165.110111236572, \"min\": 10165.110111236572}}, \"EndTime\": 1538410246.156216, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410235.990842}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:46 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=124.148373351 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:46 INFO 139663579178816] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:10:46 INFO 140252856112960] Epoch[227] Batch[0] avg_epoch_loss=1.775758\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:46 INFO 140624803325760] Epoch[232] Batch[5] avg_epoch_loss=1.826721\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:46 INFO 140624803325760] Epoch[232] Batch [5]#011Speed: 126.71 samples/sec#011loss=1.826721\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:47 INFO 139663579178816] Epoch[230] Batch[0] avg_epoch_loss=1.752107\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:47 INFO 140196492334912] Epoch[225] Batch[10] avg_epoch_loss=1.952828\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:47 INFO 140196492334912] Epoch[225] Batch [10]#011Speed: 120.01 samples/sec#011loss=2.163175\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:47 INFO 140196492334912] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11885.192155838013, \"sum\": 11885.192155838013, \"min\": 11885.192155838013}}, \"EndTime\": 1538410247.561868, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410235.676419}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:47 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=109.546949864 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:47 INFO 140196492334912] #progress_metric: host=algo-4, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:49 INFO 140196492334912] Epoch[226] Batch[0] avg_epoch_loss=2.042768\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:51 INFO 140624803325760] Epoch[232] Batch[10] avg_epoch_loss=1.830515\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:51 INFO 140624803325760] Epoch[232] Batch [10]#011Speed: 154.99 samples/sec#011loss=1.835068\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:51 INFO 140624803325760] processed a total of 1360 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10822.999000549316, \"sum\": 10822.999000549316, \"min\": 10822.999000549316}}, \"EndTime\": 1538410251.023619, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410240.200375}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=125.657160947 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 93 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:50 INFO 140252856112960] Epoch[227] Batch[5] avg_epoch_loss=1.901652\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:50 INFO 140252856112960] Epoch[227] Batch [5]#011Speed: 150.90 samples/sec#011loss=1.901652\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:52 INFO 139663579178816] Epoch[230] Batch[5] avg_epoch_loss=1.842362\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:52 INFO 139663579178816] Epoch[230] Batch [5]#011Speed: 127.91 samples/sec#011loss=1.842362\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:53 INFO 140624803325760] Epoch[233] Batch[0] avg_epoch_loss=2.027737\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:54 INFO 140252856112960] processed a total of 1191 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10554.887056350708, \"sum\": 10554.887056350708, \"min\": 10554.887056350708}}, \"EndTime\": 1538410254.711621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410244.156498}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:54 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.837206098 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:54 INFO 140252856112960] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:56 INFO 139663579178816] Epoch[230] Batch[10] avg_epoch_loss=1.706498\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:56 INFO 139663579178816] Epoch[230] Batch [10]#011Speed: 155.88 samples/sec#011loss=1.543461\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:56 INFO 139663579178816] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10762.212991714478, \"sum\": 10762.212991714478, \"min\": 10762.212991714478}}, \"EndTime\": 1538410256.918744, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410246.156296}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:56 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.76514663 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:56 INFO 139663579178816] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:57 INFO 140624803325760] Epoch[233] Batch[5] avg_epoch_loss=2.059178\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:10:57 INFO 140624803325760] Epoch[233] Batch [5]#011Speed: 157.70 samples/sec#011loss=2.059178\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:54 INFO 140196492334912] Epoch[226] Batch[5] avg_epoch_loss=1.872929\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:54 INFO 140196492334912] Epoch[226] Batch [5]#011Speed: 121.00 samples/sec#011loss=1.872929\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:10:57 INFO 140252856112960] Epoch[228] Batch[0] avg_epoch_loss=2.125975\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:58 INFO 140196492334912] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10797.49083518982, \"sum\": 10797.49083518982, \"min\": 10797.49083518982}}, \"EndTime\": 1538410258.359685, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410247.56195}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:58 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.284433456 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:58 INFO 140196492334912] #progress_metric: host=algo-4, completed 90 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:10:59 INFO 139663579178816] Epoch[231] Batch[0] avg_epoch_loss=1.819013\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:10:59 INFO 140196492334912] Epoch[227] Batch[0] avg_epoch_loss=1.855485\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:01 INFO 140252856112960] Epoch[228] Batch[5] avg_epoch_loss=1.942610\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:01 INFO 140252856112960] Epoch[228] Batch [5]#011Speed: 150.50 samples/sec#011loss=1.942610\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:02 INFO 140624803325760] Epoch[233] Batch[10] avg_epoch_loss=2.095062\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:02 INFO 140624803325760] Epoch[233] Batch [10]#011Speed: 120.63 samples/sec#011loss=2.138124\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:02 INFO 140624803325760] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11676.67007446289, \"sum\": 11676.67007446289, \"min\": 11676.67007446289}}, \"EndTime\": 1538410262.700579, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410251.023686}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:02 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=112.530917555 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:02 INFO 140624803325760] #progress_metric: host=algo-3, completed 93 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:03 INFO 139663579178816] Epoch[231] Batch[5] avg_epoch_loss=2.008656\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:03 INFO 139663579178816] Epoch[231] Batch [5]#011Speed: 158.89 samples/sec#011loss=2.008656\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:04 INFO 140624803325760] Epoch[234] Batch[0] avg_epoch_loss=1.831906\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:05 INFO 140196492334912] Epoch[227] Batch[5] avg_epoch_loss=2.039681\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:05 INFO 140196492334912] Epoch[227] Batch [5]#011Speed: 121.30 samples/sec#011loss=2.039681\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:05 INFO 140252856112960] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10524.793863296509, \"sum\": 10524.793863296509, \"min\": 10524.793863296509}}, \"EndTime\": 1538410265.236769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410254.711716}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:05 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.67064449 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:05 INFO 140252856112960] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:06 INFO 139663579178816] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9990.141868591309, \"sum\": 9990.141868591309, \"min\": 9990.141868591309}}, \"EndTime\": 1538410266.909193, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410256.918815}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.52235897 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:07 INFO 140252856112960] Epoch[229] Batch[0] avg_epoch_loss=2.494513\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:09 INFO 140196492334912] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10689.828872680664, \"sum\": 10689.828872680664, \"min\": 10689.828872680664}}, \"EndTime\": 1538410269.04986, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410258.359771}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:09 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=114.967817484 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:09 INFO 140196492334912] #progress_metric: host=algo-4, completed 91 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:09 INFO 140624803325760] Epoch[234] Batch[5] avg_epoch_loss=1.947253\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:09 INFO 140624803325760] Epoch[234] Batch [5]#011Speed: 126.04 samples/sec#011loss=1.947253\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:09 INFO 139663579178816] Epoch[232] Batch[0] avg_epoch_loss=2.050527\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:10 INFO 140196492334912] Epoch[228] Batch[0] avg_epoch_loss=1.971653\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:11 INFO 140252856112960] Epoch[229] Batch[5] avg_epoch_loss=2.066898\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:11 INFO 140252856112960] Epoch[229] Batch [5]#011Speed: 149.96 samples/sec#011loss=2.066898\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:12 INFO 140624803325760] processed a total of 1214 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10293.100833892822, \"sum\": 10293.100833892822, \"min\": 10293.100833892822}}, \"EndTime\": 1538410272.993988, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410262.700665}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:12 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=117.941840991 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:12 INFO 140624803325760] #progress_metric: host=algo-3, completed 94 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:13 INFO 139663579178816] Epoch[232] Batch[5] avg_epoch_loss=2.028721\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:13 INFO 139663579178816] Epoch[232] Batch [5]#011Speed: 157.78 samples/sec#011loss=2.028721\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:17 INFO 139663579178816] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10123.061895370483, \"sum\": 10123.061895370483, \"min\": 10123.061895370483}}, \"EndTime\": 1538410277.032566, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410266.909268}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:17 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=123.182733089 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:17 INFO 139663579178816] #progress_metric: host=algo-2, completed 93 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:11:14 INFO 140624803325760] Epoch[235] Batch[0] avg_epoch_loss=2.160388\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:15 INFO 140196492334912] Epoch[228] Batch[5] avg_epoch_loss=2.097554\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:15 INFO 140196492334912] Epoch[228] Batch [5]#011Speed: 122.54 samples/sec#011loss=2.097554\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:17 INFO 140252856112960] Epoch[229] Batch[10] avg_epoch_loss=2.046472\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:17 INFO 140252856112960] Epoch[229] Batch [10]#011Speed: 116.39 samples/sec#011loss=2.021962\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:17 INFO 140252856112960] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12196.319818496704, \"sum\": 12196.319818496704, \"min\": 12196.319818496704}}, \"EndTime\": 1538410277.433434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410265.236858}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:17 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.342531609 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:17 INFO 140252856112960] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:19 INFO 140252856112960] Epoch[230] Batch[0] avg_epoch_loss=1.649881\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:19 INFO 139663579178816] Epoch[233] Batch[0] avg_epoch_loss=1.824241\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:19 INFO 140196492334912] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10637.12191581726, \"sum\": 10637.12191581726, \"min\": 10637.12191581726}}, \"EndTime\": 1538410279.687333, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410269.04994}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:19 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=120.143823999 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:19 INFO 140196492334912] #progress_metric: host=algo-4, completed 91 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:19 INFO 140624803325760] Epoch[235] Batch[5] avg_epoch_loss=2.025477\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:19 INFO 140624803325760] Epoch[235] Batch [5]#011Speed: 122.81 samples/sec#011loss=2.025477\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:21 INFO 140196492334912] Epoch[229] Batch[0] avg_epoch_loss=1.866670\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:23 INFO 139663579178816] Epoch[233] Batch[5] avg_epoch_loss=1.823787\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:23 INFO 139663579178816] Epoch[233] Batch [5]#011Speed: 160.85 samples/sec#011loss=1.823787\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:24 INFO 140624803325760] Epoch[235] Batch[10] avg_epoch_loss=2.050813\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:24 INFO 140624803325760] Epoch[235] Batch [10]#011Speed: 150.09 samples/sec#011loss=2.081216\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:24 INFO 140624803325760] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11095.757007598877, \"sum\": 11095.757007598877, \"min\": 11095.757007598877}}, \"EndTime\": 1538410284.090057, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410272.994061}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:24 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.800330112 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:24 INFO 140624803325760] #progress_metric: host=algo-3, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:24 INFO 140252856112960] Epoch[230] Batch[5] avg_epoch_loss=1.765450\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:24 INFO 140252856112960] Epoch[230] Batch [5]#011Speed: 119.05 samples/sec#011loss=1.765450\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:28 INFO 140252856112960] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10817.432165145874, \"sum\": 10817.432165145874, \"min\": 10817.432165145874}}, \"EndTime\": 1538410288.251186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410277.433515}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:28 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.325879104 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:28 INFO 140252856112960] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:26 INFO 140624803325760] Epoch[236] Batch[0] avg_epoch_loss=1.874212\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:26 INFO 140196492334912] Epoch[229] Batch[5] avg_epoch_loss=1.974925\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:26 INFO 140196492334912] Epoch[229] Batch [5]#011Speed: 120.87 samples/sec#011loss=1.974925\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:28 INFO 139663579178816] Epoch[233] Batch[10] avg_epoch_loss=1.645601\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:28 INFO 139663579178816] Epoch[233] Batch [10]#011Speed: 125.64 samples/sec#011loss=1.431779\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:28 INFO 139663579178816] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11372.283220291138, \"sum\": 11372.283220291138, \"min\": 11372.283220291138}}, \"EndTime\": 1538410288.405152, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410277.03264}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:28 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.92738683 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:28 INFO 139663579178816] #progress_metric: host=algo-2, completed 93 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:30 INFO 139663579178816] Epoch[234] Batch[0] avg_epoch_loss=1.949962\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:29 INFO 140252856112960] Epoch[231] Batch[0] avg_epoch_loss=1.641979\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:30 INFO 140196492334912] Epoch[229] Batch[10] avg_epoch_loss=2.008889\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:30 INFO 140196492334912] Epoch[229] Batch [10]#011Speed: 147.61 samples/sec#011loss=2.049646\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:30 INFO 140196492334912] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11266.695022583008, \"sum\": 11266.695022583008, \"min\": 11266.695022583008}}, \"EndTime\": 1538410290.95438, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410279.687418}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:30 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.093104892 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:30 INFO 140196492334912] #progress_metric: host=algo-4, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:30 INFO 140624803325760] Epoch[236] Batch[5] avg_epoch_loss=1.912766\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:30 INFO 140624803325760] Epoch[236] Batch [5]#011Speed: 153.29 samples/sec#011loss=1.912766\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:33 INFO 140196492334912] Epoch[230] Batch[0] avg_epoch_loss=2.199647\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:34 INFO 140624803325760] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10380.021810531616, \"sum\": 10380.021810531616, \"min\": 10380.021810531616}}, \"EndTime\": 1538410294.470371, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410284.090126}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:34 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.385726132 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:34 INFO 140624803325760] #progress_metric: host=algo-3, completed 94 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:35 INFO 139663579178816] Epoch[234] Batch[5] avg_epoch_loss=2.051670\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:35 INFO 139663579178816] Epoch[234] Batch [5]#011Speed: 127.32 samples/sec#011loss=2.051670\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:35 INFO 140252856112960] Epoch[231] Batch[5] avg_epoch_loss=1.929754\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:35 INFO 140252856112960] Epoch[231] Batch [5]#011Speed: 119.21 samples/sec#011loss=1.929754\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:36 INFO 140624803325760] Epoch[237] Batch[0] avg_epoch_loss=2.175208\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:37 INFO 140196492334912] Epoch[230] Batch[5] avg_epoch_loss=2.225138\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:37 INFO 140196492334912] Epoch[230] Batch [5]#011Speed: 152.24 samples/sec#011loss=2.225138\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:39 INFO 139663579178816] Epoch[234] Batch[10] avg_epoch_loss=2.142748\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:39 INFO 139663579178816] Epoch[234] Batch [10]#011Speed: 153.58 samples/sec#011loss=2.252041\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:39 INFO 139663579178816] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10886.888980865479, \"sum\": 10886.888980865479, \"min\": 10886.888980865479}}, \"EndTime\": 1538410299.292354, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410288.405235}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:39 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=118.398166337 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:39 INFO 139663579178816] #progress_metric: host=algo-2, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:39 INFO 140252856112960] Epoch[231] Batch[10] avg_epoch_loss=1.876688\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:39 INFO 140252856112960] Epoch[231] Batch [10]#011Speed: 142.54 samples/sec#011loss=1.813009\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:39 INFO 140252856112960] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11441.772937774658, \"sum\": 11441.772937774658, \"min\": 11441.772937774658}}, \"EndTime\": 1538410299.693319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410288.251287}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:39 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=112.481318516 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:39 INFO 140252856112960] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:41 INFO 140196492334912] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10435.116052627563, \"sum\": 10435.116052627563, \"min\": 10435.116052627563}}, \"EndTime\": 1538410301.389827, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410290.954462}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:41 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.798735632 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:41 INFO 140196492334912] #progress_metric: host=algo-4, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:41 INFO 140624803325760] Epoch[237] Batch[5] avg_epoch_loss=1.904301\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:41 INFO 140624803325760] Epoch[237] Batch [5]#011Speed: 151.24 samples/sec#011loss=1.904301\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:41 INFO 139663579178816] Epoch[235] Batch[0] avg_epoch_loss=2.043220\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:41 INFO 140252856112960] Epoch[232] Batch[0] avg_epoch_loss=2.013515\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:43 INFO 140196492334912] Epoch[231] Batch[0] avg_epoch_loss=1.930458\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:45 INFO 139663579178816] Epoch[235] Batch[5] avg_epoch_loss=2.012992\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:45 INFO 139663579178816] Epoch[235] Batch [5]#011Speed: 155.73 samples/sec#011loss=2.012992\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:47 INFO 140196492334912] Epoch[231] Batch[5] avg_epoch_loss=1.878021\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:47 INFO 140196492334912] Epoch[231] Batch [5]#011Speed: 151.83 samples/sec#011loss=1.878021\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:11:46 INFO 140624803325760] Epoch[237] Batch[10] avg_epoch_loss=1.886845\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:46 INFO 140624803325760] Epoch[237] Batch [10]#011Speed: 118.14 samples/sec#011loss=1.865898\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:46 INFO 140624803325760] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11980.077028274536, \"sum\": 11980.077028274536, \"min\": 11980.077028274536}}, \"EndTime\": 1538410306.45075, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410294.470446}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:46 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=108.929833306 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:46 INFO 140624803325760] #progress_metric: host=algo-3, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:46 INFO 140252856112960] Epoch[232] Batch[5] avg_epoch_loss=1.915730\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:46 INFO 140252856112960] Epoch[232] Batch [5]#011Speed: 151.35 samples/sec#011loss=1.915730\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:48 INFO 140624803325760] Epoch[238] Batch[0] avg_epoch_loss=1.655008\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:50 INFO 140252856112960] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10421.41580581665, \"sum\": 10421.41580581665, \"min\": 10421.41580581665}}, \"EndTime\": 1538410310.115072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410299.693403}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:50 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.080174808 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:50 INFO 140252856112960] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:50 INFO 139663579178816] Epoch[235] Batch[10] avg_epoch_loss=2.218002\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:50 INFO 139663579178816] Epoch[235] Batch [10]#011Speed: 125.02 samples/sec#011loss=2.464013\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:50 INFO 139663579178816] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11502.274990081787, \"sum\": 11502.274990081787, \"min\": 11502.274990081787}}, \"EndTime\": 1538410310.794924, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410299.292424}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:50 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=111.542122485 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:50 INFO 139663579178816] #progress_metric: host=algo-2, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:52 INFO 140252856112960] Epoch[233] Batch[0] avg_epoch_loss=2.004955\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:52 INFO 139663579178816] Epoch[236] Batch[0] avg_epoch_loss=1.722270\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:53 INFO 140196492334912] Epoch[231] Batch[10] avg_epoch_loss=1.909981\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:53 INFO 140196492334912] Epoch[231] Batch [10]#011Speed: 120.19 samples/sec#011loss=1.948333\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:53 INFO 140196492334912] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11930.586814880371, \"sum\": 11930.586814880371, \"min\": 11930.586814880371}}, \"EndTime\": 1538410313.320754, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410301.389914}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:53 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.471026372 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:53 INFO 140196492334912] #progress_metric: host=algo-4, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:53 INFO 140624803325760] Epoch[238] Batch[5] avg_epoch_loss=1.852776\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:53 INFO 140624803325760] Epoch[238] Batch [5]#011Speed: 119.78 samples/sec#011loss=1.852776\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:11:54 INFO 140196492334912] Epoch[232] Batch[0] avg_epoch_loss=1.648728\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:57 INFO 140624803325760] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10790.72117805481, \"sum\": 10790.72117805481, \"min\": 10790.72117805481}}, \"EndTime\": 1538410317.24177, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410306.450829}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:57 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=113.800379987 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:57 INFO 140624803325760] #progress_metric: host=algo-3, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:56 INFO 140252856112960] Epoch[233] Batch[5] avg_epoch_loss=1.933703\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:11:56 INFO 140252856112960] Epoch[233] Batch [5]#011Speed: 151.77 samples/sec#011loss=1.933703\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:57 INFO 139663579178816] Epoch[236] Batch[5] avg_epoch_loss=1.929369\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:11:57 INFO 139663579178816] Epoch[236] Batch [5]#011Speed: 127.11 samples/sec#011loss=1.929369\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:11:58 INFO 140624803325760] Epoch[239] Batch[0] avg_epoch_loss=2.050586\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:00 INFO 140196492334912] Epoch[232] Batch[5] avg_epoch_loss=1.966989\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:00 INFO 140196492334912] Epoch[232] Batch [5]#011Speed: 121.12 samples/sec#011loss=1.966989\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:01 INFO 139663579178816] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10249.94707107544, \"sum\": 10249.94707107544, \"min\": 10249.94707107544}}, \"EndTime\": 1538410321.045255, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410310.794997}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:01 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=122.340760529 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:01 INFO 139663579178816] #progress_metric: host=algo-2, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:02 INFO 140252856112960] Epoch[233] Batch[10] avg_epoch_loss=1.797700\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:02 INFO 140252856112960] Epoch[233] Batch [10]#011Speed: 120.34 samples/sec#011loss=1.634497\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:02 INFO 140252856112960] processed a total of 1355 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11927.887916564941, \"sum\": 11927.887916564941, \"min\": 11927.887916564941}}, \"EndTime\": 1538410322.043308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410310.115163}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.598104327 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:02 INFO 139663579178816] Epoch[237] Batch[0] avg_epoch_loss=2.764428\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:04 INFO 140196492334912] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10727.005958557129, \"sum\": 10727.005958557129, \"min\": 10727.005958557129}}, \"EndTime\": 1538410324.048109, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410313.320855}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:04 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.459146241 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:04 INFO 140196492334912] #progress_metric: host=algo-4, completed 93 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:04 INFO 140624803325760] Epoch[239] Batch[5] avg_epoch_loss=2.144755\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:04 INFO 140624803325760] Epoch[239] Batch [5]#011Speed: 118.52 samples/sec#011loss=2.144755\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:03 INFO 140252856112960] Epoch[234] Batch[0] avg_epoch_loss=2.072608\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:05 INFO 140196492334912] Epoch[233] Batch[0] avg_epoch_loss=2.167968\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:08 INFO 140624803325760] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10958.507061004639, \"sum\": 10958.507061004639, \"min\": 10958.507061004639}}, \"EndTime\": 1538410328.20058, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410317.24184}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:08 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=115.069274979 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:08 INFO 140624803325760] #progress_metric: host=algo-3, completed 96 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/01/2018 16:12:07 INFO 139663579178816] Epoch[237] Batch[5] avg_epoch_loss=2.128160\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:07 INFO 139663579178816] Epoch[237] Batch [5]#011Speed: 126.91 samples/sec#011loss=2.128160\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:08 INFO 140252856112960] Epoch[234] Batch[5] avg_epoch_loss=2.099636\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:08 INFO 140252856112960] Epoch[234] Batch [5]#011Speed: 122.45 samples/sec#011loss=2.099636\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:09 INFO 140624803325760] Epoch[240] Batch[0] avg_epoch_loss=2.193419\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:10 INFO 140196492334912] Epoch[233] Batch[5] avg_epoch_loss=2.020563\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:10 INFO 140196492334912] Epoch[233] Batch [5]#011Speed: 121.94 samples/sec#011loss=2.020563\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:11 INFO 139663579178816] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10283.11800956726, \"sum\": 10283.11800956726, \"min\": 10283.11800956726}}, \"EndTime\": 1538410331.328688, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410321.045331}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:11 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.320485109 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:11 INFO 139663579178816] #progress_metric: host=algo-2, completed 95 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:13 INFO 139663579178816] Epoch[238] Batch[0] avg_epoch_loss=2.177442\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:13 INFO 140252856112960] Epoch[234] Batch[10] avg_epoch_loss=1.787013\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:13 INFO 140252856112960] Epoch[234] Batch [10]#011Speed: 150.79 samples/sec#011loss=1.411866\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:13 INFO 140252856112960] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11124.407052993774, \"sum\": 11124.407052993774, \"min\": 11124.407052993774}}, \"EndTime\": 1538410333.168046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410322.043394}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:13 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.207197229 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:13 INFO 140252856112960] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:15 INFO 140196492334912] Epoch[233] Batch[10] avg_epoch_loss=2.098411\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:15 INFO 140196492334912] Epoch[233] Batch [10]#011Speed: 149.11 samples/sec#011loss=2.191830\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:15 INFO 140196492334912] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11146.521091461182, \"sum\": 11146.521091461182, \"min\": 11146.521091461182}}, \"EndTime\": 1538410335.195004, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410324.048195}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:15 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.460727803 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:15 INFO 140196492334912] #progress_metric: host=algo-4, completed 93 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:15 INFO 140624803325760] Epoch[240] Batch[5] avg_epoch_loss=1.947574\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:15 INFO 140624803325760] Epoch[240] Batch [5]#011Speed: 122.83 samples/sec#011loss=1.947574\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:15 INFO 140252856112960] Epoch[235] Batch[0] avg_epoch_loss=1.858185\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:19 INFO 140624803325760] Epoch[240] Batch[10] avg_epoch_loss=1.966792\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:19 INFO 140624803325760] Epoch[240] Batch [10]#011Speed: 150.96 samples/sec#011loss=1.989854\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:19 INFO 140624803325760] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11101.330995559692, \"sum\": 11101.330995559692, \"min\": 11101.330995559692}}, \"EndTime\": 1538410339.302229, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410328.200652}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:19 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.254497139 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:19 INFO 140624803325760] #progress_metric: host=algo-3, completed 96 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:18 INFO 139663579178816] Epoch[238] Batch[5] avg_epoch_loss=1.965948\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:18 INFO 139663579178816] Epoch[238] Batch [5]#011Speed: 127.58 samples/sec#011loss=1.965948\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:17 INFO 140196492334912] Epoch[234] Batch[0] avg_epoch_loss=1.828913\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:19 INFO 140252856112960] Epoch[235] Batch[5] avg_epoch_loss=1.910495\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:19 INFO 140252856112960] Epoch[235] Batch [5]#011Speed: 150.88 samples/sec#011loss=1.910495\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:21 INFO 140624803325760] Epoch[241] Batch[0] avg_epoch_loss=2.086276\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:21 INFO 140196492334912] Epoch[234] Batch[5] avg_epoch_loss=1.815375\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:21 INFO 140196492334912] Epoch[234] Batch [5]#011Speed: 153.12 samples/sec#011loss=1.815375\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:22 INFO 139663579178816] Epoch[238] Batch[10] avg_epoch_loss=1.746501\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:22 INFO 139663579178816] Epoch[238] Batch [10]#011Speed: 147.39 samples/sec#011loss=1.483165\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:22 INFO 139663579178816] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11062.087059020996, \"sum\": 11062.087059020996, \"min\": 11062.087059020996}}, \"EndTime\": 1538410342.39108, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410331.328759}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:22 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=117.788600728 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:22 INFO 139663579178816] #progress_metric: host=algo-2, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:23 INFO 140252856112960] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10437.604904174805, \"sum\": 10437.604904174805, \"min\": 10437.604904174805}}, \"EndTime\": 1538410343.605992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410333.168132}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:23 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.224801152 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:23 INFO 140252856112960] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:24 INFO 139663579178816] Epoch[239] Batch[0] avg_epoch_loss=1.885692\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:25 INFO 140624803325760] Epoch[241] Batch[5] avg_epoch_loss=1.847738\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:25 INFO 140624803325760] Epoch[241] Batch [5]#011Speed: 158.05 samples/sec#011loss=1.847738\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:26 INFO 140252856112960] Epoch[236] Batch[0] avg_epoch_loss=1.774663\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:27 INFO 140196492334912] Epoch[234] Batch[10] avg_epoch_loss=1.890244\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:27 INFO 140196492334912] Epoch[234] Batch [10]#011Speed: 120.50 samples/sec#011loss=1.980087\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:27 INFO 140196492334912] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11912.62698173523, \"sum\": 11912.62698173523, \"min\": 11912.62698173523}}, \"EndTime\": 1538410347.107964, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410335.195087}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:27 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.637762009 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:27 INFO 140196492334912] #progress_metric: host=algo-4, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:28 INFO 140196492334912] Epoch[235] Batch[0] avg_epoch_loss=1.540628\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:30 INFO 140252856112960] Epoch[236] Batch[5] avg_epoch_loss=1.961477\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:30 INFO 140252856112960] Epoch[236] Batch [5]#011Speed: 152.40 samples/sec#011loss=1.961477\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:28 INFO 139663579178816] Epoch[239] Batch[5] avg_epoch_loss=1.887364\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:28 INFO 139663579178816] Epoch[239] Batch [5]#011Speed: 162.19 samples/sec#011loss=1.887364\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:29 INFO 140624803325760] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10044.928789138794, \"sum\": 10044.928789138794, \"min\": 10044.928789138794}}, \"EndTime\": 1538410349.347498, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410339.302314}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:29 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.457183567 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:29 INFO 140624803325760] #progress_metric: host=algo-3, completed 96 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:31 INFO 140624803325760] Epoch[242] Batch[0] avg_epoch_loss=1.910370\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:33 INFO 139663579178816] Epoch[239] Batch[10] avg_epoch_loss=2.044774\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:33 INFO 139663579178816] Epoch[239] Batch [10]#011Speed: 125.47 samples/sec#011loss=2.233665\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:33 INFO 139663579178816] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11346.695899963379, \"sum\": 11346.695899963379, \"min\": 11346.695899963379}}, \"EndTime\": 1538410353.738133, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410342.39115}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:33 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.186649973 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:33 INFO 139663579178816] #progress_metric: host=algo-2, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:33 INFO 140196492334912] Epoch[235] Batch[5] avg_epoch_loss=1.876077\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:33 INFO 140196492334912] Epoch[235] Batch [5]#011Speed: 121.87 samples/sec#011loss=1.876077\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:35 INFO 139663579178816] Epoch[240] Batch[0] avg_epoch_loss=1.894986\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:35 INFO 140624803325760] Epoch[242] Batch[5] avg_epoch_loss=1.932591\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:35 INFO 140624803325760] Epoch[242] Batch [5]#011Speed: 158.72 samples/sec#011loss=1.932591\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:35 INFO 140252856112960] Epoch[236] Batch[10] avg_epoch_loss=2.226546\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:35 INFO 140252856112960] Epoch[236] Batch [10]#011Speed: 118.23 samples/sec#011loss=2.544629\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:35 INFO 140252856112960] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12066.951036453247, \"sum\": 12066.951036453247, \"min\": 12066.951036453247}}, \"EndTime\": 1538410355.673292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410343.606082}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:35 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.311206832 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:35 INFO 140252856112960] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:37 INFO 140252856112960] Epoch[237] Batch[0] avg_epoch_loss=2.172319\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:37 INFO 140196492334912] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10653.97310256958, \"sum\": 10653.97310256958, \"min\": 10653.97310256958}}, \"EndTime\": 1538410357.762257, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410347.108045}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:37 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=116.199364252 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:37 INFO 140196492334912] #progress_metric: host=algo-4, completed 94 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:12:39 INFO 140196492334912] Epoch[236] Batch[0] avg_epoch_loss=1.796101\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:40 INFO 139663579178816] Epoch[240] Batch[5] avg_epoch_loss=1.912104\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:40 INFO 139663579178816] Epoch[240] Batch [5]#011Speed: 127.67 samples/sec#011loss=1.912104\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:41 INFO 140624803325760] Epoch[242] Batch[10] avg_epoch_loss=1.823283\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:41 INFO 140624803325760] Epoch[242] Batch [10]#011Speed: 117.55 samples/sec#011loss=1.692113\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:41 INFO 140624803325760] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11774.45101737976, \"sum\": 11774.45101737976, \"min\": 11774.45101737976}}, \"EndTime\": 1538410361.122298, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410349.347588}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:41 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=110.237506686 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:41 INFO 140624803325760] #progress_metric: host=algo-3, completed 97 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:42 INFO 140252856112960] Epoch[237] Batch[5] avg_epoch_loss=1.935306\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:42 INFO 140252856112960] Epoch[237] Batch [5]#011Speed: 120.29 samples/sec#011loss=1.935306\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:42 INFO 140624803325760] Epoch[243] Batch[0] avg_epoch_loss=2.086488\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:44 INFO 139663579178816] Epoch[240] Batch[10] avg_epoch_loss=2.018903\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:44 INFO 139663579178816] Epoch[240] Batch [10]#011Speed: 155.72 samples/sec#011loss=2.147062\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:44 INFO 139663579178816] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10760.447025299072, \"sum\": 10760.447025299072, \"min\": 10760.447025299072}}, \"EndTime\": 1538410364.498885, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410353.738205}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:44 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=120.161133283 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:44 INFO 139663579178816] #progress_metric: host=algo-2, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:44 INFO 140196492334912] Epoch[236] Batch[5] avg_epoch_loss=1.783443\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:44 INFO 140196492334912] Epoch[236] Batch [5]#011Speed: 120.64 samples/sec#011loss=1.783443\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:46 INFO 139663579178816] Epoch[241] Batch[0] avg_epoch_loss=1.948743\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:46 INFO 140252856112960] Epoch[237] Batch[10] avg_epoch_loss=1.648855\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:46 INFO 140252856112960] Epoch[237] Batch [10]#011Speed: 147.17 samples/sec#011loss=1.305114\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:46 INFO 140252856112960] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11320.17183303833, \"sum\": 11320.17183303833, \"min\": 11320.17183303833}}, \"EndTime\": 1538410366.993797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410355.673378}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:46 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=116.251347287 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:46 INFO 140252856112960] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:47 INFO 140624803325760] Epoch[243] Batch[5] avg_epoch_loss=2.065629\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:47 INFO 140624803325760] Epoch[243] Batch [5]#011Speed: 121.80 samples/sec#011loss=2.065629\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:48 INFO 140196492334912] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10774.166107177734, \"sum\": 10774.166107177734, \"min\": 10774.166107177734}}, \"EndTime\": 1538410368.53678, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410357.762344}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:48 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.88212999 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:48 INFO 140196492334912] #progress_metric: host=algo-4, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:49 INFO 140252856112960] Epoch[238] Batch[0] avg_epoch_loss=1.838744\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:50 INFO 139663579178816] Epoch[241] Batch[5] avg_epoch_loss=1.825785\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:50 INFO 139663579178816] Epoch[241] Batch [5]#011Speed: 161.33 samples/sec#011loss=1.825785\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:51 INFO 140624803325760] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10497.958183288574, \"sum\": 10497.958183288574, \"min\": 10497.958183288574}}, \"EndTime\": 1538410371.62059, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410361.122381}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:51 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=118.212271819 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:51 INFO 140624803325760] #progress_metric: host=algo-3, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:50 INFO 140196492334912] Epoch[237] Batch[0] avg_epoch_loss=1.999220\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:53 INFO 140624803325760] Epoch[244] Batch[0] avg_epoch_loss=1.554555\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:53 INFO 140252856112960] Epoch[238] Batch[5] avg_epoch_loss=1.786703\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:53 INFO 140252856112960] Epoch[238] Batch [5]#011Speed: 151.56 samples/sec#011loss=1.786703\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:55 INFO 140196492334912] Epoch[237] Batch[5] avg_epoch_loss=1.968119\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:55 INFO 140196492334912] Epoch[237] Batch [5]#011Speed: 123.25 samples/sec#011loss=1.968119\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:55 INFO 139663579178816] Epoch[241] Batch[10] avg_epoch_loss=1.835039\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:55 INFO 139663579178816] Epoch[241] Batch [10]#011Speed: 125.82 samples/sec#011loss=1.846143\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:55 INFO 139663579178816] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11382.187843322754, \"sum\": 11382.187843322754, \"min\": 11382.187843322754}}, \"EndTime\": 1538410375.881426, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410364.498955}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:55 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=114.299802583 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:55 INFO 139663579178816] #progress_metric: host=algo-2, completed 96 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:12:57 INFO 139663579178816] Epoch[242] Batch[0] avg_epoch_loss=2.054348\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:58 INFO 140624803325760] Epoch[244] Batch[5] avg_epoch_loss=1.849599\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:12:58 INFO 140624803325760] Epoch[244] Batch [5]#011Speed: 126.26 samples/sec#011loss=1.849599\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:59 INFO 140196492334912] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10578.985214233398, \"sum\": 10578.985214233398, \"min\": 10578.985214233398}}, \"EndTime\": 1538410379.116124, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410368.536869}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:59 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.440854191 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:12:59 INFO 140196492334912] #progress_metric: host=algo-4, completed 95 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:02 INFO 140624803325760] Epoch[244] Batch[10] avg_epoch_loss=1.754875\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:02 INFO 140624803325760] Epoch[244] Batch [10]#011Speed: 153.45 samples/sec#011loss=1.641206\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:02 INFO 140624803325760] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10862.663984298706, \"sum\": 10862.663984298706, \"min\": 10862.663984298706}}, \"EndTime\": 1538410382.483567, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410371.620661}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:02 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=120.595422283 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:02 INFO 140624803325760] #progress_metric: host=algo-3, completed 98 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[10/01/2018 16:12:58 INFO 140252856112960] Epoch[238] Batch[10] avg_epoch_loss=1.678356\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:58 INFO 140252856112960] Epoch[238] Batch [10]#011Speed: 120.92 samples/sec#011loss=1.548339\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:58 INFO 140252856112960] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11939.412832260132, \"sum\": 11939.412832260132, \"min\": 11939.412832260132}}, \"EndTime\": 1538410378.933546, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410366.993881}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:58 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=111.310810751 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:12:58 INFO 140252856112960] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:00 INFO 140252856112960] Epoch[239] Batch[0] avg_epoch_loss=2.223569\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:00 INFO 140196492334912] Epoch[238] Batch[0] avg_epoch_loss=1.683629\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:02 INFO 139663579178816] Epoch[242] Batch[5] avg_epoch_loss=1.936443\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:02 INFO 139663579178816] Epoch[242] Batch [5]#011Speed: 127.59 samples/sec#011loss=1.936443\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:04 INFO 140624803325760] Epoch[245] Batch[0] avg_epoch_loss=2.269834\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:06 INFO 139663579178816] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10180.393934249878, \"sum\": 10180.393934249878, \"min\": 10180.393934249878}}, \"EndTime\": 1538410386.062124, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410375.881497}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:06 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.632250629 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:06 INFO 139663579178816] #progress_metric: host=algo-2, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:05 INFO 140196492334912] Epoch[238] Batch[5] avg_epoch_loss=1.858624\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:05 INFO 140196492334912] Epoch[238] Batch [5]#011Speed: 121.53 samples/sec#011loss=1.858624\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:05 INFO 140252856112960] Epoch[239] Batch[5] avg_epoch_loss=1.981524\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:05 INFO 140252856112960] Epoch[239] Batch [5]#011Speed: 119.46 samples/sec#011loss=1.981524\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:07 INFO 139663579178816] Epoch[243] Batch[0] avg_epoch_loss=2.148149\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:09 INFO 140624803325760] Epoch[245] Batch[5] avg_epoch_loss=2.153800\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:09 INFO 140624803325760] Epoch[245] Batch [5]#011Speed: 152.03 samples/sec#011loss=2.153800\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:09 INFO 140196492334912] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10632.771968841553, \"sum\": 10632.771968841553, \"min\": 10632.771968841553}}, \"EndTime\": 1538410389.749262, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410379.116215}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:09 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.405988979 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:09 INFO 140196492334912] #progress_metric: host=algo-4, completed 95 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:09 INFO 140252856112960] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10834.435939788818, \"sum\": 10834.435939788818, \"min\": 10834.435939788818}}, \"EndTime\": 1538410389.768317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410378.933632}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:09 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=114.540741367 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:09 INFO 140252856112960] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:11 INFO 140196492334912] Epoch[239] Batch[0] avg_epoch_loss=1.495694\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:11 INFO 140252856112960] Epoch[240] Batch[0] avg_epoch_loss=1.914980\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:12 INFO 139663579178816] Epoch[243] Batch[5] avg_epoch_loss=2.072425\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:12 INFO 139663579178816] Epoch[243] Batch [5]#011Speed: 127.56 samples/sec#011loss=2.072425\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:12 INFO 140624803325760] processed a total of 1223 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10495.089054107666, \"sum\": 10495.089054107666, \"min\": 10495.089054107666}}, \"EndTime\": 1538410392.978962, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410382.483637}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:12 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=116.529215801 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:12 INFO 140624803325760] #progress_metric: host=algo-3, completed 98 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:15 INFO 140624803325760] Epoch[246] Batch[0] avg_epoch_loss=2.065429\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:16 INFO 140252856112960] Epoch[240] Batch[5] avg_epoch_loss=1.987387\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:16 INFO 140252856112960] Epoch[240] Batch [5]#011Speed: 123.40 samples/sec#011loss=1.987387\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:16 INFO 139663579178816] Epoch[243] Batch[10] avg_epoch_loss=2.137825\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:16 INFO 139663579178816] Epoch[243] Batch [10]#011Speed: 155.09 samples/sec#011loss=2.216305\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:16 INFO 139663579178816] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10734.309911727905, \"sum\": 10734.309911727905, \"min\": 10734.309911727905}}, \"EndTime\": 1538410396.796743, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410386.062202}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:16 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.428968175 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:16 INFO 139663579178816] #progress_metric: host=algo-2, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:16 INFO 140196492334912] Epoch[239] Batch[5] avg_epoch_loss=1.811789\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:16 INFO 140196492334912] Epoch[239] Batch [5]#011Speed: 120.98 samples/sec#011loss=1.811789\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:19 INFO 139663579178816] Epoch[244] Batch[0] avg_epoch_loss=2.326979\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:20 INFO 140196492334912] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10662.549018859863, \"sum\": 10662.549018859863, \"min\": 10662.549018859863}}, \"EndTime\": 1538410400.412228, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410389.749354}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:20 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.919468412 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:20 INFO 140196492334912] #progress_metric: host=algo-4, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:20 INFO 140252856112960] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10517.947912216187, \"sum\": 10517.947912216187, \"min\": 10517.947912216187}}, \"EndTime\": 1538410400.286633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410389.76841}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:20 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=120.743643957 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:20 INFO 140252856112960] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:23 INFO 139663579178816] Epoch[244] Batch[5] avg_epoch_loss=2.051439\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:23 INFO 139663579178816] Epoch[244] Batch [5]#011Speed: 159.62 samples/sec#011loss=2.051439\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:19 INFO 140624803325760] Epoch[246] Batch[5] avg_epoch_loss=2.030983\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:19 INFO 140624803325760] Epoch[246] Batch [5]#011Speed: 155.82 samples/sec#011loss=2.030983\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:22 INFO 140196492334912] Epoch[240] Batch[0] avg_epoch_loss=1.981744\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:21 INFO 140252856112960] Epoch[241] Batch[0] avg_epoch_loss=2.391605\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:23 INFO 140624803325760] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10180.758953094482, \"sum\": 10180.758953094482, \"min\": 10180.758953094482}}, \"EndTime\": 1538410403.160071, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410392.979051}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:23 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.466565175 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:23 INFO 140624803325760] #progress_metric: host=algo-3, completed 98 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:25 INFO 140624803325760] Epoch[247] Batch[0] avg_epoch_loss=1.734162\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:27 INFO 140196492334912] Epoch[240] Batch[5] avg_epoch_loss=2.046683\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:27 INFO 140196492334912] Epoch[240] Batch [5]#011Speed: 120.91 samples/sec#011loss=2.046683\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:27 INFO 140252856112960] Epoch[241] Batch[5] avg_epoch_loss=1.946520\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:27 INFO 140252856112960] Epoch[241] Batch [5]#011Speed: 122.06 samples/sec#011loss=1.946520\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:28 INFO 139663579178816] Epoch[244] Batch[10] avg_epoch_loss=1.937161\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:28 INFO 139663579178816] Epoch[244] Batch [10]#011Speed: 125.09 samples/sec#011loss=1.800027\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:28 INFO 139663579178816] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11445.793151855469, \"sum\": 11445.793151855469, \"min\": 11445.793151855469}}, \"EndTime\": 1538410408.24283, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410396.796812}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:28 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=115.936658204 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:28 INFO 139663579178816] #progress_metric: host=algo-2, completed 98 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:29 INFO 139663579178816] Epoch[245] Batch[0] avg_epoch_loss=1.857250\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:31 INFO 140196492334912] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10718.125104904175, \"sum\": 10718.125104904175, \"min\": 10718.125104904175}}, \"EndTime\": 1538410411.130693, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410400.412313}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:31 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=118.76932221 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:31 INFO 140196492334912] #progress_metric: host=algo-4, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:30 INFO 140252856112960] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10710.22915840149, \"sum\": 10710.22915840149, \"min\": 10710.22915840149}}, \"EndTime\": 1538410410.99727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410400.286787}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:30 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.13696335 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:30 INFO 140252856112960] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[10/01/2018 16:13:29 INFO 140624803325760] Epoch[247] Batch[5] avg_epoch_loss=1.807809\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:29 INFO 140624803325760] Epoch[247] Batch [5]#011Speed: 153.39 samples/sec#011loss=1.807809\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:32 INFO 140252856112960] Epoch[242] Batch[0] avg_epoch_loss=2.171554\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:32 INFO 140196492334912] Epoch[241] Batch[0] avg_epoch_loss=1.982539\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:33 INFO 140624803325760] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10346.842050552368, \"sum\": 10346.842050552368, \"min\": 10346.842050552368}}, \"EndTime\": 1538410413.507324, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410403.160161}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:33 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=123.514325273 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:33 INFO 140624803325760] #progress_metric: host=algo-3, completed 99 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:34 INFO 139663579178816] Epoch[245] Batch[5] avg_epoch_loss=1.966135\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:34 INFO 139663579178816] Epoch[245] Batch [5]#011Speed: 126.50 samples/sec#011loss=1.966135\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:35 INFO 140624803325760] Epoch[248] Batch[0] avg_epoch_loss=1.789113\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:38 INFO 140196492334912] Epoch[241] Batch[5] avg_epoch_loss=1.845470\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:38 INFO 140196492334912] Epoch[241] Batch [5]#011Speed: 122.28 samples/sec#011loss=1.845470\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:37 INFO 140252856112960] Epoch[242] Batch[5] avg_epoch_loss=1.978776\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:37 INFO 140252856112960] Epoch[242] Batch [5]#011Speed: 119.40 samples/sec#011loss=1.978776\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:38 INFO 139663579178816] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10352.02407836914, \"sum\": 10352.02407836914, \"min\": 10352.02407836914}}, \"EndTime\": 1538410418.595224, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410408.24291}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:38 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=121.423501531 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:38 INFO 139663579178816] #progress_metric: host=algo-2, completed 98 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:40 INFO 139663579178816] Epoch[246] Batch[0] avg_epoch_loss=1.463521\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:39 INFO 140624803325760] Epoch[248] Batch[5] avg_epoch_loss=1.814798\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:39 INFO 140624803325760] Epoch[248] Batch [5]#011Speed: 156.81 samples/sec#011loss=1.814798\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:42 INFO 140196492334912] Epoch[241] Batch[10] avg_epoch_loss=1.905355\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:42 INFO 140196492334912] Epoch[241] Batch [10]#011Speed: 150.43 samples/sec#011loss=1.977216\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:42 INFO 140196492334912] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11125.761985778809, \"sum\": 11125.761985778809, \"min\": 11125.761985778809}}, \"EndTime\": 1538410422.256807, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410411.130781}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:42 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.271390217 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:42 INFO 140196492334912] #progress_metric: host=algo-4, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:41 INFO 140252856112960] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10797.69492149353, \"sum\": 10797.69492149353, \"min\": 10797.69492149353}}, \"EndTime\": 1538410421.79531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410410.997358}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:41 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.616257361 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:41 INFO 140252856112960] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:43 INFO 140252856112960] Epoch[243] Batch[0] avg_epoch_loss=1.531712\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:43 INFO 140624803325760] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10225.013971328735, \"sum\": 10225.013971328735, \"min\": 10225.013971328735}}, \"EndTime\": 1538410423.732696, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410413.507415}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:43 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=121.856438731 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:43 INFO 140624803325760] #progress_metric: host=algo-3, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:44 INFO 140196492334912] Epoch[242] Batch[0] avg_epoch_loss=1.813328\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:45 INFO 139663579178816] Epoch[246] Batch[5] avg_epoch_loss=1.904872\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:45 INFO 139663579178816] Epoch[246] Batch [5]#011Speed: 126.07 samples/sec#011loss=1.904872\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:46 INFO 140624803325760] Epoch[249] Batch[0] avg_epoch_loss=2.078314\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:48 INFO 140252856112960] Epoch[243] Batch[5] avg_epoch_loss=1.878859\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:48 INFO 140252856112960] Epoch[243] Batch [5]#011Speed: 120.93 samples/sec#011loss=1.878859\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:48 INFO 140196492334912] Epoch[242] Batch[5] avg_epoch_loss=1.875339\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:48 INFO 140196492334912] Epoch[242] Batch [5]#011Speed: 151.76 samples/sec#011loss=1.875339\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:49 INFO 139663579178816] Epoch[246] Batch[10] avg_epoch_loss=1.676175\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:49 INFO 139663579178816] Epoch[246] Batch [10]#011Speed: 152.13 samples/sec#011loss=1.401739\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:49 INFO 139663579178816] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10900.893926620483, \"sum\": 10900.893926620483, \"min\": 10900.893926620483}}, \"EndTime\": 1538410429.496431, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410418.5953}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:49 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=119.989046694 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:49 INFO 139663579178816] #progress_metric: host=algo-2, completed 98 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:52 INFO 140252856112960] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10661.263942718506, \"sum\": 10661.263942718506, \"min\": 10661.263942718506}}, \"EndTime\": 1538410432.456938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410421.7954}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:52 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=117.057751911 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:52 INFO 140252856112960] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:50 INFO 140624803325760] Epoch[249] Batch[5] avg_epoch_loss=1.843545\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:50 INFO 140624803325760] Epoch[249] Batch [5]#011Speed: 157.36 samples/sec#011loss=1.843545\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:51 INFO 139663579178816] Epoch[247] Batch[0] avg_epoch_loss=2.277424\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:54 INFO 140196492334912] Epoch[242] Batch[10] avg_epoch_loss=2.161226\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:54 INFO 140196492334912] Epoch[242] Batch [10]#011Speed: 119.87 samples/sec#011loss=2.504290\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:54 INFO 140196492334912] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11974.87998008728, \"sum\": 11974.87998008728, \"min\": 11974.87998008728}}, \"EndTime\": 1538410434.232019, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410422.25689}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:54 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=110.313003931 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:54 INFO 140196492334912] #progress_metric: host=algo-4, completed 97 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10109.466791152954, \"sum\": 10109.466791152954, \"min\": 10109.466791152954}}, \"EndTime\": 1538410433.842513, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410423.732785}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] #throughput_metric: host=algo-3, train throughput=125.820977246 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] #progress_metric: host=algo-3, completed 100 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] Final loss: 1.8235758543 (occurred at epoch 249)\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] #quality_metric: host=algo-3, train final_loss <loss>=1.8235758543\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 WARNING 140624803325760] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:13:53 INFO 140624803325760] Worker algo-3 finished training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:54 INFO 140252856112960] Epoch[244] Batch[0] avg_epoch_loss=1.763225\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:55 INFO 139663579178816] Epoch[247] Batch[5] avg_epoch_loss=2.131369\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:55 INFO 139663579178816] Epoch[247] Batch [5]#011Speed: 159.23 samples/sec#011loss=2.131369\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:13:55 INFO 140196492334912] Epoch[243] Batch[0] avg_epoch_loss=2.152255\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:59 INFO 140252856112960] Epoch[244] Batch[5] avg_epoch_loss=1.929947\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:13:59 INFO 140252856112960] Epoch[244] Batch [5]#011Speed: 123.76 samples/sec#011loss=1.929947\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:59 INFO 139663579178816] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9963.130950927734, \"sum\": 9963.130950927734, \"min\": 9963.130950927734}}, \"EndTime\": 1538410439.45985, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410429.496499}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:59 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=126.264160253 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:13:59 INFO 139663579178816] #progress_metric: host=algo-2, completed 99 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:01 INFO 139663579178816] Epoch[248] Batch[0] avg_epoch_loss=2.049522\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:02 INFO 140252856112960] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10536.97419166565, \"sum\": 10536.97419166565, \"min\": 10536.97419166565}}, \"EndTime\": 1538410442.994279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410432.457033}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:02 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=119.957024362 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:02 INFO 140252856112960] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[10/01/2018 16:14:01 INFO 140196492334912] Epoch[243] Batch[5] avg_epoch_loss=1.815403\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:01 INFO 140196492334912] Epoch[243] Batch [5]#011Speed: 121.66 samples/sec#011loss=1.815403\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:04 INFO 140252856112960] Epoch[245] Batch[0] avg_epoch_loss=1.974760\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:04 INFO 140196492334912] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10658.473014831543, \"sum\": 10658.473014831543, \"min\": 10658.473014831543}}, \"EndTime\": 1538410444.890851, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410434.232112}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:04 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.621849354 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:04 INFO 140196492334912] #progress_metric: host=algo-4, completed 97 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:05 INFO 139663579178816] Epoch[248] Batch[5] avg_epoch_loss=1.827965\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:05 INFO 139663579178816] Epoch[248] Batch [5]#011Speed: 159.36 samples/sec#011loss=1.827965\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:06 INFO 140196492334912] Epoch[244] Batch[0] avg_epoch_loss=1.803082\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:09 INFO 139663579178816] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9981.971025466919, \"sum\": 9981.971025466919, \"min\": 9981.971025466919}}, \"EndTime\": 1538410449.442126, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410439.459924}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:09 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=125.925658998 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:09 INFO 139663579178816] #progress_metric: host=algo-2, completed 99 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:09 INFO 140252856112960] Epoch[245] Batch[5] avg_epoch_loss=2.021264\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:09 INFO 140252856112960] Epoch[245] Batch [5]#011Speed: 120.87 samples/sec#011loss=2.021264\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:11 INFO 139663579178816] Epoch[249] Batch[0] avg_epoch_loss=2.007982\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:11 INFO 140196492334912] Epoch[244] Batch[5] avg_epoch_loss=1.896844\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:11 INFO 140196492334912] Epoch[244] Batch [5]#011Speed: 121.78 samples/sec#011loss=1.896844\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:14 INFO 140252856112960] Epoch[245] Batch[10] avg_epoch_loss=1.941552\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:14 INFO 140252856112960] Epoch[245] Batch [10]#011Speed: 148.82 samples/sec#011loss=1.845897\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:14 INFO 140252856112960] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11267.5039768219, \"sum\": 11267.5039768219, \"min\": 11267.5039768219}}, \"EndTime\": 1538410454.262138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410442.994368}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:14 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=118.835949213 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:14 INFO 140252856112960] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:15 INFO 139663579178816] Epoch[249] Batch[5] avg_epoch_loss=1.916710\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:15 INFO 139663579178816] Epoch[249] Batch [5]#011Speed: 160.28 samples/sec#011loss=1.916710\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:16 INFO 140196492334912] Epoch[244] Batch[10] avg_epoch_loss=1.753501\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:16 INFO 140196492334912] Epoch[244] Batch [10]#011Speed: 150.86 samples/sec#011loss=1.581490\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:16 INFO 140196492334912] processed a total of 1358 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11153.82695198059, \"sum\": 11153.82695198059, \"min\": 11153.82695198059}}, \"EndTime\": 1538410456.044988, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410444.890923}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:16 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=121.75058045 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:16 INFO 140196492334912] #progress_metric: host=algo-4, completed 98 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:16 INFO 140252856112960] Epoch[246] Batch[0] avg_epoch_loss=1.796801\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:18 INFO 140196492334912] Epoch[245] Batch[0] avg_epoch_loss=1.900486\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9951.07102394104, \"sum\": 9951.07102394104, \"min\": 9951.07102394104}}, \"EndTime\": 1538410459.393497, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410449.442198}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] #throughput_metric: host=algo-2, train throughput=127.623019307 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] #progress_metric: host=algo-2, completed 100 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] Final loss: 1.91032941341 (occurred at epoch 249)\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] #quality_metric: host=algo-2, train final_loss <loss>=1.91032941341\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 WARNING 139663579178816] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:14:19 INFO 139663579178816] Worker algo-2 finished training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:20 INFO 140252856112960] Epoch[246] Batch[5] avg_epoch_loss=1.804624\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:20 INFO 140252856112960] Epoch[246] Batch [5]#011Speed: 154.22 samples/sec#011loss=1.804624\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:22 INFO 140196492334912] Epoch[245] Batch[5] avg_epoch_loss=1.847829\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:22 INFO 140196492334912] Epoch[245] Batch [5]#011Speed: 152.49 samples/sec#011loss=1.847829\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:26 INFO 140252856112960] Epoch[246] Batch[10] avg_epoch_loss=1.774918\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:26 INFO 140252856112960] Epoch[246] Batch [10]#011Speed: 120.81 samples/sec#011loss=1.739271\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:26 INFO 140252856112960] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11818.526983261108, \"sum\": 11818.526983261108, \"min\": 11818.526983261108}}, \"EndTime\": 1538410466.080994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410454.262223}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:26 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=108.388105627 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:26 INFO 140252856112960] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:27 INFO 140196492334912] Epoch[245] Batch[10] avg_epoch_loss=1.631883\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:27 INFO 140196492334912] Epoch[245] Batch [10]#011Speed: 121.20 samples/sec#011loss=1.372748\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:27 INFO 140196492334912] processed a total of 1339 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11844.789028167725, \"sum\": 11844.789028167725, \"min\": 11844.789028167725}}, \"EndTime\": 1538410467.890107, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410456.045071}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:27 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.044212935 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:27 INFO 140196492334912] #progress_metric: host=algo-4, completed 98 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:27 INFO 140252856112960] Epoch[247] Batch[0] avg_epoch_loss=1.722449\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:29 INFO 140196492334912] Epoch[246] Batch[0] avg_epoch_loss=1.761422\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:33 INFO 140252856112960] Epoch[247] Batch[5] avg_epoch_loss=1.713584\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:33 INFO 140252856112960] Epoch[247] Batch [5]#011Speed: 118.40 samples/sec#011loss=1.713584\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:34 INFO 140196492334912] Epoch[246] Batch[5] avg_epoch_loss=1.721433\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:34 INFO 140196492334912] Epoch[246] Batch [5]#011Speed: 121.67 samples/sec#011loss=1.721433\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:37 INFO 140252856112960] Epoch[247] Batch[10] avg_epoch_loss=1.789057\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:37 INFO 140252856112960] Epoch[247] Batch [10]#011Speed: 146.46 samples/sec#011loss=1.879625\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:37 INFO 140252856112960] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11490.755081176758, \"sum\": 11490.755081176758, \"min\": 11490.755081176758}}, \"EndTime\": 1538410477.572051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410466.081071}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:37 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.220390009 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:37 INFO 140252856112960] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:38 INFO 140196492334912] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10851.412057876587, \"sum\": 10851.412057876587, \"min\": 10851.412057876587}}, \"EndTime\": 1538410478.741855, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410467.8902}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:38 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=113.071373669 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:38 INFO 140196492334912] #progress_metric: host=algo-4, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:40 INFO 140196492334912] Epoch[247] Batch[0] avg_epoch_loss=1.769130\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:40 INFO 140252856112960] Epoch[248] Batch[0] avg_epoch_loss=1.784138\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:44 INFO 140252856112960] Epoch[248] Batch[5] avg_epoch_loss=1.655302\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:44 INFO 140252856112960] Epoch[248] Batch [5]#011Speed: 147.93 samples/sec#011loss=1.655302\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:45 INFO 140196492334912] Epoch[247] Batch[5] avg_epoch_loss=1.763575\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:45 INFO 140196492334912] Epoch[247] Batch [5]#011Speed: 119.07 samples/sec#011loss=1.763575\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:49 INFO 140196492334912] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10878.502130508423, \"sum\": 10878.502130508423, \"min\": 10878.502130508423}}, \"EndTime\": 1538410489.620709, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410478.741954}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:49 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=117.56994189 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:49 INFO 140196492334912] #progress_metric: host=algo-4, completed 99 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:49 INFO 140252856112960] Epoch[248] Batch[10] avg_epoch_loss=1.607151\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:49 INFO 140252856112960] Epoch[248] Batch [10]#011Speed: 115.57 samples/sec#011loss=1.549369\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:49 INFO 140252856112960] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 12319.756031036377, \"sum\": 12319.756031036377, \"min\": 12319.756031036377}}, \"EndTime\": 1538410489.892111, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410477.572122}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:49 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=106.170002877 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:49 INFO 140252856112960] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:51 INFO 140196492334912] Epoch[248] Batch[0] avg_epoch_loss=1.572411\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:51 INFO 140252856112960] Epoch[249] Batch[0] avg_epoch_loss=1.687290\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:56 INFO 140196492334912] Epoch[248] Batch[5] avg_epoch_loss=1.659555\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:14:56 INFO 140196492334912] Epoch[248] Batch [5]#011Speed: 121.40 samples/sec#011loss=1.659555\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:57 INFO 140252856112960] Epoch[249] Batch[5] avg_epoch_loss=1.705921\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:14:57 INFO 140252856112960] Epoch[249] Batch [5]#011Speed: 117.42 samples/sec#011loss=1.705921\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:00 INFO 140196492334912] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10675.392866134644, \"sum\": 10675.392866134644, \"min\": 10675.392866134644}}, \"EndTime\": 1538410500.296457, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410489.620796}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:00 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=119.057241521 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:00 INFO 140196492334912] #progress_metric: host=algo-4, completed 99 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11048.63691329956, \"sum\": 11048.63691329956, \"min\": 11048.63691329956}}, \"EndTime\": 1538410500.941067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410489.892184}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] #throughput_metric: host=algo-1, train throughput=113.497044342 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] Final loss: 1.74657210112 (occurred at epoch 249)\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] #quality_metric: host=algo-1, train final_loss <loss>=1.74657210112\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 WARNING 140252856112960] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:00 INFO 140252856112960] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:01 INFO 140196492334912] Epoch[249] Batch[0] avg_epoch_loss=1.750560\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:07 INFO 140196492334912] Epoch[249] Batch[5] avg_epoch_loss=1.779380\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:07 INFO 140196492334912] Epoch[249] Batch [5]#011Speed: 121.29 samples/sec#011loss=1.779380\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] Epoch[249] Batch[10] avg_epoch_loss=1.770658\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] Epoch[249] Batch [10]#011Speed: 147.69 samples/sec#011loss=1.760193\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 11224.791049957275, \"sum\": 11224.791049957275, \"min\": 11224.791049957275}}, \"EndTime\": 1538410511.521614, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410500.296558}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] #throughput_metric: host=algo-4, train throughput=115.54651206 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] #progress_metric: host=algo-4, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] Final loss: 1.77065839551 (occurred at epoch 249)\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] #quality_metric: host=algo-4, train final_loss <loss>=1.77065839551\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 WARNING 140196492334912] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:15:11 INFO 140196492334912] Worker algo-4 finished training.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2819770.528078079, \"sum\": 2819770.528078079, \"min\": 2819770.528078079}, \"setuptime\": {\"count\": 1, \"max\": 20.4010009765625, \"sum\": 20.4010009765625, \"min\": 20.4010009765625}}, \"EndTime\": 1538410511.523177, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410511.521698}\n",
      "\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2818274.062871933, \"sum\": 2818274.062871933, \"min\": 2818274.062871933}, \"setuptime\": {\"count\": 1, \"max\": 16.10589027404785, \"sum\": 16.10589027404785, \"min\": 16.10589027404785}}, \"EndTime\": 1538410511.523269, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410433.842603}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:15:11 INFO 140252856112960] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 2820331.99095726, \"sum\": 2820331.99095726, \"min\": 2820331.99095726}, \"setuptime\": {\"count\": 1, \"max\": 18.191814422607422, \"sum\": 18.191814422607422, \"min\": 18.191814422607422}}, \"EndTime\": 1538410511.523554, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538410459.393573}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-10-01 16:15:42 Uploading - Uploading generated training model\n",
      "2018-10-01 16:15:47 Completed - Training job completed\n",
      "Billable seconds: 11467\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "tor = sagemaker.estimator.Estimator.attach(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build inference data to test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:684473352813:model/devday-deepartraining-2018-10-01-15-25-01\n"
     ]
    }
   ],
   "source": [
    "model_name = job_name\n",
    "\n",
    "info = sgmaker.describe_training_job(TrainingJobName=job_name)\n",
    "modeldata = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "container = {\n",
    "    'Image': img,\n",
    "    'ModelDataUrl': modeldata\n",
    "}\n",
    "\n",
    "created_model = sgmaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = roleARN,\n",
    "    PrimaryContainer = container)\n",
    "\n",
    "print(created_model['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DevDayEndpointConfig-2018-10-01-16-16-08\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:684473352813:endpoint-config/devdayendpointconfig-2018-10-01-16-16-08\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = 'DevDayEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "created_endpoint_config = sgmaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + created_endpoint_config['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DevDayEndpoint-2018-10-01-16-16-12\n",
      "arn:aws:sagemaker:us-east-1:684473352813:endpoint/devdayendpoint-2018-10-01-16-16-12\n",
      "Status: Creating\n",
      "Arn: arn:aws:sagemaker:us-east-1:684473352813:endpoint/devdayendpoint-2018-10-01-16-16-12\n",
      "Create endpoint ended with status: InService\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'DevDayEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "created_endpoint = sgmaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(created_endpoint['EndpointArn'])\n",
    "\n",
    "resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sgmaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sgmaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sgmaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processedDF[processedDF['Date2'] == '2008-01'].iloc[0]['TimeHrs'] < 10:\n",
    "    starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ '0' + str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "else:\n",
    "    starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-14')]['Global_active_power'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromLastSolstice'].tolist(),processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromNextSolstice'].tolist()]\n",
    "instances = []\n",
    "\n",
    "instances.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "request = {}\n",
    "request['instances'] = instances\n",
    "request['configuration'] = {}\n",
    "request['configuration']['num_samples'] = 50\n",
    "request['configuration']['output_types'] = ['mean']\n",
    "json_request = json.dumps(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'mean': [0.2014748901, 0.2074831575, 0.163772583, 0.1995865107, 0.2048069537, 0.2064585537, 0.2140551805, 0.1933979094, 0.2028960288, 0.1940477341, 0.1748319566, 0.1916774213, 0.1936586797, 0.2143019736, 0.2204672098, 0.2261081189, 0.1802844107, 0.1921952814, 0.2314457893, 0.2358057797, 0.1985491812, 0.2387798131, 0.2182034999, 0.2298857123]}]}\n"
     ]
    }
   ],
   "source": [
    "runtime = boto3.client('sagemaker-runtime')\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json_request)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltFrame = processedDF.replace(to_replace=\"NaN\",value=np.nan,regex=True)\n",
    "\n",
    "def getTime(row):\n",
    "    if row['TimeHrs'] < 10:\n",
    "        return str(row['Date2'])[:10] + ' 0' + str(row['TimeHrs']) + \":00:00\"\n",
    "    else:\n",
    "        return str(row['Date2'])[:10] + ' ' + str(row['TimeHrs']) + \":00:00\"\n",
    "\n",
    "pltFrame['TimeFull'] = pltFrame.apply(lambda row: getTime(row),axis=1)\n",
    "pltFrame['TimeFull'] = pd.to_datetime(pltFrame['TimeFull'])\n",
    "pltFrame.set_index('TimeFull',inplace=True)\n",
    "predictions = pd.Series(data=result['predictions'][0]['mean'],index=pltFrame.index[pltFrame['Date2'] >= '2008-08-14'][:24])\n",
    "pltf = pd.concat([pltFrame,predictions],axis=1)\n",
    "\n",
    "## Check your dataframe\n",
    "#pltf.loc[pltf.index[pltf['Date2'] >= '2008-08-14'][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6d2a738d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltf[0].plot(kind='line',style='ko--',use_index=True,rot=30,ylim=[0.0,3.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))\n",
    "pltf['Global_active_power'].plot(kind='line',style='bo--',use_index=True,rot=30,ylim=[0.0,3.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, to see how the model also predicts for a different series, say we ask it to predict for a different series. See what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processedDF[processedDF['Date2'] == '2008-01'].iloc[0]['TimeHrs'] < 10:\n",
    "    starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ '0' + str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "else:\n",
    "    starttime = str(processedDF[(processedDF['Date2'] == '2008-08')].iloc[0]['Date2'])[:10]+' '+ str(processedDF[processedDF['Date2'] == '2008-08'].iloc[0]['TimeHrs'])+':00:00'\n",
    "\n",
    "target = processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-14')]['Global_intensity'].tolist()\n",
    "cat = [0,0]\n",
    "dynfeat = [processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromLastSolstice'].tolist(),processedDF[(processedDF['Date2'] >= '2008-08-01') & (processedDF['Date2'] < '2008-08-15')]['distanceFromNextSolstice'].tolist()]\n",
    "instances = []\n",
    "\n",
    "instances.append({\n",
    "    \"start\": starttime,\n",
    "    \"target\": target,\n",
    "    \"cat\": cat,\n",
    "    \"dynamic_feat\": dynfeat\n",
    "})\n",
    "\n",
    "request = {}\n",
    "request['instances'] = instances\n",
    "request['configuration'] = {}\n",
    "request['configuration']['num_samples'] = 50\n",
    "request['configuration']['output_types'] = ['mean']\n",
    "json_request = json.dumps(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'mean': [0.8886301517, 0.9159537554, 0.7416194677, 0.8516206145, 0.8483208418, 0.8817822933, 0.9169275165, 0.8365494609, 0.8921685219, 0.8741668463, 0.7856843472, 0.8761405945, 0.8715088367, 0.960237205, 1.0142562389, 1.0668101311, 0.8398330808, 0.9008331299, 1.0748158693, 1.0996569395, 0.9420467615, 1.124576211, 1.0516116619, 1.1171754599]}]}\n"
     ]
    }
   ],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/json', \n",
    "                                   Body=json_request)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take this and plot it against the truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pltFrame = processedDF.replace(to_replace=\"NaN\",value=np.nan,regex=True)\n",
    "\n",
    "def getTime(row):\n",
    "    if row['TimeHrs'] < 10:\n",
    "        return str(row['Date2'])[:10] + ' 0' + str(row['TimeHrs']) + \":00:00\"\n",
    "    else:\n",
    "        return str(row['Date2'])[:10] + ' ' + str(row['TimeHrs']) + \":00:00\"\n",
    "\n",
    "pltFrame['TimeFull'] = pltFrame.apply(lambda row: getTime(row),axis=1)\n",
    "pltFrame['TimeFull'] = pd.to_datetime(pltFrame['TimeFull'])\n",
    "pltFrame.set_index('TimeFull',inplace=True)\n",
    "predictions = pd.Series(data=result['predictions'][0]['mean'],index=pltFrame.index[pltFrame['Date2'] >= '2008-08-14'][:24])\n",
    "pltf = pd.concat([pltFrame,predictions],axis=1)\n",
    "\n",
    "## Check your dataframe\n",
    "#pltf.loc[pltf.index[pltf['Date2'] >= '2008-08-14'][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6d2414160>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pltf[0].plot(kind='line',style='ko--',use_index=True,rot=30,ylim=[0.0,5.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))\n",
    "pltf['Global_intensity'].plot(kind='line',style='bo--',use_index=True,rot=30,ylim=[0.0,5.0],xlim=[pltf.index[pltf['Date2'] >= '2008-08-13'][0],pltf.index[pltf['Date2'] >= '2008-08-14'][24]],figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '67dc57e7-3802-4fa5-ad28-5bfdccc3c31d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '67dc57e7-3802-4fa5-ad28-5bfdccc3c31d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 01 Oct 2018 16:23:07 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgmaker.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"0.2\",\n",
    "          \"MinValue\": \"0.00\",\n",
    "          \"Name\": \"dropout_rate\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"0.050\",\n",
    "          \"MinValue\": \"0.001\",\n",
    "          \"Name\": \"learning_rate\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"MaxValue\": \"512\",\n",
    "          \"MinValue\": \"64\",\n",
    "          \"Name\": \"mini_batch_size\"\n",
    "        },\n",
    "        {\n",
    "          \"MaxValue\": \"20\",\n",
    "          \"MinValue\": \"10\",\n",
    "          \"Name\": \"epochs\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"ResourceLimits\": {\n",
    "      \"MaxNumberOfTrainingJobs\": 10,\n",
    "      \"MaxParallelTrainingJobs\": 2\n",
    "    },\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\n",
    "      \"MetricName\": \"test:RMSE\",\n",
    "      \"Type\": \"Minimize\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "training_job_definition = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": img,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": roleARN,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": output_location\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 4,\n",
    "        \"InstanceType\": \"ml.c4.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"StaticHyperParameters\": {\n",
    "        \"time_freq\": 'H', # hourly series\n",
    "        \"context_length\": str(context_length),\n",
    "        \"prediction_length\": str(prediction_length), # number of data points to predict\n",
    "        \"num_dynamic_feat\": \"auto\",\n",
    "        \"num_cells\": \"60\", \n",
    "        \"num_layers\": \"4\",\n",
    "        \"likelihood\": \"gaussian\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 60 * 240 # Give it four hours at best, could increase this for production scale\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": training_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"test\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": testing_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO Job started, name of the HPO Job: HPO-Tune-16-25-14 and \n",
      " Tuning Job ARN: arn:aws:sagemaker:us-east-1:684473352813:hyper-parameter-tuning-job/hpo-tune-16-25-14\n"
     ]
    }
   ],
   "source": [
    "tuning_job_name = \"HPO-Tune-\" + strftime(\"%H-%M-%S\", gmtime())\n",
    "hpo_tuning_job_arn = sgmaker.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,\n",
    "                                           HyperParameterTuningJobConfig = tuning_job_config,\n",
    "                                           TrainingJobDefinition = training_job_definition)\n",
    "print(\"HPO Job started, name of the HPO Job: {} and \\n Tuning Job ARN: {}\".format(tuning_job_name,hpo_tuning_job_arn['HyperParameterTuningJobArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Hyperparameeter tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.tuner as tun\n",
    "\n",
    "hpo_tuner = tun.HyperparameterTuner.attach(tuning_job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = hpo_tuner.analytics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mini_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.994919</td>\n",
       "      <td>729.0</td>\n",
       "      <td>2018-10-01 17:17:02+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-010-09786c9d</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 17:04:53+00:00</td>\n",
       "      <td>0.192124</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.449408</td>\n",
       "      <td>147.0</td>\n",
       "      <td>2018-10-01 17:03:21+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-009-04eb6793</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 17:00:54+00:00</td>\n",
       "      <td>0.076334</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>343.415253</td>\n",
       "      <td>226.0</td>\n",
       "      <td>2018-10-01 17:01:25+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-008-5aaf7b4c</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:57:39+00:00</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.021494</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.831675</td>\n",
       "      <td>388.0</td>\n",
       "      <td>2018-10-01 16:57:39+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-007-de863ffe</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:51:11+00:00</td>\n",
       "      <td>0.175772</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.909111</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2018-10-01 16:54:27+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-006-f892d336</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:50:22+00:00</td>\n",
       "      <td>0.190274</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.011114</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35.749325</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2018-10-01 16:48:17+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-005-02e716a5</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:42:38+00:00</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.065823</td>\n",
       "      <td>327.0</td>\n",
       "      <td>2018-10-01 16:46:51+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-004-8ec6d6c9</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:41:24+00:00</td>\n",
       "      <td>0.183658</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.028968</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85.052689</td>\n",
       "      <td>382.0</td>\n",
       "      <td>2018-10-01 16:38:12+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-003-d96543e0</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:31:50+00:00</td>\n",
       "      <td>0.024268</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.012843</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2018-10-01 16:29:15+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-002-ed7f091e</td>\n",
       "      <td>Failed</td>\n",
       "      <td>2018-10-01 16:28:03+00:00</td>\n",
       "      <td>0.075455</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.041002</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1175.794312</td>\n",
       "      <td>762.0</td>\n",
       "      <td>2018-10-01 16:40:32+00:00</td>\n",
       "      <td>HPO-Tune-16-25-14-001-50d7e9ea</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2018-10-01 16:27:50+00:00</td>\n",
       "      <td>0.110003</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.046835</td>\n",
       "      <td>502.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FinalObjectiveValue  TrainingElapsedTimeSeconds           TrainingEndTime  \\\n",
       "0            18.994919                       729.0 2018-10-01 17:17:02+00:00   \n",
       "1            23.449408                       147.0 2018-10-01 17:03:21+00:00   \n",
       "2           343.415253                       226.0 2018-10-01 17:01:25+00:00   \n",
       "3             7.831675                       388.0 2018-10-01 16:57:39+00:00   \n",
       "4            16.909111                       245.0 2018-10-01 16:54:27+00:00   \n",
       "5            35.749325                       339.0 2018-10-01 16:48:17+00:00   \n",
       "6            19.065823                       327.0 2018-10-01 16:46:51+00:00   \n",
       "7            85.052689                       382.0 2018-10-01 16:38:12+00:00   \n",
       "8                  NaN                        72.0 2018-10-01 16:29:15+00:00   \n",
       "9          1175.794312                       762.0 2018-10-01 16:40:32+00:00   \n",
       "\n",
       "                  TrainingJobName TrainingJobStatus         TrainingStartTime  \\\n",
       "0  HPO-Tune-16-25-14-010-09786c9d         Completed 2018-10-01 17:04:53+00:00   \n",
       "1  HPO-Tune-16-25-14-009-04eb6793         Completed 2018-10-01 17:00:54+00:00   \n",
       "2  HPO-Tune-16-25-14-008-5aaf7b4c         Completed 2018-10-01 16:57:39+00:00   \n",
       "3  HPO-Tune-16-25-14-007-de863ffe         Completed 2018-10-01 16:51:11+00:00   \n",
       "4  HPO-Tune-16-25-14-006-f892d336         Completed 2018-10-01 16:50:22+00:00   \n",
       "5  HPO-Tune-16-25-14-005-02e716a5         Completed 2018-10-01 16:42:38+00:00   \n",
       "6  HPO-Tune-16-25-14-004-8ec6d6c9         Completed 2018-10-01 16:41:24+00:00   \n",
       "7  HPO-Tune-16-25-14-003-d96543e0         Completed 2018-10-01 16:31:50+00:00   \n",
       "8  HPO-Tune-16-25-14-002-ed7f091e            Failed 2018-10-01 16:28:03+00:00   \n",
       "9  HPO-Tune-16-25-14-001-50d7e9ea         Completed 2018-10-01 16:27:50+00:00   \n",
       "\n",
       "   dropout_rate  epochs  learning_rate  mini_batch_size  \n",
       "0      0.192124    20.0       0.010139            370.0  \n",
       "1      0.076334    10.0       0.001789             64.0  \n",
       "2      0.098689    12.0       0.021494             88.0  \n",
       "3      0.175772    13.0       0.001149            285.0  \n",
       "4      0.190274    13.0       0.011114            123.0  \n",
       "5      0.022688    12.0       0.005519            167.0  \n",
       "6      0.183658    10.0       0.028968            172.0  \n",
       "7      0.024268    13.0       0.012843            231.0  \n",
       "8      0.075455    15.0       0.041002             75.0  \n",
       "9      0.110003    14.0       0.046835            502.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.dataframe(force_refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 16:48:36 Starting - Preparing the instances for training\n",
      "2018-10-01 16:51:11 Downloading - Downloading input data\n",
      "2018-10-01 16:51:20 Training - Training image download completed. Training in progress.\n",
      "2018-10-01 16:57:30 Uploading - Uploading generated training model\n",
      "2018-10-01 16:57:39 Completed - Training job completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: forecasting-deepar-2018-10-01-17-42-54-098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[33mArguments: train\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.17577224158449561', u'_tuning_objective_metric': u'test:RMSE', u'learning_rate': u'0.0011491169372165944', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'13', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'4', u'mini_batch_size': u'285', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Final configuration: {u'dropout_rate': u'0.17577224158449561', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'test:RMSE', u'num_eval_samples': u'100', u'learning_rate': u'0.0011491169372165944', u'num_layers': u'4', u'epochs': u'13', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'285', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df93e5a9-abfa-4b47-9eaf-817d1e3cf509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f08ee70f-2512-4116-b9c9-4baafe5586cc', 'PWD': '/'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df93e5a9-abfa-4b47-9eaf-817d1e3cf509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f08ee70f-2512-4116-b9c9-4baafe5586cc', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Launching parameter server for role server\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df93e5a9-abfa-4b47-9eaf-817d1e3cf509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f08ee70f-2512-4116-b9c9-4baafe5586cc', 'PWD': '/'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df93e5a9-abfa-4b47-9eaf-817d1e3cf509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f08ee70f-2512-4116-b9c9-4baafe5586cc', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/df93e5a9-abfa-4b47-9eaf-817d1e3cf509', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f08ee70f-2512-4116-b9c9-4baafe5586cc', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Training set statistics:\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Real time series\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] number of time series: 6\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] number of observations: 54762\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] mean target length: 9127\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Test set statistics:\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Real time series\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] number of time series: 6\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] number of observations: 26208\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] mean target length: 4368\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] contains missing values: no\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] nvidia-smi took: 0.0251860618591 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:26 INFO 140088290715456] Create Store: dist_async\u001b[0m\n",
      "\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.17577224158449561', u'_tuning_objective_metric': u'test:RMSE', u'learning_rate': u'0.0011491169372165944', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'13', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'4', u'mini_batch_size': u'285', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Final configuration: {u'dropout_rate': u'0.17577224158449561', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'test:RMSE', u'num_eval_samples': u'100', u'learning_rate': u'0.0011491169372165944', u'num_layers': u'4', u'epochs': u'13', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'285', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Launching parameter server for role server\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/c3283a2a-e27e-4125-925a-8e87668d8225', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/6b046b91-af83-4969-99b1-5659f0d4aa73', 'PWD': '/'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/c3283a2a-e27e-4125-925a-8e87668d8225', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/6b046b91-af83-4969-99b1-5659f0d4aa73', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/c3283a2a-e27e-4125-925a-8e87668d8225', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/6b046b91-af83-4969-99b1-5659f0d4aa73', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Training set statistics:\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Real time series\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] number of time series: 6\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] number of observations: 54762\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] mean target length: 9127\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Test set statistics:\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Real time series\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] number of time series: 6\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] number of observations: 26208\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] mean target length: 4368\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] contains missing values: no\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] nvidia-smi took: 0.0251979827881 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:26 INFO 140274382190400] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.17577224158449561', u'_tuning_objective_metric': u'test:RMSE', u'learning_rate': u'0.0011491169372165944', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'13', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'4', u'mini_batch_size': u'285', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Final configuration: {u'dropout_rate': u'0.17577224158449561', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'test:RMSE', u'num_eval_samples': u'100', u'learning_rate': u'0.0011491169372165944', u'num_layers': u'4', u'epochs': u'13', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'285', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/897b6dc3-c30f-44f3-aa7a-c8ed1d474286', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/46bdc52d-cd34-476f-a36f-59357bc01cda', 'PWD': '/'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/897b6dc3-c30f-44f3-aa7a-c8ed1d474286', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/46bdc52d-cd34-476f-a36f-59357bc01cda', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/897b6dc3-c30f-44f3-aa7a-c8ed1d474286', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/46bdc52d-cd34-476f-a36f-59357bc01cda', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.17577224158449561', u'_tuning_objective_metric': u'test:RMSE', u'learning_rate': u'0.0011491169372165944', u'num_cells': u'60', u'prediction_length': u'24', u'epochs': u'13', u'time_freq': u'H', u'context_length': u'216', u'num_layers': u'4', u'mini_batch_size': u'285', u'likelihood': u'gaussian'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Final configuration: {u'dropout_rate': u'0.17577224158449561', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'test:RMSE', u'num_eval_samples': u'100', u'learning_rate': u'0.0011491169372165944', u'num_layers': u'4', u'epochs': u'13', u'embedding_dimension': u'10', u'num_cells': u'60', u'_num_kv_servers': u'auto', u'mini_batch_size': u'285', u'likelihood': u'gaussian', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'24', u'time_freq': u'H', u'context_length': u'216', u'_kvstore': u'auto', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Launching parameter server for role server\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/da12352d-d561-4654-bb05-1396b048d01f', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f92412c7-1ff5-4cbe-8b7e-391b07659429', 'PWD': '/'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/da12352d-d561-4654-bb05-1396b048d01f', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '4', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f92412c7-1ff5-4cbe-8b7e-391b07659429', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/da12352d-d561-4654-bb05-1396b048d01f', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '4', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib:/usr/local/nvidia/lib64:', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'ethwe', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.32.0.5', 'AWS_REGION': 'us-east-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TRAINING_JOB_NAME': 'HPO-Tune-16-25-14-007-de863ffe', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '18', 'HOSTNAME': 'aws', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/f92412c7-1ff5-4cbe-8b7e-391b07659429', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '4'}\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:28 INFO 140595750582080] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Training set statistics:\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Real time series\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] number of time series: 6\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] number of observations: 54762\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] mean target length: 9127\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Test set statistics:\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Real time series\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] number of time series: 6\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] number of observations: 26208\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] mean target length: 4368\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] contains missing values: no\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] nvidia-smi took: 0.0251979827881 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:29 INFO 140595750582080] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainingdata.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] [cardinality=auto] Inferred value of cardinality=[3, 6] from dataset.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Training set statistics:\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Real time series\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] number of time series: 6\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] number of observations: 54762\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] mean target length: 9127\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] min/mean/max target: 0.0/41.2984892251/250.342498779\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] mean abs(target): 41.2984892251\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] contains missing values: yes (0.7%)\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Test set statistics:\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Real time series\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] number of time series: 6\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] number of observations: 26208\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] mean target length: 4368\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] min/mean/max target: 0.0/41.8609201154/248.294174194\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] mean abs(target): 41.8609201154\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] contains missing values: no\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] nvidia-smi took: 0.0251779556274 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:29 INFO 140521997772608] Create Store: dist_async\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 15089.828968048096, \"sum\": 15089.828968048096, \"min\": 15089.828968048096}}, \"EndTime\": 1538412705.122668, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412686.860389}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:45 INFO 140274382190400] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 14977.447986602783, \"sum\": 14977.447986602783, \"min\": 14977.447986602783}}, \"EndTime\": 1538412705.009907, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412689.660082}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:45 INFO 140521997772608] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 15004.15587425232, \"sum\": 15004.15587425232, \"min\": 15004.15587425232}}, \"EndTime\": 1538412705.039614, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412689.198224}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:45 INFO 140595750582080] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 15045.837879180908, \"sum\": 15045.837879180908, \"min\": 15045.837879180908}}, \"EndTime\": 1538412705.080604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412686.65331}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:45 INFO 140088290715456] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.000877192982456 vs. 0.00350877192982). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[33m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.000877192982456 vs. 0.00350877192982). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.000877192982456 vs. 0.00350877192982). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 19870.607137680054, \"sum\": 19870.607137680054, \"min\": 19870.607137680054}}, \"EndTime\": 1538412706.523987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412705.080745}\n",
      "\u001b[0m\n",
      "\u001b[32m/opt/amazon/lib/python2.7/site-packages/algorithm/algo.py:367: UserWarning: Optimizer created manually outside Module but rescale_grad is not normalized to 1.0/batch_size/num_workers (0.000877192982456 vs. 0.00350877192982). Is this intended?\n",
      "  module.init_optimizer(kvstore=kvstore, optimizer=optimizer)\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 19661.005973815918, \"sum\": 19661.005973815918, \"min\": 19661.005973815918}}, \"EndTime\": 1538412706.521481, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412705.122808}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 16861.16600036621, \"sum\": 16861.16600036621, \"min\": 16861.16600036621}}, \"EndTime\": 1538412706.521338, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412705.010041}\n",
      "\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 17326.096057891846, \"sum\": 17326.096057891846, \"min\": 17326.096057891846}}, \"EndTime\": 1538412706.524416, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412705.03977}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:51:50 INFO 140088290715456] Epoch[0] Batch[0] avg_epoch_loss=4.710820\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:51:50 INFO 140274382190400] Epoch[0] Batch[0] avg_epoch_loss=4.226365\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:51:50 INFO 140521997772608] Epoch[0] Batch[0] avg_epoch_loss=5.649863\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:51:50 INFO 140595750582080] Epoch[0] Batch[0] avg_epoch_loss=3.886154\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:01 INFO 140088290715456] Epoch[0] Batch[5] avg_epoch_loss=3.622887\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:01 INFO 140088290715456] Epoch[0] Batch [5]#011Speed: 132.84 samples/sec#011loss=3.622887\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:01 INFO 140274382190400] Epoch[0] Batch[5] avg_epoch_loss=3.546909\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:01 INFO 140274382190400] Epoch[0] Batch [5]#011Speed: 128.57 samples/sec#011loss=3.546909\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:01 INFO 140521997772608] Epoch[0] Batch[5] avg_epoch_loss=3.652691\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:01 INFO 140521997772608] Epoch[0] Batch [5]#011Speed: 129.82 samples/sec#011loss=3.652691\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:01 INFO 140595750582080] Epoch[0] Batch[5] avg_epoch_loss=3.407982\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:01 INFO 140595750582080] Epoch[0] Batch [5]#011Speed: 130.17 samples/sec#011loss=3.407982\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:09 INFO 140088290715456] processed a total of 2760 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}, \"update.time\": {\"count\": 1, \"max\": 22741.293907165527, \"sum\": 22741.293907165527, \"min\": 22741.293907165527}}, \"EndTime\": 1538412729.26543, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412706.524049}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:09 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=121.364489791 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:09 INFO 140088290715456] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:10 INFO 140521997772608] Epoch[0] Batch[10] avg_epoch_loss=3.408109\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:10 INFO 140521997772608] Epoch[0] Batch [10]#011Speed: 154.94 samples/sec#011loss=3.114610\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:10 INFO 140521997772608] processed a total of 2906 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}, \"update.time\": {\"count\": 1, \"max\": 24447.447061538696, \"sum\": 24447.447061538696, \"min\": 24447.447061538696}}, \"EndTime\": 1538412730.968917, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412706.521391}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:10 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=118.866643391 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:10 INFO 140521997772608] #progress_metric: host=algo-4, completed 7 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:10 INFO 140595750582080] Epoch[0] Batch[10] avg_epoch_loss=3.325887\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:10 INFO 140595750582080] Epoch[0] Batch [10]#011Speed: 153.96 samples/sec#011loss=3.227374\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:10 INFO 140595750582080] processed a total of 2880 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}, \"update.time\": {\"count\": 1, \"max\": 24468.31202507019, \"sum\": 24468.31202507019, \"min\": 24468.31202507019}}, \"EndTime\": 1538412730.992862, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412706.524469}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:10 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=117.702694261 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:10 INFO 140595750582080] #progress_metric: host=algo-3, completed 7 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:11 INFO 140274382190400] Epoch[0] Batch[10] avg_epoch_loss=3.359054\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:11 INFO 140274382190400] Epoch[0] Batch [10]#011Speed: 152.03 samples/sec#011loss=3.133628\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:11 INFO 140274382190400] processed a total of 2859 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}, \"update.time\": {\"count\": 1, \"max\": 24772.873163223267, \"sum\": 24772.873163223267, \"min\": 24772.873163223267}}, \"EndTime\": 1538412731.294485, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412706.521531}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:11 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=115.407507239 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:11 INFO 140274382190400] #progress_metric: host=algo-2, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:12 INFO 140088290715456] Epoch[1] Batch[0] avg_epoch_loss=3.066243\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:15 INFO 140521997772608] Epoch[1] Batch[0] avg_epoch_loss=2.690716\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:15 INFO 140274382190400] Epoch[1] Batch[0] avg_epoch_loss=3.063345\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:15 INFO 140595750582080] Epoch[1] Batch[0] avg_epoch_loss=3.006824\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:23 INFO 140088290715456] Epoch[1] Batch[5] avg_epoch_loss=3.132497\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:23 INFO 140088290715456] Epoch[1] Batch [5]#011Speed: 128.50 samples/sec#011loss=3.132497\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:24 INFO 140521997772608] Epoch[1] Batch[5] avg_epoch_loss=3.046656\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:24 INFO 140521997772608] Epoch[1] Batch [5]#011Speed: 161.44 samples/sec#011loss=3.046656\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:24 INFO 140274382190400] Epoch[1] Batch[5] avg_epoch_loss=3.013378\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:24 INFO 140274382190400] Epoch[1] Batch [5]#011Speed: 160.79 samples/sec#011loss=3.013378\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:24 INFO 140595750582080] Epoch[1] Batch[5] avg_epoch_loss=3.015561\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:24 INFO 140595750582080] Epoch[1] Batch [5]#011Speed: 159.21 samples/sec#011loss=3.015561\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:32 INFO 140521997772608] processed a total of 2748 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21248.937845230103, \"sum\": 21248.937845230103, \"min\": 21248.937845230103}}, \"EndTime\": 1538412752.21819, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412730.969001}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:32 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=129.323440782 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:32 INFO 140521997772608] #progress_metric: host=algo-4, completed 15 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:32 INFO 140274382190400] processed a total of 2783 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21596.560955047607, \"sum\": 21596.560955047607, \"min\": 21596.560955047607}}, \"EndTime\": 1538412752.891492, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412731.294581}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:32 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=128.862280165 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:32 INFO 140274382190400] #progress_metric: host=algo-2, completed 15 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:32 INFO 140595750582080] processed a total of 2804 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21589.15090560913, \"sum\": 21589.15090560913, \"min\": 21589.15090560913}}, \"EndTime\": 1538412752.58235, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412730.992945}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:32 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=129.879345794 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:32 INFO 140595750582080] #progress_metric: host=algo-3, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:33 INFO 140088290715456] Epoch[1] Batch[10] avg_epoch_loss=2.823249\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:33 INFO 140088290715456] Epoch[1] Batch [10]#011Speed: 151.66 samples/sec#011loss=2.452152\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:33 INFO 140088290715456] processed a total of 2861 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23890.061140060425, \"sum\": 23890.061140060425, \"min\": 23890.061140060425}}, \"EndTime\": 1538412753.155822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412729.265513}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:33 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=119.756367926 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:33 INFO 140088290715456] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:37 INFO 140595750582080] Epoch[2] Batch[0] avg_epoch_loss=2.744679\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:36 INFO 140521997772608] Epoch[2] Batch[0] avg_epoch_loss=3.036980\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:37 INFO 140088290715456] Epoch[2] Batch[0] avg_epoch_loss=2.941892\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:37 INFO 140274382190400] Epoch[2] Batch[0] avg_epoch_loss=2.847694\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:46 INFO 140595750582080] Epoch[2] Batch[5] avg_epoch_loss=2.952218\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:46 INFO 140595750582080] Epoch[2] Batch [5]#011Speed: 160.52 samples/sec#011loss=2.952218\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:45 INFO 140521997772608] Epoch[2] Batch[5] avg_epoch_loss=3.003180\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:45 INFO 140521997772608] Epoch[2] Batch [5]#011Speed: 160.50 samples/sec#011loss=3.003180\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:46 INFO 140088290715456] Epoch[2] Batch[5] avg_epoch_loss=2.875627\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:46 INFO 140088290715456] Epoch[2] Batch [5]#011Speed: 162.79 samples/sec#011loss=2.875627\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:46 INFO 140274382190400] Epoch[2] Batch[5] avg_epoch_loss=2.931730\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:46 INFO 140274382190400] Epoch[2] Batch [5]#011Speed: 160.49 samples/sec#011loss=2.931730\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:54 INFO 140595750582080] processed a total of 2765 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21595.01886367798, \"sum\": 21595.01886367798, \"min\": 21595.01886367798}}, \"EndTime\": 1538412774.177704, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412752.58243}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:54 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=128.038096213 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:54 INFO 140595750582080] #progress_metric: host=algo-3, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:53 INFO 140521997772608] processed a total of 2849 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21619.309902191162, \"sum\": 21619.309902191162, \"min\": 21619.309902191162}}, \"EndTime\": 1538412773.837816, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412752.218264}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:53 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=131.779644898 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:53 INFO 140521997772608] #progress_metric: host=algo-4, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:54 INFO 140088290715456] processed a total of 2840 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21071.558952331543, \"sum\": 21071.558952331543, \"min\": 21071.558952331543}}, \"EndTime\": 1538412774.227692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412753.155894}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:54 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=134.778128625 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:54 INFO 140088290715456] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:57 INFO 140274382190400] Epoch[2] Batch[10] avg_epoch_loss=2.895998\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:57 INFO 140274382190400] Epoch[2] Batch [10]#011Speed: 127.66 samples/sec#011loss=2.853120\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:57 INFO 140274382190400] processed a total of 2952 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24824.2130279541, \"sum\": 24824.2130279541, \"min\": 24824.2130279541}}, \"EndTime\": 1538412777.716067, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412752.891584}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:57 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=118.915519152 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:52:57 INFO 140274382190400] #progress_metric: host=algo-2, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:52:58 INFO 140521997772608] Epoch[3] Batch[0] avg_epoch_loss=2.931962\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:52:58 INFO 140595750582080] Epoch[3] Batch[0] avg_epoch_loss=2.844573\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:52:58 INFO 140088290715456] Epoch[3] Batch[0] avg_epoch_loss=3.223903\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:01 INFO 140274382190400] Epoch[3] Batch[0] avg_epoch_loss=2.688428\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:07 INFO 140521997772608] Epoch[3] Batch[5] avg_epoch_loss=2.936676\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:07 INFO 140521997772608] Epoch[3] Batch [5]#011Speed: 163.32 samples/sec#011loss=2.936676\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:07 INFO 140088290715456] Epoch[3] Batch[5] avg_epoch_loss=3.014610\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:07 INFO 140088290715456] Epoch[3] Batch [5]#011Speed: 163.95 samples/sec#011loss=3.014610\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:07 INFO 140595750582080] Epoch[3] Batch[5] avg_epoch_loss=2.951020\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:07 INFO 140595750582080] Epoch[3] Batch [5]#011Speed: 161.84 samples/sec#011loss=2.951020\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:12 INFO 140274382190400] Epoch[3] Batch[5] avg_epoch_loss=2.966471\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:12 INFO 140274382190400] Epoch[3] Batch [5]#011Speed: 129.35 samples/sec#011loss=2.966471\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:15 INFO 140521997772608] processed a total of 2828 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21198.80509376526, \"sum\": 21198.80509376526, \"min\": 21198.80509376526}}, \"EndTime\": 1538412795.036938, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412773.837894}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:15 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=133.403053853 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:15 INFO 140521997772608] #progress_metric: host=algo-4, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:18 INFO 140088290715456] Epoch[3] Batch[10] avg_epoch_loss=2.729962\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:18 INFO 140088290715456] Epoch[3] Batch [10]#011Speed: 129.95 samples/sec#011loss=2.388386\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:18 INFO 140088290715456] processed a total of 2891 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24331.639051437378, \"sum\": 24331.639051437378, \"min\": 24331.639051437378}}, \"EndTime\": 1538412798.55964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412774.227766}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:18 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=118.815989183 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:18 INFO 140088290715456] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:18 INFO 140595750582080] Epoch[3] Batch[10] avg_epoch_loss=2.785589\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:18 INFO 140595750582080] Epoch[3] Batch [10]#011Speed: 129.74 samples/sec#011loss=2.587072\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:18 INFO 140595750582080] processed a total of 2925 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24474.776029586792, \"sum\": 24474.776029586792, \"min\": 24474.776029586792}}, \"EndTime\": 1538412798.6528, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412774.177781}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:18 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=119.51027004 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:18 INFO 140595750582080] #progress_metric: host=algo-3, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:19 INFO 140521997772608] Epoch[4] Batch[0] avg_epoch_loss=2.866979\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:21 INFO 140088290715456] Epoch[4] Batch[0] avg_epoch_loss=3.115077\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:21 INFO 140274382190400] Epoch[3] Batch[10] avg_epoch_loss=2.864580\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:21 INFO 140274382190400] Epoch[3] Batch [10]#011Speed: 153.01 samples/sec#011loss=2.742311\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:21 INFO 140274382190400] processed a total of 2874 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23627.048015594482, \"sum\": 23627.048015594482, \"min\": 23627.048015594482}}, \"EndTime\": 1538412801.343456, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412777.716155}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:21 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=121.639573741 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:21 INFO 140274382190400] #progress_metric: host=algo-2, completed 30 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:21 INFO 140595750582080] Epoch[4] Batch[0] avg_epoch_loss=3.005942\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:25 INFO 140274382190400] Epoch[4] Batch[0] avg_epoch_loss=2.765996\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:28 INFO 140521997772608] Epoch[4] Batch[5] avg_epoch_loss=2.913687\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:28 INFO 140521997772608] Epoch[4] Batch [5]#011Speed: 164.69 samples/sec#011loss=2.913687\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:32 INFO 140088290715456] Epoch[4] Batch[5] avg_epoch_loss=2.805852\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:32 INFO 140088290715456] Epoch[4] Batch [5]#011Speed: 131.93 samples/sec#011loss=2.805852\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:32 INFO 140595750582080] Epoch[4] Batch[5] avg_epoch_loss=2.752575\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:32 INFO 140595750582080] Epoch[4] Batch [5]#011Speed: 131.78 samples/sec#011loss=2.752575\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:34 INFO 140274382190400] Epoch[4] Batch[5] avg_epoch_loss=2.828145\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:34 INFO 140274382190400] Epoch[4] Batch [5]#011Speed: 160.55 samples/sec#011loss=2.828145\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:36 INFO 140521997772608] processed a total of 2808 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21219.807147979736, \"sum\": 21219.807147979736, \"min\": 21219.807147979736}}, \"EndTime\": 1538412816.257056, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412795.037012}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:36 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=132.32851563 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:36 INFO 140521997772608] #progress_metric: host=algo-4, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:40 INFO 140521997772608] Epoch[5] Batch[0] avg_epoch_loss=2.696436\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:41 INFO 140088290715456] Epoch[4] Batch[10] avg_epoch_loss=2.408539\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:41 INFO 140088290715456] Epoch[4] Batch [10]#011Speed: 155.69 samples/sec#011loss=1.931764\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:41 INFO 140088290715456] processed a total of 2886 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23242.318153381348, \"sum\": 23242.318153381348, \"min\": 23242.318153381348}}, \"EndTime\": 1538412821.802248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412798.559709}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:41 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=124.169492761 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:41 INFO 140088290715456] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:41 INFO 140595750582080] Epoch[4] Batch[10] avg_epoch_loss=2.759258\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:41 INFO 140595750582080] Epoch[4] Batch [10]#011Speed: 154.86 samples/sec#011loss=2.767276\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:41 INFO 140595750582080] processed a total of 2870 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23326.90191268921, \"sum\": 23326.90191268921, \"min\": 23326.90191268921}}, \"EndTime\": 1538412821.980015, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412798.652874}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:41 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=123.033363854 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:41 INFO 140595750582080] #progress_metric: host=algo-3, completed 38 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:43 INFO 140274382190400] processed a total of 2809 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21678.170204162598, \"sum\": 21678.170204162598, \"min\": 21678.170204162598}}, \"EndTime\": 1538412823.021975, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412801.343541}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:43 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=129.57651621 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:43 INFO 140274382190400] #progress_metric: host=algo-2, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:46 INFO 140088290715456] Epoch[5] Batch[0] avg_epoch_loss=2.748173\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:46 INFO 140595750582080] Epoch[5] Batch[0] avg_epoch_loss=2.724155\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:47 INFO 140274382190400] Epoch[5] Batch[0] avg_epoch_loss=2.802625\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:49 INFO 140521997772608] Epoch[5] Batch[5] avg_epoch_loss=2.958278\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:49 INFO 140521997772608] Epoch[5] Batch [5]#011Speed: 164.45 samples/sec#011loss=2.958278\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:55 INFO 140595750582080] Epoch[5] Batch[5] avg_epoch_loss=2.984736\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:53:55 INFO 140595750582080] Epoch[5] Batch [5]#011Speed: 161.40 samples/sec#011loss=2.984736\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:55 INFO 140088290715456] Epoch[5] Batch[5] avg_epoch_loss=2.950163\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:53:55 INFO 140088290715456] Epoch[5] Batch [5]#011Speed: 162.04 samples/sec#011loss=2.950163\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:56 INFO 140274382190400] Epoch[5] Batch[5] avg_epoch_loss=3.068463\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:53:56 INFO 140274382190400] Epoch[5] Batch [5]#011Speed: 161.35 samples/sec#011loss=3.068463\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:57 INFO 140521997772608] processed a total of 2820 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21069.373846054077, \"sum\": 21069.373846054077, \"min\": 21069.373846054077}}, \"EndTime\": 1538412837.326741, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412816.25713}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:57 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=133.842873789 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:53:57 INFO 140521997772608] #progress_metric: host=algo-4, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:01 INFO 140521997772608] Epoch[6] Batch[0] avg_epoch_loss=3.197747\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:03 INFO 140595750582080] processed a total of 2827 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21476.972103118896, \"sum\": 21476.972103118896, \"min\": 21476.972103118896}}, \"EndTime\": 1538412843.457296, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412821.980085}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:03 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=131.628676215 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:03 INFO 140595750582080] #progress_metric: host=algo-3, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:03 INFO 140088290715456] processed a total of 2721 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21542.404174804688, \"sum\": 21542.404174804688, \"min\": 21542.404174804688}}, \"EndTime\": 1538412843.34495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412821.80232}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:03 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=126.308401197 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:03 INFO 140088290715456] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:04 INFO 140274382190400] processed a total of 2801 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21473.450899124146, \"sum\": 21473.450899124146, \"min\": 21473.450899124146}}, \"EndTime\": 1538412844.495786, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412823.022069}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:04 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=130.43932735 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:04 INFO 140274382190400] #progress_metric: host=algo-2, completed 46 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:08 INFO 140595750582080] Epoch[6] Batch[0] avg_epoch_loss=3.130795\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:08 INFO 140088290715456] Epoch[6] Batch[0] avg_epoch_loss=2.958439\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:09 INFO 140274382190400] Epoch[6] Batch[0] avg_epoch_loss=2.950954\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:10 INFO 140521997772608] Epoch[6] Batch[5] avg_epoch_loss=2.980484\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:10 INFO 140521997772608] Epoch[6] Batch [5]#011Speed: 166.16 samples/sec#011loss=2.980484\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:16 INFO 140595750582080] Epoch[6] Batch[5] avg_epoch_loss=2.873824\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:16 INFO 140595750582080] Epoch[6] Batch [5]#011Speed: 161.31 samples/sec#011loss=2.873824\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:16 INFO 140088290715456] Epoch[6] Batch[5] avg_epoch_loss=2.805274\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:16 INFO 140088290715456] Epoch[6] Batch [5]#011Speed: 160.19 samples/sec#011loss=2.805274\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:17 INFO 140274382190400] Epoch[6] Batch[5] avg_epoch_loss=2.810507\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:17 INFO 140274382190400] Epoch[6] Batch [5]#011Speed: 160.11 samples/sec#011loss=2.810507\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:21 INFO 140521997772608] Epoch[6] Batch[10] avg_epoch_loss=2.779251\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:21 INFO 140521997772608] Epoch[6] Batch [10]#011Speed: 130.99 samples/sec#011loss=2.537772\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:21 INFO 140521997772608] processed a total of 2940 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24127.1870136261, \"sum\": 24127.1870136261, \"min\": 24127.1870136261}}, \"EndTime\": 1538412861.454243, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412837.326815}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:21 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=121.853612326 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:21 INFO 140521997772608] #progress_metric: host=algo-4, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:24 INFO 140521997772608] Epoch[7] Batch[0] avg_epoch_loss=2.606654\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:25 INFO 140088290715456] processed a total of 2831 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21669.688940048218, \"sum\": 21669.688940048218, \"min\": 21669.688940048218}}, \"EndTime\": 1538412865.014943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412843.345022}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:25 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=130.642658202 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:25 INFO 140088290715456] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:26 INFO 140274382190400] processed a total of 2769 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21607.36608505249, \"sum\": 21607.36608505249, \"min\": 21607.36608505249}}, \"EndTime\": 1538412866.103508, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412844.495874}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:26 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=128.149876782 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:26 INFO 140274382190400] #progress_metric: host=algo-2, completed 53 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:28 INFO 140595750582080] Epoch[6] Batch[10] avg_epoch_loss=2.777232\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:28 INFO 140595750582080] Epoch[6] Batch [10]#011Speed: 127.14 samples/sec#011loss=2.661322\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:28 INFO 140595750582080] processed a total of 2927 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24730.95989227295, \"sum\": 24730.95989227295, \"min\": 24730.95989227295}}, \"EndTime\": 1538412868.188566, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412843.45737}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:28 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=118.353149431 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:28 INFO 140595750582080] #progress_metric: host=algo-3, completed 53 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:29 INFO 140088290715456] Epoch[7] Batch[0] avg_epoch_loss=2.396931\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:30 INFO 140274382190400] Epoch[7] Batch[0] avg_epoch_loss=2.651757\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:31 INFO 140595750582080] Epoch[7] Batch[0] avg_epoch_loss=2.548667\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:35 INFO 140521997772608] Epoch[7] Batch[5] avg_epoch_loss=2.506600\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:35 INFO 140521997772608] Epoch[7] Batch [5]#011Speed: 134.66 samples/sec#011loss=2.506600\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:38 INFO 140088290715456] Epoch[7] Batch[5] avg_epoch_loss=2.303492\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:38 INFO 140088290715456] Epoch[7] Batch [5]#011Speed: 160.18 samples/sec#011loss=2.303492\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:39 INFO 140274382190400] Epoch[7] Batch[5] avg_epoch_loss=2.515844\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:39 INFO 140274382190400] Epoch[7] Batch [5]#011Speed: 160.40 samples/sec#011loss=2.515844\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:42 INFO 140595750582080] Epoch[7] Batch[5] avg_epoch_loss=2.440132\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:42 INFO 140595750582080] Epoch[7] Batch [5]#011Speed: 130.53 samples/sec#011loss=2.440132\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:42 INFO 140521997772608] processed a total of 2838 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21419.49701309204, \"sum\": 21419.49701309204, \"min\": 21419.49701309204}}, \"EndTime\": 1538412882.874047, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412861.454331}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:42 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=132.495434506 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:42 INFO 140521997772608] #progress_metric: host=algo-4, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:46 INFO 140521997772608] Epoch[8] Batch[0] avg_epoch_loss=2.327294\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:49 INFO 140088290715456] Epoch[7] Batch[10] avg_epoch_loss=2.795921\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:49 INFO 140088290715456] Epoch[7] Batch [10]#011Speed: 128.04 samples/sec#011loss=3.386836\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:49 INFO 140088290715456] processed a total of 2939 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24717.89288520813, \"sum\": 24717.89288520813, \"min\": 24717.89288520813}}, \"EndTime\": 1538412889.733159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412865.015018}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:49 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=118.901138246 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:49 INFO 140088290715456] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:50 INFO 140274382190400] Epoch[7] Batch[10] avg_epoch_loss=2.677932\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:50 INFO 140274382190400] Epoch[7] Batch [10]#011Speed: 128.40 samples/sec#011loss=2.872437\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:50 INFO 140274382190400] processed a total of 2919 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24815.29712677002, \"sum\": 24815.29712677002, \"min\": 24815.29712677002}}, \"EndTime\": 1538412890.91918, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412866.103604}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:50 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=117.628446715 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:50 INFO 140274382190400] #progress_metric: host=algo-2, completed 61 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:51 INFO 140595750582080] Epoch[7] Batch[10] avg_epoch_loss=2.504862\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:51 INFO 140595750582080] Epoch[7] Batch [10]#011Speed: 155.39 samples/sec#011loss=2.582539\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:51 INFO 140595750582080] processed a total of 2896 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23371.546030044556, \"sum\": 23371.546030044556, \"min\": 23371.546030044556}}, \"EndTime\": 1538412891.56041, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412868.18864}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:51 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=123.910801516 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:51 INFO 140595750582080] #progress_metric: host=algo-3, completed 61 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:54:53 INFO 140088290715456] Epoch[8] Batch[0] avg_epoch_loss=2.944887\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:54:54 INFO 140274382190400] Epoch[8] Batch[0] avg_epoch_loss=3.040453\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:54:56 INFO 140595750582080] Epoch[8] Batch[0] avg_epoch_loss=2.908516\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:56 INFO 140521997772608] Epoch[8] Batch[5] avg_epoch_loss=2.817884\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:54:56 INFO 140521997772608] Epoch[8] Batch [5]#011Speed: 134.83 samples/sec#011loss=2.817884\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:04 INFO 140088290715456] Epoch[8] Batch[5] avg_epoch_loss=2.608419\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:04 INFO 140088290715456] Epoch[8] Batch [5]#011Speed: 129.85 samples/sec#011loss=2.608419\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:05 INFO 140274382190400] Epoch[8] Batch[5] avg_epoch_loss=2.656495\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:05 INFO 140274382190400] Epoch[8] Batch [5]#011Speed: 131.26 samples/sec#011loss=2.656495\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:04 INFO 140595750582080] Epoch[8] Batch[5] avg_epoch_loss=2.733279\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:04 INFO 140595750582080] Epoch[8] Batch [5]#011Speed: 163.38 samples/sec#011loss=2.733279\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:05 INFO 140521997772608] Epoch[8] Batch[10] avg_epoch_loss=2.425451\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:05 INFO 140521997772608] Epoch[8] Batch [10]#011Speed: 157.78 samples/sec#011loss=1.954530\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:05 INFO 140521997772608] processed a total of 2894 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22818.89820098877, \"sum\": 22818.89820098877, \"min\": 22818.89820098877}}, \"EndTime\": 1538412905.693255, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412882.87412}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:05 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=126.824109921 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:05 INFO 140521997772608] #progress_metric: host=algo-4, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:09 INFO 140521997772608] Epoch[9] Batch[0] avg_epoch_loss=2.482340\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:12 INFO 140088290715456] processed a total of 2846 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22328.88102531433, \"sum\": 22328.88102531433, \"min\": 22328.88102531433}}, \"EndTime\": 1538412912.062349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412889.73324}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:12 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=127.45758819 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:12 INFO 140088290715456] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:12 INFO 140595750582080] processed a total of 2838 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21432.487964630127, \"sum\": 21432.487964630127, \"min\": 21432.487964630127}}, \"EndTime\": 1538412912.993211, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412891.56048}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:12 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=132.415099936 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:12 INFO 140595750582080] #progress_metric: host=algo-3, completed 69 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:14 INFO 140274382190400] Epoch[8] Batch[10] avg_epoch_loss=2.602447\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:14 INFO 140274382190400] Epoch[8] Batch [10]#011Speed: 155.72 samples/sec#011loss=2.537589\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:14 INFO 140274382190400] processed a total of 2976 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23424.696922302246, \"sum\": 23424.696922302246, \"min\": 23424.696922302246}}, \"EndTime\": 1538412914.344218, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412890.919265}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:14 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=127.044706031 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:14 INFO 140274382190400] #progress_metric: host=algo-2, completed 69 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:15 INFO 140088290715456] Epoch[9] Batch[0] avg_epoch_loss=2.384904\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:17 INFO 140595750582080] Epoch[9] Batch[0] avg_epoch_loss=2.290306\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:19 INFO 140274382190400] Epoch[9] Batch[0] avg_epoch_loss=2.541893\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:18 INFO 140521997772608] Epoch[9] Batch[5] avg_epoch_loss=2.420791\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:18 INFO 140521997772608] Epoch[9] Batch [5]#011Speed: 163.65 samples/sec#011loss=2.420791\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:26 INFO 140595750582080] Epoch[9] Batch[5] avg_epoch_loss=2.345863\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:26 INFO 140595750582080] Epoch[9] Batch [5]#011Speed: 163.89 samples/sec#011loss=2.345863\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:26 INFO 140088290715456] Epoch[9] Batch[5] avg_epoch_loss=2.320635\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:26 INFO 140088290715456] Epoch[9] Batch [5]#011Speed: 129.17 samples/sec#011loss=2.320635\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:26 INFO 140521997772608] processed a total of 2812 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20931.51807785034, \"sum\": 20931.51807785034, \"min\": 20931.51807785034}}, \"EndTime\": 1538412926.625078, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412905.693326}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:26 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=134.342110066 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:26 INFO 140521997772608] #progress_metric: host=algo-4, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:27 INFO 140274382190400] Epoch[9] Batch[5] avg_epoch_loss=2.360189\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:27 INFO 140274382190400] Epoch[9] Batch [5]#011Speed: 161.51 samples/sec#011loss=2.360189\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:31 INFO 140521997772608] Epoch[10] Batch[0] avg_epoch_loss=2.453482\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:35 INFO 140088290715456] Epoch[9] Batch[10] avg_epoch_loss=2.014645\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:35 INFO 140088290715456] Epoch[9] Batch [10]#011Speed: 153.21 samples/sec#011loss=1.647457\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:35 INFO 140088290715456] processed a total of 2868 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23689.743995666504, \"sum\": 23689.743995666504, \"min\": 23689.743995666504}}, \"EndTime\": 1538412935.752403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412912.06243}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:35 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=121.064506059 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:35 INFO 140088290715456] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:35 INFO 140274382190400] processed a total of 2822 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21488.12699317932, \"sum\": 21488.12699317932, \"min\": 21488.12699317932}}, \"EndTime\": 1538412935.832676, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412914.344302}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:35 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=131.327513045 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:35 INFO 140274382190400] #progress_metric: host=algo-2, completed 76 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:37 INFO 140595750582080] Epoch[9] Batch[10] avg_epoch_loss=2.324851\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:37 INFO 140595750582080] Epoch[9] Batch [10]#011Speed: 129.10 samples/sec#011loss=2.299637\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:37 INFO 140595750582080] processed a total of 2857 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24445.11890411377, \"sum\": 24445.11890411377, \"min\": 24445.11890411377}}, \"EndTime\": 1538412937.438655, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412912.993288}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:37 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=116.873555559 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:37 INFO 140595750582080] #progress_metric: host=algo-3, completed 76 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:40 INFO 140274382190400] Epoch[10] Batch[0] avg_epoch_loss=2.447661\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:39 INFO 140521997772608] Epoch[10] Batch[5] avg_epoch_loss=2.393070\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:39 INFO 140521997772608] Epoch[10] Batch [5]#011Speed: 162.66 samples/sec#011loss=2.393070\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:40 INFO 140088290715456] Epoch[10] Batch[0] avg_epoch_loss=2.442774\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:40 INFO 140595750582080] Epoch[10] Batch[0] avg_epoch_loss=2.470974\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:47 INFO 140521997772608] processed a total of 2811 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21199.587106704712, \"sum\": 21199.587106704712, \"min\": 21199.587106704712}}, \"EndTime\": 1538412947.824983, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412926.625155}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:47 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=132.596228673 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:47 INFO 140521997772608] #progress_metric: host=algo-4, completed 84 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:49 INFO 140274382190400] Epoch[10] Batch[5] avg_epoch_loss=2.445494\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:49 INFO 140274382190400] Epoch[10] Batch [5]#011Speed: 159.75 samples/sec#011loss=2.445494\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:49 INFO 140088290715456] Epoch[10] Batch[5] avg_epoch_loss=2.440153\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:55:49 INFO 140088290715456] Epoch[10] Batch [5]#011Speed: 161.17 samples/sec#011loss=2.440153\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:51 INFO 140595750582080] Epoch[10] Batch[5] avg_epoch_loss=2.379165\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:51 INFO 140595750582080] Epoch[10] Batch [5]#011Speed: 131.83 samples/sec#011loss=2.379165\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:55:52 INFO 140521997772608] Epoch[11] Batch[0] avg_epoch_loss=2.572970\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:57 INFO 140274382190400] processed a total of 2802 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21333.926916122437, \"sum\": 21333.926916122437, \"min\": 21333.926916122437}}, \"EndTime\": 1538412957.16695, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412935.832765}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:57 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=131.339404238 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:55:57 INFO 140274382190400] #progress_metric: host=algo-2, completed 84 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:59 INFO 140595750582080] processed a total of 2773 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21968.86110305786, \"sum\": 21968.86110305786, \"min\": 21968.86110305786}}, \"EndTime\": 1538412959.407807, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412937.438724}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:59 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=126.223578469 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:55:59 INFO 140595750582080] #progress_metric: host=algo-3, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:00 INFO 140088290715456] Epoch[10] Batch[10] avg_epoch_loss=2.775001\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:00 INFO 140088290715456] Epoch[10] Batch [10]#011Speed: 127.69 samples/sec#011loss=3.176820\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:00 INFO 140088290715456] processed a total of 2912 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24706.523180007935, \"sum\": 24706.523180007935, \"min\": 24706.523180007935}}, \"EndTime\": 1538412960.45923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412935.752475}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:00 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=117.86303284 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:00 INFO 140088290715456] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:01 INFO 140521997772608] Epoch[11] Batch[5] avg_epoch_loss=2.702381\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:01 INFO 140521997772608] Epoch[11] Batch [5]#011Speed: 161.95 samples/sec#011loss=2.702381\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:01 INFO 140274382190400] Epoch[11] Batch[0] avg_epoch_loss=2.545398\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:02 INFO 140595750582080] Epoch[11] Batch[0] avg_epoch_loss=2.691120\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:03 INFO 140088290715456] Epoch[11] Batch[0] avg_epoch_loss=2.476202\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:10 INFO 140274382190400] Epoch[11] Batch[5] avg_epoch_loss=2.425684\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:10 INFO 140274382190400] Epoch[11] Batch [5]#011Speed: 162.24 samples/sec#011loss=2.425684\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:12 INFO 140521997772608] Epoch[11] Batch[10] avg_epoch_loss=2.508195\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:12 INFO 140521997772608] Epoch[11] Batch [10]#011Speed: 127.42 samples/sec#011loss=2.275173\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:12 INFO 140521997772608] processed a total of 2866 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24597.33009338379, \"sum\": 24597.33009338379, \"min\": 24597.33009338379}}, \"EndTime\": 1538412972.422627, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412947.82506}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:12 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=116.516181247 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:12 INFO 140521997772608] #progress_metric: host=algo-4, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:13 INFO 140595750582080] Epoch[11] Batch[5] avg_epoch_loss=2.315167\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:13 INFO 140595750582080] Epoch[11] Batch [5]#011Speed: 132.56 samples/sec#011loss=2.315167\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:14 INFO 140088290715456] Epoch[11] Batch[5] avg_epoch_loss=2.330443\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:14 INFO 140088290715456] Epoch[11] Batch [5]#011Speed: 132.28 samples/sec#011loss=2.330443\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:15 INFO 140521997772608] Epoch[12] Batch[0] avg_epoch_loss=2.374857\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:18 INFO 140274382190400] processed a total of 2804 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21420.46308517456, \"sum\": 21420.46308517456, \"min\": 21420.46308517456}}, \"EndTime\": 1538412978.58773, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412957.167024}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:18 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=130.902207023 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:18 INFO 140274382190400] #progress_metric: host=algo-2, completed 92 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:21 INFO 140595750582080] processed a total of 2838 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21777.41503715515, \"sum\": 21777.41503715515, \"min\": 21777.41503715515}}, \"EndTime\": 1538412981.185494, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412959.407864}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:21 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=130.317905087 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:21 INFO 140595750582080] #progress_metric: host=algo-3, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:22 INFO 140088290715456] processed a total of 2761 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22053.328037261963, \"sum\": 22053.328037261963, \"min\": 22053.328037261963}}, \"EndTime\": 1538412982.512867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412960.459316}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:22 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=125.195915697 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:22 INFO 140088290715456] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:23 INFO 140274382190400] Epoch[12] Batch[0] avg_epoch_loss=2.583084\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:24 INFO 140595750582080] Epoch[12] Batch[0] avg_epoch_loss=2.253812\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:25 INFO 140088290715456] Epoch[12] Batch[0] avg_epoch_loss=2.086680\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:26 INFO 140521997772608] Epoch[12] Batch[5] avg_epoch_loss=2.307309\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:26 INFO 140521997772608] Epoch[12] Batch [5]#011Speed: 132.99 samples/sec#011loss=2.307309\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:31 INFO 140274382190400] Epoch[12] Batch[5] avg_epoch_loss=2.363134\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:31 INFO 140274382190400] Epoch[12] Batch [5]#011Speed: 163.36 samples/sec#011loss=2.363134\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] processed a total of 2825 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21801.11813545227, \"sum\": 21801.11813545227, \"min\": 21801.11813545227}}, \"EndTime\": 1538412994.224053, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412972.422703}\n",
      "\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] #throughput_metric: host=algo-4, train throughput=129.579862087 records/second\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] #progress_metric: host=algo-4, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] Final loss: 2.24639830472 (occurred at epoch 12)\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] #quality_metric: host=algo-4, train final_loss <loss>=2.24639830472\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 WARNING 140521997772608] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[10/01/2018 16:56:34 INFO 140521997772608] Worker algo-4 finished training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:35 INFO 140595750582080] Epoch[12] Batch[5] avg_epoch_loss=2.132079\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:35 INFO 140595750582080] Epoch[12] Batch [5]#011Speed: 132.14 samples/sec#011loss=2.132079\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:36 INFO 140088290715456] Epoch[12] Batch[5] avg_epoch_loss=2.221646\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:36 INFO 140088290715456] Epoch[12] Batch [5]#011Speed: 130.13 samples/sec#011loss=2.221646\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] Epoch[12] Batch[10] avg_epoch_loss=2.596992\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] Epoch[12] Batch [10]#011Speed: 130.27 samples/sec#011loss=2.877621\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] processed a total of 2948 examples\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24287.513971328735, \"sum\": 24287.513971328735, \"min\": 24287.513971328735}}, \"EndTime\": 1538413002.875556, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412978.587802}\n",
      "\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] #throughput_metric: host=algo-2, train throughput=121.378730553 records/second\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] #progress_metric: host=algo-2, completed 100 % of epochs\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] Final loss: 2.59699177476 (occurred at epoch 12)\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] #quality_metric: host=algo-2, train final_loss <loss>=2.59699177476\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 WARNING 140274382190400] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[32m[10/01/2018 16:56:42 INFO 140274382190400] Worker algo-2 finished training.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] processed a total of 2773 examples\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21951.318979263306, \"sum\": 21951.318979263306, \"min\": 21951.318979263306}}, \"EndTime\": 1538413003.137119, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412981.185563}\n",
      "\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] #throughput_metric: host=algo-3, train throughput=126.32435612 records/second\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] #progress_metric: host=algo-3, completed 100 % of epochs\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] Final loss: 2.32120412726 (occurred at epoch 12)\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] #quality_metric: host=algo-3, train final_loss <loss>=2.32120412726\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 WARNING 140595750582080] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[33m[10/01/2018 16:56:43 INFO 140595750582080] Worker algo-3 finished training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] Epoch[12] Batch[10] avg_epoch_loss=2.279032\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] Epoch[12] Batch [10]#011Speed: 154.90 samples/sec#011loss=2.347895\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] processed a total of 2913 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23429.658889770508, \"sum\": 23429.658889770508, \"min\": 23429.658889770508}}, \"EndTime\": 1538413005.942834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412982.512942}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] #throughput_metric: host=algo-1, train throughput=124.328930758 records/second\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] Final loss: 2.27903168844 (occurred at epoch 12)\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] #quality_metric: host=algo-1, train final_loss <loss>=2.27903168844\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 WARNING 140088290715456] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:56:45 INFO 140088290715456] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[32m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 319321.94900512695, \"sum\": 319321.94900512695, \"min\": 319321.94900512695}, \"setuptime\": {\"count\": 1, \"max\": 19.208192825317383, \"sum\": 19.208192825317383, \"min\": 19.208192825317383}}, \"EndTime\": 1538413005.944028, \"Dimensions\": {\"Host\": \"algo-2\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413002.875626}\n",
      "\u001b[0m\n",
      "\u001b[33m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 317140.527009964, \"sum\": 317140.527009964, \"min\": 317140.527009964}, \"setuptime\": {\"count\": 1, \"max\": 23.602008819580078, \"sum\": 23.602008819580078, \"min\": 23.602008819580078}}, \"EndTime\": 1538413005.944236, \"Dimensions\": {\"Host\": \"algo-3\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413003.137192}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 316656.9559574127, \"sum\": 316656.9559574127, \"min\": 316656.9559574127}, \"setuptime\": {\"count\": 1, \"max\": 32.41109848022461, \"sum\": 32.41109848022461, \"min\": 32.41109848022461}}, \"EndTime\": 1538413005.944226, \"Dimensions\": {\"Host\": \"algo-4\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538412994.224128}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 29539.823055267334, \"sum\": 29539.823055267334, \"min\": 29539.823055267334}}, \"EndTime\": 1538413035.483999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413005.942919}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:16 INFO 140088290715456] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 30128.196001052856, \"sum\": 30128.196001052856, \"min\": 30128.196001052856}}, \"EndTime\": 1538413036.072324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413035.484155}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:16 INFO 140088290715456] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:16 INFO 140088290715456] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 85.68716049194336, \"sum\": 85.68716049194336, \"min\": 85.68716049194336}}, \"EndTime\": 1538413036.158146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413036.072403}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:16 INFO 140088290715456] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:16 INFO 140088290715456] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03600120544433594, \"sum\": 0.03600120544433594, \"min\": 0.03600120544433594}}, \"EndTime\": 1538413036.158989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413036.158202}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 8358.402013778687, \"sum\": 8358.402013778687, \"min\": 8358.402013778687}}, \"EndTime\": 1538413044.517372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413036.159026}\n",
      "\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, RMSE): 7.83167446779\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, mean_wQuantileLoss): 0.144229\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.1]): 0.110104\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.2]): 0.136995\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.3]): 0.104191\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.4]): 0.0650102\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.5]): 0.117413\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.6]): 0.188247\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.7]): 0.22053\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.8]): 0.205455\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #test_score (algo-1, wQuantileLoss[0.9]): 0.150112\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #quality_metric: host=algo-1, test RMSE <loss>=7.83167446779\u001b[0m\n",
      "\u001b[31m[10/01/2018 16:57:24 INFO 140088290715456] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.144228622317\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 358313.218832016, \"sum\": 358313.218832016, \"min\": 358313.218832016}, \"setuptime\": {\"count\": 1, \"max\": 20.27297019958496, \"sum\": 20.27297019958496, \"min\": 20.27297019958496}}, \"EndTime\": 1538413044.728679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1538413044.517463}\n",
      "\u001b[0m\n",
      "Billable seconds: 1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name HPO-Tune-16-25-14-007-de863ffe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predobj = hpo_tuner.deploy(1,'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"predictions\":[{\"mean\":[1.0501497984,1.4032204151,1.0133697987,1.2503783703,1.2417641878,1.2623944283,1.3716146946,1.1025470495,1.2747721672,1.0614644289,0.876834631,1.1310921907,1.1453151703,1.3082934618,1.2679457664,1.3313109875,0.9887498617,1.0146009922,1.3495147228,1.1981393099,0.9242219329,1.3315390348,1.0903037786,1.0839303732]}]}'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predobj.predict(json_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: HPO-Tune-16-58-41-004-d0db2d46\n"
     ]
    }
   ],
   "source": [
    "predobj.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
